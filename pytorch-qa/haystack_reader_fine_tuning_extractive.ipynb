{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7dbe5b",
   "metadata": {},
   "source": [
    "# Fine tune Haystack FarmReader for Extractive Question Answering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ba79d",
   "metadata": {},
   "source": [
    "## Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519a91bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install -qq faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22cbf535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip install -qq 'farm-haystack[faiss]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d4b9d",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12196722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from haystack.nodes import FARMReader, EmbeddingRetriever\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c409f",
   "metadata": {},
   "source": [
    "## Load the stack overflow dataset . \n",
    "`Dataset can be downloaded from - https://github.com/chauhang/pt-experiments/blob/pytorch-qa/pytorch-qa/pt_question_answers.csv `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d59718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_post_id</th>\n",
       "      <th>pt_post_type_id</th>\n",
       "      <th>pt_accepted_answer_id</th>\n",
       "      <th>pt_creation_date</th>\n",
       "      <th>pt_score</th>\n",
       "      <th>pt_title</th>\n",
       "      <th>pt_body</th>\n",
       "      <th>pt_tags</th>\n",
       "      <th>pt_parent_id</th>\n",
       "      <th>context</th>\n",
       "      <th>pt_answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34750268</td>\n",
       "      <td>1</td>\n",
       "      <td>34762233.0</td>\n",
       "      <td>2016-01-12T17:36:25.473</td>\n",
       "      <td>9</td>\n",
       "      <td>Extracting the top-k value-indices from a 1-D Tensor</td>\n",
       "      <td>&lt;p&gt;Given a 1-D tensor in Torch (&lt;code&gt;torch.Tensor&lt;/code&gt;), containing value...</td>\n",
       "      <td>&lt;python&gt;&lt;lua&gt;&lt;pytorch&gt;&lt;torch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Just loop through the tensor and run your compare:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;requ...</td>\n",
       "      <td>&lt;p&gt;As of pull request &lt;a href=\"https://github.com/torch/torch7/pull/496\" rel...</td>\n",
       "      <td>Extracting the top-k value-indices from a 1-D Tensor &lt;p&gt;Given a 1-D tensor i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38543850</td>\n",
       "      <td>1</td>\n",
       "      <td>38676842.0</td>\n",
       "      <td>2016-07-23T16:15:43.967</td>\n",
       "      <td>40</td>\n",
       "      <td>How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)?</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorf...</td>\n",
       "      <td>&lt;python&gt;&lt;tensorflow&gt;&lt;matplotlib&gt;&lt;pytorch&gt;&lt;tensorboard&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;It is quite easy to do if you have the image in a memory buffer. Below, I...</td>\n",
       "      <td>&lt;p&gt;It is quite easy to do if you have the image in a memory buffer. Below, I...</td>\n",
       "      <td>How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)? &lt;p&gt;The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41767005</td>\n",
       "      <td>1</td>\n",
       "      <td>43824857.0</td>\n",
       "      <td>2017-01-20T15:22:08.063</td>\n",
       "      <td>11</td>\n",
       "      <td>Python wheels: cp27mu not supported</td>\n",
       "      <td>&lt;p&gt;I'm trying to install pytorch (&lt;a href=\"http://pytorch.org/\" rel=\"norefer...</td>\n",
       "      <td>&lt;python&gt;&lt;linux&gt;&lt;unicode&gt;&lt;pytorch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;This is exactly that. \\nRecompile python under slack with --enable-unicod...</td>\n",
       "      <td>&lt;p&gt;This is exactly that. \\nRecompile python under slack with --enable-unicod...</td>\n",
       "      <td>Python wheels: cp27mu not supported &lt;p&gt;I'm trying to install pytorch (&lt;a hre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41861354</td>\n",
       "      <td>1</td>\n",
       "      <td>54261158.0</td>\n",
       "      <td>2017-01-25T20:45:35.297</td>\n",
       "      <td>8</td>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch</td>\n",
       "      <td>&lt;p&gt;I am using Torch7 library for implementing neural networks.  Mostly, I re...</td>\n",
       "      <td>&lt;python&gt;&lt;lua&gt;&lt;pytorch&gt;&lt;torch&gt;&lt;pre-trained-model&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The correct function is &lt;code&gt;load_lua&lt;/code&gt;:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;from tor...</td>\n",
       "      <td>&lt;p&gt;As of PyTorch 1.0 &lt;code&gt;torch.utils.serialization&lt;/code&gt; is completely re...</td>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch &lt;p&gt;I am using Torch7 library ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41924453</td>\n",
       "      <td>1</td>\n",
       "      <td>42054194.0</td>\n",
       "      <td>2017-01-29T18:31:24.687</td>\n",
       "      <td>65</td>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Datasets</td>\n",
       "      <td>&lt;p&gt;How to make use of the &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt; and &lt;code&gt;to...</td>\n",
       "      <td>&lt;python&gt;&lt;torch&gt;&lt;pytorch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Yes, that is possible. Just create the objects by yourself, e.g.&lt;/p&gt;\\n\\n&lt;...</td>\n",
       "      <td>&lt;p&gt;Yes, that is possible. Just create the objects by yourself, e.g.&lt;/p&gt;\\n\\n&lt;...</td>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Datasets &lt;p&gt;How to make use of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10758</th>\n",
       "      <td>74612146</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-29T09:54:30.430</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it possible to perform quantization on densenet169 and how?</td>\n",
       "      <td>&lt;p&gt;I have been trying to performing quantization on a densenet model without...</td>\n",
       "      <td>&lt;machine-learning&gt;&lt;pytorch&gt;&lt;artificial-intelligence&gt;&lt;densenet&gt;&lt;static-quanti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Here's how to do this on DenseNet169 from torchvision:&lt;/p&gt;\\n&lt;pre class=\"l...</td>\n",
       "      <td>&lt;p&gt;Here's how to do this on DenseNet169 from torchvision:&lt;/p&gt;\\n&lt;pre class=\"l...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>74637151</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-01T05:08:37.150</td>\n",
       "      <td>1</td>\n",
       "      <td>Why when the batch size increased, the epoch time will also increasing?</td>\n",
       "      <td>&lt;p&gt;Epoch time means the time required to train for an epoch.&lt;/p&gt;\\n&lt;p&gt;From my...</td>\n",
       "      <td>&lt;deep-learning&gt;&lt;pytorch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;As you already noticed, there are many factors that may affect epoch-time...</td>\n",
       "      <td>&lt;p&gt;As you already noticed, there are many factors that may affect epoch-time...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10760</th>\n",
       "      <td>74642594</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-01T13:23:27.277</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does StableDiffusionPipeline return black images when generating multipl...</td>\n",
       "      <td>&lt;p&gt;I am using the &lt;a href=\"https://github.com/huggingface/diffusers/tree/mai...</td>\n",
       "      <td>&lt;python&gt;&lt;pytorch&gt;&lt;apple-m1&gt;&lt;huggingface-transformers&gt;&lt;stable-diffusion&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Apparently it is indeed an Apple Silicon (M1/M2) issue, of which Hugging ...</td>\n",
       "      <td>&lt;p&gt;Apparently it is indeed an Apple Silicon (M1/M2) issue, of which Hugging ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10761</th>\n",
       "      <td>74671399</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-03T22:46:46.443</td>\n",
       "      <td>1</td>\n",
       "      <td>Locating tags in a string in PHP (with respect to the string with tags removed)</td>\n",
       "      <td>&lt;p&gt;I want to create a function that labels the location of certain HTML tags...</td>\n",
       "      <td>&lt;php&gt;&lt;string&gt;&lt;pytorch&gt;&lt;label&gt;&lt;italics&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I think I've got something. How about this:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;function labe...</td>\n",
       "      <td>&lt;p&gt;I think I've got something. How about this:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;function labe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10762</th>\n",
       "      <td>74679922</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04T18:58:11.630</td>\n",
       "      <td>0</td>\n",
       "      <td>BGR to RGB for CUB_200 images by Image.split()</td>\n",
       "      <td>&lt;p&gt;I am creating a PyTorch dataset and dataloader from CUB_200. When reading...</td>\n",
       "      <td>&lt;python&gt;&lt;image&gt;&lt;pytorch&gt;&lt;pytorch-dataloader&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I would strongly recommend you use skimage.io to load your images, not op...</td>\n",
       "      <td>&lt;p&gt;I would strongly recommend you use skimage.io to load your images, not op...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10763 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pt_post_id  pt_post_type_id  pt_accepted_answer_id  \\\n",
       "0        34750268                1             34762233.0   \n",
       "1        38543850                1             38676842.0   \n",
       "2        41767005                1             43824857.0   \n",
       "3        41861354                1             54261158.0   \n",
       "4        41924453                1             42054194.0   \n",
       "...           ...              ...                    ...   \n",
       "10758    74612146                1                    NaN   \n",
       "10759    74637151                1                    NaN   \n",
       "10760    74642594                1                    NaN   \n",
       "10761    74671399                1                    NaN   \n",
       "10762    74679922                1                    NaN   \n",
       "\n",
       "              pt_creation_date  pt_score  \\\n",
       "0      2016-01-12T17:36:25.473         9   \n",
       "1      2016-07-23T16:15:43.967        40   \n",
       "2      2017-01-20T15:22:08.063        11   \n",
       "3      2017-01-25T20:45:35.297         8   \n",
       "4      2017-01-29T18:31:24.687        65   \n",
       "...                        ...       ...   \n",
       "10758  2022-11-29T09:54:30.430         0   \n",
       "10759  2022-12-01T05:08:37.150         1   \n",
       "10760  2022-12-01T13:23:27.277         0   \n",
       "10761  2022-12-03T22:46:46.443         1   \n",
       "10762  2022-12-04T18:58:11.630         0   \n",
       "\n",
       "                                                                              pt_title  \\\n",
       "0                                 Extracting the top-k value-indices from a 1-D Tensor   \n",
       "1                 How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)?   \n",
       "2                                                  Python wheels: cp27mu not supported   \n",
       "3                                       Loading Torch7 trained models (.t7) in PyTorch   \n",
       "4                                  PyTorch: How to use DataLoaders for custom Datasets   \n",
       "...                                                                                ...   \n",
       "10758                   Is it possible to perform quantization on densenet169 and how?   \n",
       "10759          Why when the batch size increased, the epoch time will also increasing?   \n",
       "10760  Why does StableDiffusionPipeline return black images when generating multipl...   \n",
       "10761  Locating tags in a string in PHP (with respect to the string with tags removed)   \n",
       "10762                                   BGR to RGB for CUB_200 images by Image.split()   \n",
       "\n",
       "                                                                               pt_body  \\\n",
       "0      <p>Given a 1-D tensor in Torch (<code>torch.Tensor</code>), containing value...   \n",
       "1      <p>The <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorf...   \n",
       "2      <p>I'm trying to install pytorch (<a href=\"http://pytorch.org/\" rel=\"norefer...   \n",
       "3      <p>I am using Torch7 library for implementing neural networks.  Mostly, I re...   \n",
       "4      <p>How to make use of the <code>torch.utils.data.Dataset</code> and <code>to...   \n",
       "...                                                                                ...   \n",
       "10758  <p>I have been trying to performing quantization on a densenet model without...   \n",
       "10759  <p>Epoch time means the time required to train for an epoch.</p>\\n<p>From my...   \n",
       "10760  <p>I am using the <a href=\"https://github.com/huggingface/diffusers/tree/mai...   \n",
       "10761  <p>I want to create a function that labels the location of certain HTML tags...   \n",
       "10762  <p>I am creating a PyTorch dataset and dataloader from CUB_200. When reading...   \n",
       "\n",
       "                                                                               pt_tags  \\\n",
       "0                                                        <python><lua><pytorch><torch>   \n",
       "1                               <python><tensorflow><matplotlib><pytorch><tensorboard>   \n",
       "2                                                    <python><linux><unicode><pytorch>   \n",
       "3                                     <python><lua><pytorch><torch><pre-trained-model>   \n",
       "4                                                             <python><torch><pytorch>   \n",
       "...                                                                                ...   \n",
       "10758  <machine-learning><pytorch><artificial-intelligence><densenet><static-quanti...   \n",
       "10759                                                         <deep-learning><pytorch>   \n",
       "10760          <python><pytorch><apple-m1><huggingface-transformers><stable-diffusion>   \n",
       "10761                                           <php><string><pytorch><label><italics>   \n",
       "10762                                     <python><image><pytorch><pytorch-dataloader>   \n",
       "\n",
       "       pt_parent_id  \\\n",
       "0               NaN   \n",
       "1               NaN   \n",
       "2               NaN   \n",
       "3               NaN   \n",
       "4               NaN   \n",
       "...             ...   \n",
       "10758           NaN   \n",
       "10759           NaN   \n",
       "10760           NaN   \n",
       "10761           NaN   \n",
       "10762           NaN   \n",
       "\n",
       "                                                                               context  \\\n",
       "0      <p>Just loop through the tensor and run your compare:</p>\\n\\n<pre><code>requ...   \n",
       "1      <p>It is quite easy to do if you have the image in a memory buffer. Below, I...   \n",
       "2      <p>This is exactly that. \\nRecompile python under slack with --enable-unicod...   \n",
       "3      <p>The correct function is <code>load_lua</code>:</p>\\n\\n<pre><code>from tor...   \n",
       "4      <p>Yes, that is possible. Just create the objects by yourself, e.g.</p>\\n\\n<...   \n",
       "...                                                                                ...   \n",
       "10758  <p>Here's how to do this on DenseNet169 from torchvision:</p>\\n<pre class=\"l...   \n",
       "10759  <p>As you already noticed, there are many factors that may affect epoch-time...   \n",
       "10760  <p>Apparently it is indeed an Apple Silicon (M1/M2) issue, of which Hugging ...   \n",
       "10761  <p>I think I've got something. How about this:</p>\\n<pre><code>function labe...   \n",
       "10762  <p>I would strongly recommend you use skimage.io to load your images, not op...   \n",
       "\n",
       "                                                                             pt_answer  \\\n",
       "0      <p>As of pull request <a href=\"https://github.com/torch/torch7/pull/496\" rel...   \n",
       "1      <p>It is quite easy to do if you have the image in a memory buffer. Below, I...   \n",
       "2      <p>This is exactly that. \\nRecompile python under slack with --enable-unicod...   \n",
       "3      <p>As of PyTorch 1.0 <code>torch.utils.serialization</code> is completely re...   \n",
       "4      <p>Yes, that is possible. Just create the objects by yourself, e.g.</p>\\n\\n<...   \n",
       "...                                                                                ...   \n",
       "10758  <p>Here's how to do this on DenseNet169 from torchvision:</p>\\n<pre class=\"l...   \n",
       "10759  <p>As you already noticed, there are many factors that may affect epoch-time...   \n",
       "10760  <p>Apparently it is indeed an Apple Silicon (M1/M2) issue, of which Hugging ...   \n",
       "10761  <p>I think I've got something. How about this:</p>\\n<pre><code>function labe...   \n",
       "10762  <p>I would strongly recommend you use skimage.io to load your images, not op...   \n",
       "\n",
       "                                                                              question  \n",
       "0      Extracting the top-k value-indices from a 1-D Tensor <p>Given a 1-D tensor i...  \n",
       "1      How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)? <p>The ...  \n",
       "2      Python wheels: cp27mu not supported <p>I'm trying to install pytorch (<a hre...  \n",
       "3      Loading Torch7 trained models (.t7) in PyTorch <p>I am using Torch7 library ...  \n",
       "4      PyTorch: How to use DataLoaders for custom Datasets <p>How to make use of th...  \n",
       "...                                                                                ...  \n",
       "10758                                                                              NaN  \n",
       "10759                                                                              NaN  \n",
       "10760                                                                              NaN  \n",
       "10761                                                                              NaN  \n",
       "10762                                                                              NaN  \n",
       "\n",
       "[10763 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"pt_question_answers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fd0d6",
   "metadata": {},
   "source": [
    "### The raw stack overflow answers contains html tags, lets remove the html tags and convert everything to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b6e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove html\n",
    "\n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "df[\"context\"] = df[\"context\"].apply(lambda x: cleanhtml(x))\n",
    "df[\"pt_answer\"] = df[\"pt_answer\"].apply(lambda x: cleanhtml(x))\n",
    "\n",
    "df[\"context\"] = df[\"context\"].str.lower()\n",
    "df[\"pt_answer\"] = df[\"pt_answer\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8b112",
   "metadata": {},
   "source": [
    "### converting to SQuAD format for fine tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d36f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQuAD format\n",
    "# {\n",
    "#     version: \"Version du dataset\"\n",
    "#     data:[\n",
    "#             {\n",
    "#                 title: \"Titre de l'article Wikipedia\"\n",
    "#                 paragraphs:[\n",
    "#                     {\n",
    "#                         context: \"Paragraph de l'article\"\n",
    "#                         qas:[\n",
    "#                             {\n",
    "#                                 id: \"Id du pair question-réponse\"\n",
    "#                                 question: \"Question\"\n",
    "#                                 answers:[\n",
    "#                                     {\n",
    "#                                         \"answer_start\": \"Position de la réponse\"\n",
    "#                                         \"text\": \"Réponse\"\n",
    "#                                     }\n",
    "#                                 ],\n",
    "#                                 is_impossible: (not in v1)\n",
    "#                             }\n",
    "#                         ]\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c71bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json1 = {\n",
    "            'version': \"Version 1\",\n",
    "            'data': []\n",
    "}\n",
    "\n",
    "i = 0\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    data_json ={}\n",
    "    \n",
    "    data_json['title'] = row['pt_title']\n",
    "    search_index = row['context'].find(row['pt_answer'])\n",
    "    data_json['paragraphs'] = [\n",
    "                             {\n",
    "                                 'context': row['context'],\n",
    "                                 'qas':[\n",
    "                                        {\n",
    "                                            'id': i,\n",
    "                                            'question': row['pt_title'],\n",
    "                                            'answers':[\n",
    "                                                {\n",
    "                                                    \"answer_start\": search_index,\n",
    "                                                    \"text\": row['pt_answer']\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            'is_impossible': False\n",
    "                                        }\n",
    "                                    ]\n",
    "                             }\n",
    "                            ]\n",
    "    data_json1['data'].append(data_json)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27de4d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Extracting the top-k value-indices from a 1-D Tensor',\n",
       " 'paragraphs': [{'context': \"just loop through the tensor and run your compare:\\n\\nrequire 'torch'\\n\\ndata = torch.tensor({1,2,3,4,505,6,7,8,9,10,11,12})\\nidx  = 1\\nmax  = data[1]\\n\\nfor i=1,data:size()[1] do\\n   if data[i]&gt;max then\\n      max=data[i]\\n      idx=i\\n   end\\nend\\n\\nprint(idx,max)\\n\\n\\n--edit--\\nresponding to your edit: use the torch.max operation documented here: https://github.com/torch/torch7/blob/master/doc/maths.md#torchmaxresval-resind-x-dim ...\\n\\ny, i = torch.max(x, 1) returns the largest element in each column (across rows) of x, and a tensor i of their corresponding indices in x\\n\\nas of pull request #496 torch now includes a built-in api named torch.topk. example:\\n\\n&gt; t = torch.tensor{9, 1, 8, 2, 7, 3, 6, 4, 5}\\n\\n-- obtain the 3 smallest elements\\n&gt; res = t:topk(3)\\n&gt; print(res)\\n 1\\n 2\\n 3\\n[torch.doubletensor of size 3]\\n\\n-- you can also get the indices in addition\\n&gt; res, ind = t:topk(3)\\n&gt; print(ind)\\n 2\\n 4\\n 6\\n[torch.longtensor of size 3]\\n\\n-- alternatively you can obtain the k largest elements as follow\\n-- (see the api documentation for more details)\\n&gt; res = t:topk(3, true)\\n&gt; print(res)\\n 9\\n 8\\n 7\\n[torch.doubletensor of size 3]\\n\\n\\nat the time of writing the cpu implementation follows a sort and narrow approach (there are plans to improve it in the future). that being said an optimized gpu implementation for cutorch is currently being reviewed.\\nyou can use topk function.\\n\\nfor example:\\n\\nimport torch\\n\\nt = torch.tensor([5.7, 1.4, 9.5, 1.6, 6.1, 4.3])\\n\\nvalues,indices = t.topk(2)\\n\\nprint(values)\\nprint(indices)\\n\\n\\nthe result:\\n\\ntensor([9.5000, 6.1000])\\ntensor([2, 4])\\n\\n\",\n",
       "   'qas': [{'id': 0,\n",
       "     'question': 'Extracting the top-k value-indices from a 1-D Tensor',\n",
       "     'answers': [{'answer_start': 564,\n",
       "       'text': 'as of pull request #496 torch now includes a built-in api named torch.topk. example:\\n\\n&gt; t = torch.tensor{9, 1, 8, 2, 7, 3, 6, 4, 5}\\n\\n-- obtain the 3 smallest elements\\n&gt; res = t:topk(3)\\n&gt; print(res)\\n 1\\n 2\\n 3\\n[torch.doubletensor of size 3]\\n\\n-- you can also get the indices in addition\\n&gt; res, ind = t:topk(3)\\n&gt; print(ind)\\n 2\\n 4\\n 6\\n[torch.longtensor of size 3]\\n\\n-- alternatively you can obtain the k largest elements as follow\\n-- (see the api documentation for more details)\\n&gt; res = t:topk(3, true)\\n&gt; print(res)\\n 9\\n 8\\n 7\\n[torch.doubletensor of size 3]\\n\\n\\nat the time of writing the cpu implementation follows a sort and narrow approach (there are plans to improve it in the future). that being said an optimized gpu implementation for cutorch is currently being reviewed.\\n'}],\n",
       "     'is_impossible': False}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json1['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de72ba6",
   "metadata": {},
   "source": [
    "### Export the squad dataset in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807da183",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "283bccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qa.json\", \"w\") as outfile:\n",
    "    json.dump(data_json1, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746c75b",
   "metadata": {},
   "source": [
    "### Initialize Haystack FARMReader using `deepset/roberta-base-squad2` base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c72aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfeb/albert-xxlarge-v2-squad2\n",
    "#deepset/roberta-large-squad2\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "data_dir = \"data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dfc6f",
   "metadata": {},
   "source": [
    "### Finetune the reberta model based on the generated squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.train(data_dir=data_dir, train_filename=\"qa.json\", use_gpu=True, n_epochs=1, save_dir=\"data/qa_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbfda1",
   "metadata": {},
   "source": [
    "### Initialize the document store - FAISS Document store is used for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35623d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.modeling.model.prediction_head:Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": true, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS document store.\n",
    "\n",
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\", return_embedding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52301356",
   "metadata": {},
   "source": [
    "### Use EmbeddingRetriever - use `sentence-transformers/multi-qa-mpnet-base-dot-v1`  as the base model for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3876f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_retriever = EmbeddingRetriever(document_store=document_store,\n",
    "                              embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "                               model_format=\"sentence_transformers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987bcc1",
   "metadata": {},
   "source": [
    "### Initialize the FARMReader using the finetuned model - data/qa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## loading fine tuned reader\n",
    "new_reader = FARMReader(model_name_or_path=\"data/qa_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22889a6",
   "metadata": {},
   "source": [
    "### Create a haystack pipeline with retriever and reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_querying_pipeline = Pipeline()\n",
    "embedding_querying_pipeline.add_node(component=embedding_retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "embedding_querying_pipeline.add_node(component=new_reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572699be",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16407efb",
   "metadata": {},
   "source": [
    "### Combine question and answer together into a single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802d8187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question: Extracting the top-k value-indices from a 1-D Tensor\\nanswer: as o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question: How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question: Python wheels: cp27mu not supported\\nanswer: this is exactly that....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>question: Loading Torch7 trained models (.t7) in PyTorch\\nanswer: as of pyto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question: PyTorch: How to use DataLoaders for custom Datasets\\nanswer: yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10758</th>\n",
       "      <td>question: Is it possible to perform quantization on densenet169 and how?\\nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>question: Why when the batch size increased, the epoch time will also increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10760</th>\n",
       "      <td>question: Why does StableDiffusionPipeline return black images when generati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10761</th>\n",
       "      <td>question: Locating tags in a string in PHP (with respect to the string with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10762</th>\n",
       "      <td>question: BGR to RGB for CUB_200 images by Image.split()\\nanswer: i would st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10763 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text\n",
       "0      question: Extracting the top-k value-indices from a 1-D Tensor\\nanswer: as o...\n",
       "1      question: How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots...\n",
       "2      question: Python wheels: cp27mu not supported\\nanswer: this is exactly that....\n",
       "3      question: Loading Torch7 trained models (.t7) in PyTorch\\nanswer: as of pyto...\n",
       "4      question: PyTorch: How to use DataLoaders for custom Datasets\\nanswer: yes, ...\n",
       "...                                                                                ...\n",
       "10758  question: Is it possible to perform quantization on densenet169 and how?\\nan...\n",
       "10759  question: Why when the batch size increased, the epoch time will also increa...\n",
       "10760  question: Why does StableDiffusionPipeline return black images when generati...\n",
       "10761  question: Locating tags in a string in PHP (with respect to the string with ...\n",
       "10762  question: BGR to RGB for CUB_200 images by Image.split()\\nanswer: i would st...\n",
       "\n",
       "[10763 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = \"question: \" + df[\"pt_title\"] + \"\\n\" + \"answer: \" + df[\"pt_answer\"]\n",
    "\n",
    "df = df[[\"text\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73383710",
   "metadata": {},
   "source": [
    "### Convert the text to haystack document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489e6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data to initialize Document objects\n",
    "\n",
    "texts = list(df[\"text\"].values)\n",
    "documents = []\n",
    "for text in texts:\n",
    "    documents.append(Document(content=text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ba618b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008579492568969727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Writing Documents",
       "rate": null,
       "total": 10763,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b60943b21664af5b64a6dbeb145f4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/10763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00800180435180664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Updating Embedding",
       "rate": null,
       "total": 10763,
       "unit": " docs",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bf9e26dadf4798958a234de3592a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/10763 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008467912673950195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 313,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94242fdcbabb4e22b0b3384cee71377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00803685188293457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 24,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ba4512daa24586ac27bee618e0a0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete existing documents in documents store\n",
    "document_store.delete_documents()\n",
    "\n",
    "# Write documents to document store\n",
    "document_store.write_documents(documents)\n",
    "\n",
    "# Add documents embeddings to index\n",
    "document_store.update_embeddings(retriever=embedding_retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdf7a2",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da9d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict answer\n",
    "def get_answer(querying_pipeline, query):\n",
    "    prediction = querying_pipeline.run(\n",
    "    query=query,\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 5},\n",
    "        \"Reader\": {\"top_k\": 1,\"debug\": True}\n",
    "    })\n",
    "    \n",
    "    return prediction[\"answers\"][0].answer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69674c35",
   "metadata": {},
   "source": [
    "### Load top 10 frequently asked questions and run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca28651",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_questions = pd.read_csv(\"top100questions.csv\").iloc[:10].question.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9e5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008482217788696289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f09d458bd24d889167a1eec98dbdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008068323135375977,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78640298f94643ae93b294d032606d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I check if PyTorch is using the GPU?\n",
      "\n",
      "Answer:  question: How do I check if PyTorch is using the GPU?\n",
      "answer: these functions should help:\n",
      "&gt;&gt;&gt; import torch\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.is_available()\n",
      "true\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.device_count()\n",
      "1\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.current_device()\n",
      "0\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.device(0)\n",
      "&lt;torch.cuda.device at 0x7efce0b03be0&gt;\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.get_device_name(0)\n",
      "'geforce gtx 950m'\n",
      "\n",
      "this tells us:\n",
      "\n",
      "cuda is available and can be used by one device.\n",
      "device 0 refers to the gpu geforce gtx 950m, and it is currently chosen by pytorch.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007582902908325195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3044e91bd87b4276bf431c195511d8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00800943374633789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c96b50350a540d9b58c8438c977d247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I save a trained model in PyTorch?\n",
      "\n",
      "Answer:  question: how to save a Pytorch model?\n",
      "answer: to save:\n",
      "# save the weights of the model to a .pt file\n",
      "torch.save(model.state_dict(), &quot;your_model_path.pt&quot;)\n",
      "\n",
      "to load:\n",
      "# load your model architecture/module\n",
      "model = yourmodel()\n",
      "# fill your architecture with the trained weights\n",
      "model.load_state_dict(torch.load(&quot;your_model_path.pt&quot;))\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007555961608886719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ae9496adc7429a9c8daeefbcfbec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I save a trained model in PyTorch?\n",
      "\n",
      "Answer:  question: how to save a Pytorch model?\n",
      "answer: to save:\n",
      "# save the weights of the model to a .pt file\n",
      "torch.save(model.state_dict(), &quot;your_model_path.pt&quot;)\n",
      "\n",
      "to load:\n",
      "# load your model architecture/module\n",
      "model = yourmodel()\n",
      "# fill your architecture with the trained weights\n",
      "model.load_state_dict(torch.load(&quot;your_model_path.pt&quot;))\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007555961608886719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ae9496adc7429a9c8daeefbcfbec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007637977600097656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931bae10199548fc972c6c0b935603ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What does .view() do in PyTorch?\n",
      "\n",
      "Answer:  question: What is the difference between x.view(x.size(0), -1) and torch.nn.Flatten() layer and torch.flatten(x)? pytorch question\n",
      "answer: a view is a way to modify the way you look at your data without modifying the data itself:\n",
      "\n",
      "torch.view returns a view on the data: the data is not copied, only the &quot;window&quot; which you look through on the data changes\n",
      "torch.flatten returns a one-dimensional output from a multi-dimensional input. it may not copy the data if\n",
      "\n",
      "\n",
      "[the] input can be viewed as the flattened shape (source)\n",
      "\n",
      "\n",
      "torch.nn.flatten is just a wrapper for convenience around torch.flatten\n",
      "\n",
      "contiugous data just means that the data is linearly adressable in memory, e.g. for two dimension data this would mean that element [i][j] is at position i * num_columns + j. if this is already the case then .contiguous will not change your data or copy anything.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007504463195800781,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ee596981a64ba3b5b69f9c44f0f5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008152008056640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c15d428bbd4e0288143f485bbf9bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Why do we need to call zero_grad() in PyTorch?\n",
      "\n",
      "Answer:  question: Shall I use grad.zero_() in PyTorch with or without gradient tracking?\n",
      "answer: in your snippet that doesn't really matter. the underscore in the name of zero_() means it is an inplace function, and since w.grad.requires_grad == false we know that there won't be any gradient computation with respect to w.grad happening anyway. the only important thing is that it happens before the loss.backward() call.\n",
      "i would recommend though to use different names for your loss function and the actuall loss tensor it computes, otherwise you're overwriting one with the other.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00771021842956543,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f52e92c1ed48cb865ad34afa76eecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008199453353881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aee81a7e6094d239588013307d3f964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I print the model summary in PyTorch?\n",
      "\n",
      "Answer:  question: Print Bert model summary using Pytorch\n",
      "answer: i used torch-summary module-\n",
      "pip install torch-summary\n",
      "\n",
      "summary(model,input_size=(768,),depth=1,batch_dim=1, dtypes=[‘torch.inttensor’])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007742643356323242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27ad7338d54cccae43669cc1beceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008137702941894531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dd91c20d2b4c0e8e1c900b041cac72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I initialize weights in PyTorch?\n",
      "\n",
      "Answer:  question: pytorch initialize two sub-modules with same weights?\n",
      "answer: i think the most easy way would be to init one of the sub-modules at random, save the state_dict and then load_state_dict from the other module.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007527589797973633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b784a8cb7c5342cf89544e0a176e29b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007920026779174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fe5cba499f49b88169a7af99b50ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What does model.eval() do in pytorch?\n",
      "\n",
      "Answer:  question: Training model in eval() mode gives better result in PyTorch?\n",
      "answer: this seems like the model architecture is simple and when in train mode, is not able to capture the features in the data and hence undergoes underfitting.\n",
      "\n",
      "eval() disables dropouts and batch normalization, among other modules.\n",
      "\n",
      "this means that the model trains better without dropout helping the model the learn better with more neurons, also increasing the layer size, increasing the number of layers, decreasing the dropout probability, helps.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0074787139892578125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68353eb08b2b401f89215882e6bb8953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00806117057800293,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89046914d21f4fa59ff6895dd26ca645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What's the difference between reshape and view in pytorch?\n",
      "\n",
      "Answer:  question: what does reshape`(1, 1, 28, 28)` mean\n",
      "answer: a pytorch model mostly requires the first dimension of the input to be the batch size. so the shape of the image is (1, 28, 28). if you want to feed only one image to the model you still have to specify the batch size, which is of course 1 for one image. therefore he adds the batch size dimension to the image by &quot;reshaping&quot; it to (1, 1, 28, 28).\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007427692413330078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c9cd5aaa874373860aff1195ad3066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008031845092773438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41213554383c436a8a3e0b3ce3661726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What does model.train() do in PyTorch?\n",
      "\n",
      "Answer:  question: What does model.train() do in PyTorch?\n",
      "answer: model.train() tells your model that you are training the model. this helps inform layers such as dropout and batchnorm, which are designed to behave differently during training and evaluation. for instance, in training mode, batchnorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.\n",
      "more details:\n",
      "model.train() sets the mode to train\n",
      "(see source code). you can call either model.eval() or model.train(mode=false) to tell that you are testing.\n",
      "it is somewhat intuitive to expect train function to train model but it does not do that. it just sets the mode.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007951974868774414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319f010a620647eaa207b33397643804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007866144180297852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a477206db8674834831dbb0705e0d6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What does .contiguous() do in PyTorch?\n",
      "\n",
      "Answer:  question: What is the difference between x.view(x.size(0), -1) and torch.nn.Flatten() layer and torch.flatten(x)? pytorch question\n",
      "answer: a view is a way to modify the way you look at your data without modifying the data itself:\n",
      "\n",
      "torch.view returns a view on the data: the data is not copied, only the &quot;window&quot; which you look through on the data changes\n",
      "torch.flatten returns a one-dimensional output from a multi-dimensional input. it may not copy the data if\n",
      "\n",
      "\n",
      "[the] input can be viewed as the flattened shape (source)\n",
      "\n",
      "\n",
      "torch.nn.flatten is just a wrapper for convenience around torch.flatten\n",
      "\n",
      "contiugous data just means that the data is linearly adressable in memory, e.g. for two dimension data this would mean that element [i][j] is at position i * num_columns + j. if this is already the case then .contiguous will not change your data or copy anything.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for query in top_10_questions:\n",
    "    answer = get_answer(embedding_querying_pipeline, query)\n",
    "    print(\"Query: \", query)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48222fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00938272476196289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41428186d24ebca28bf0c4640b92a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007538318634033203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5fb300e8054993b26b3f958f383c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Why is my training so slow?\n",
      "Answer:  question: How to impove the speed of tf.data.experimental.CsvDataset in tensorflow 1.13.1?\n",
      "answer: you may play with the batch size in the first example, and if it reads batches from file every time you can prove it if you make it 2x bigger, you may expect 2x speed improvement. i haven't played with (experimental) class csvdataset  in tf. \n",
      "\n",
      "i am sure pandas reads your document faster and this is part of the reason why you have these times.\n",
      "\n",
      "probable the next step you should unset the loss function nn.crossentropyloss(). most probable have the regression problem and not the classification problem judging by float labels you have at the end.\n",
      "\n",
      "so try torch.nn.mseloss as the loss function.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007239818572998047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986cbf4b1c03492393135aee9982fb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010135889053344727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde40b5f4daf429ab7ec932fc4cade81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How should I scale up my Pytorch models?\n",
      "Answer:  question: Reducing batch size in pytorch\n",
      "answer: the batch size depends on the model.  typically, it's the first dimension of your input tensors.  your model uses different names than i'm used to, some of which are general terms, so i'm not sure of your model topology or usage.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0073473453521728516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46216ebd7b914bcda5e09fe60d2f24f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00803995132446289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e612aed1e5f54ea6a1b060cb5530b279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How do I make my experiment deterministic?\n",
      "Answer:  question: is it possible make cuda deterministic?\n",
      "answer: cpu and gpu can't produce the same result even if the seeds are set equal.\n",
      "refer to this and this.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007422208786010742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51886dc587a3439bbcc427a8dc4a7c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008086442947387695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83b79fe304f4dc8bb96b555fbd05236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Does PyTorch work on windows 32-bit?\n",
      "Answer:  question: Windows installing pytorch 0.3\n",
      "answer: peterjc123 released the version for windows here: https://anaconda.org/peterjc123/pytorch\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = ['Why is my training so slow?',\n",
    "             'How should I scale up my Pytorch models?',\n",
    "             'How do I make my experiment deterministic?',\n",
    "             'Does PyTorch work on windows 32-bit?']\n",
    "\n",
    "for query in questions:\n",
    "    answer = get_answer(embedding_querying_pipeline, query)\n",
    "    print(\"Query: \", query)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cb08d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008283853530883789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb101bc3c5411ea0b30b9244c98731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007783412933349609,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75da324b509438a836ef3947640f796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: L1/L2 regularization in PyTorch\\nanswer: see the documentation. add a weight_decay parameter to the optimizer for l2 regularization.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to perform L2 regularization in pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "813db56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009025335311889648,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10776803ab0245319f7b116e6e1d1639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007796764373779297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bd7c229ecb40f5b3b89cb1a78da9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: require_grad = True in pytorch model despite changing require_grad = false for all parameters\\nanswer: you should run the method:\\nmodel.requires_grad_(false)\\n\\nyou probably want to freeze only part of the network though, in your case you should change the fc1 attribute:\\nmodel.fc1 = torch.nn.linear(128, num_classes)\\n\\nwhere num_classes is the number of classes you have (you should at least unfreeze the last linear layer).'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to freeze layers whiie finetuning pytorch model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "354518e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008323907852172852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f874a918a3f4bcfb290b662a4efe1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007933378219604492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea722b3f9f32452bbd713bd71d3af482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: How to adjust the learning rate after N number of epochs?\\nanswer: you could train in two steps,\\nfirst, train with desired initial learning rate then create a second optimizer with the final learning rate. it is equivalent.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to use different learning rates while training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8f562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008915901184082031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6082d5c9435545a28348f5592ad7d1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00749969482421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573c214fff31471da75e233f8515b858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: Pytorch: use pretrained vectors to initialize nn.Embedding, but this embedding layer is not updated during the training\\nanswer: the torch.nn.embedding.from_pretrained classmethod by default freezes the parameters. if you want to train the parameters, you need to set the freeze keyword argument to false. see the documentation.\\nso you might try this instead:\\nself.embeds = torch.nn.embedding.from_pretrained(self.vec_weights, freeze=false)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to add embedding layer in pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7d979fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008244991302490234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879fb36d3d7b4cd2a49dcd169cbbc31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00748753547668457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bbfb7402c640ac81c0267d2e4c1314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: Unable to Normalize Tensor in PyTorch\\nanswer: in order to apply transforms.normalize you have to convert the input to a tensor. for this you can use transforms.totensor.\\ninv_normalize = transforms.compose(\\n    [\\n        transforms.totensor(),\\n        transforms.normalize(mean=[-0.5/0.5], std=[1/0.5])\\n    ]\\n)\\n\\n\\nthis tensor must consist of three dimensions (channels, height, width). currently you have one dimension too much. just remove the extra dimension in your view call:\\noutput = model(input).to(device).view(1, 150, 150)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to normalize the tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ebcec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007904052734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb12b98ecd44344b105c0ff27a02a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008072614669799805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcc99f413434b5d99766dc6b69d9659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: What kinds of optimization are used in PyTorch methods?\\nanswer: pytorch uses an efficient blas implementation and multithreading (openmp, if i\\'m not wrong) to parallelize such operations with multiple cores. some performance loss comes from the python itself - since this is an interpreted language, no significant compiler-like optimization can be done. you can use the jit module to speed up the \"wrapper\" code around the matrix multiplies, but for anything more than very small matrices this cost is probably negligible.\\n\\none big improvement you may be able to get manually, but which pytorch doesn\\'t apply automatically, is to properly order the matrix multiplies. as you probably know, depending on matrix shapes, a multiplication abcd may have different performance computed as a(b(cd)) than if computed as (ab)(cd), etc.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"what are the different optimizers available in pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d42fab4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007918119430541992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5cf04e0ef841658c2eb05c09522f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007937431335449219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "Inferencing Samples",
       "rate": null,
       "total": 1,
       "unit": " Batches",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6158aeabba51424e83409d3a513fa54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'question: Dropout in custom LSTM in pytorch\\nanswer: nn.lstm(... dropout=0.3) applies a dropout layer on the outputs of each lstm layer except the last layer. you can have multiple stacked layers by passing parameter num_layers &gt; 1. if you want to add a dropout to the final layer (or if lstm has only one layer), you have to add it as you are doing now.\\nif you want to replicate what lstm dropout does (which is only in case of multiple layers), you can stack lstm layers manually and add a dropout layer in between.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(embedding_querying_pipeline, \"how to add a dropout layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dce989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
