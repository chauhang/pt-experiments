{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d125aa5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5166c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4188bc",
   "metadata": {},
   "source": [
    "### Load Pytorch Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39be8e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('blogs.json')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde516c4",
   "metadata": {},
   "source": [
    "### Using langchain - split the data into multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770e8437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages: 2539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_pages(df):\n",
    "    markdown_splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    pages = []\n",
    "    for index, row in df.iterrows():\n",
    "        markdown_text = row[\"text\"]\n",
    "        metadata = row[\"metadata\"]\n",
    "        docs = markdown_splitter.create_documents([markdown_text], [metadata])\n",
    "        pages.extend(docs)\n",
    "    \n",
    "    print('total pages:', len(pages))\n",
    "    return pages\n",
    "\n",
    "\n",
    "pages = split_pages(df)\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56972c32",
   "metadata": {},
   "source": [
    "### Run this step multiple times with the start and end values as the multiple of 1000\n",
    "```\n",
    "start = 0, end = 1000\n",
    "start = 1000, end = 2000\n",
    "start = 2000, end = 3000\n",
    "start = 3000, end = 4070\n",
    "```\n",
    "\n",
    "### At the end of each iteration, blogs_qa_openai_*.csv will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d837fbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████████████████                                                                                                                                           | 374/2539 [03:21<12:07,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 557e40cc5bc0e37013c0261df8e0542b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████████████████████████                                                                                                                            | 608/2539 [05:33<22:21,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 06039c1cecc114462fb7285a40d366a6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████████████████████▍                                                                                                                      | 692/2539 [06:18<16:51,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID de02e84ba9cb18ff081d9cb762f8d48e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████████████████████▊                                                                                                        | 916/2539 [08:13<09:57,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1547fd67b6fe635997de5c561b4ba631 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                            | 1336/2539 [12:02<17:15,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d5f6767d8ed587f00a5f5074d4722b07 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████████████████████████████████████▍                                                                        | 1402/2539 [12:35<07:49,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 746b002ff36bd88d2ffecb9931304c04 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                | 1528/2539 [13:41<09:27,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 43fa05bb28c5868620520ede95293ba6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                           | 1610/2539 [14:24<10:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0f9317bb81e2dc0473c379bbfa266f47 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 1686/2539 [15:03<03:34,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 62d6ec3cfd413d86646f4347d4a0491f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 1900/2539 [16:56<04:02,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 61a2697b78b546a14f7c4e56125b63b2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 2002/2539 [17:52<02:21,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>nginx</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 2062/2539 [18:23<04:32,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9bb658aec7414dc4f76e663ffe00cada in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 2235/2539 [19:52<03:35,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a0270d650bd151200a5babdbd96caac0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 2255/2539 [20:01<03:13,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7df5c0c07be76d55852c7d2772dde790 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2539/2539 [22:38<00:00,  1.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2525, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = ''\n",
    "\n",
    "def get_qa_openai(context):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\", api_key = api_key,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": context}\n",
    "                  ]\n",
    "                )\n",
    "\n",
    "        qa = completion.choices[0].message.content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Request failed with error: {str(e)}.')\n",
    "        print(f'Waiting for 3 minutes before trying again...')\n",
    "        time.sleep(180)\n",
    "    \n",
    "    return qa\n",
    "\n",
    "questions_ans = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    futures = []\n",
    "    start = 0\n",
    "    end = 2540\n",
    "    for i in pages[start:end]:\n",
    "        \n",
    "        context = f\"Generate question and answer only in this format 'Question: Answer:' using this context \\\n",
    "        and you can decide the number of question and answer to generate based \\\n",
    "        on context size but don't generate too many same kind of questions: {i.page_content}\"\n",
    "        \n",
    "        futures.append(executor.submit(get_qa_openai, context))\n",
    "\n",
    "    for future, i in tqdm(zip(concurrent.futures.as_completed(futures), pages[start:end]), total=len(pages[start:end])):\n",
    "        try:\n",
    "            qa = future.result()\n",
    "            questions_ans.append({'text':qa, 'metadata':i.metadata})\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')\n",
    "\n",
    "df1 = pd.DataFrame(questions_ans)\n",
    "df1.to_csv(f'blogs_qa_openai_{start}_{end}.csv')\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc7a1e",
   "metadata": {},
   "source": [
    "### Each row contains multiple question and answers. Split it into multiple rows to have one question and answer per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d717cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence_by_word(sentence, split_word):\n",
    "    sentences = sentence.split(split_word)\n",
    "    result = [''.join([split_word, s.strip()]) for s in sentences if s.strip()]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c35930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_qa(df1):\n",
    "\n",
    "    final_text = []\n",
    "\n",
    "    for index,i in df1.iterrows():\n",
    "        result = split_sentence_by_word(i['text'], 'Question: ')\n",
    "        metadata = i['metadata']\n",
    "        for i in result:\n",
    "            final_text.append({'text':i, 'metadata':metadata})\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f95a9648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11887"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_qa_list = final_qa(df1)\n",
    "len(blogs_qa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b940069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dfba1d0",
   "metadata": {},
   "source": [
    "### Remove the rows where LLM couldnt find the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84206d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_excluded = []\n",
    "for i in range(len(blogs_qa_list)):\n",
    "    row = blogs_qa_list[i]\n",
    "    text = row[\"text\"]\n",
    "    answer = text.split(\"Answer: \")[-1]\n",
    "\n",
    "    if answer.strip() == \"None\":\n",
    "        to_be_excluded.append(i)\n",
    "        continue\n",
    "        \n",
    "    if \"not\" in answer.lower() and \"supported\" in answer.lower():\n",
    "        to_be_excluded.append(i)\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a77982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11887"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in sorted(to_be_excluded, reverse=True):\n",
    "    del blogs_qa_list[index]\n",
    "    \n",
    "len(docs_qa_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b944bdb",
   "metadata": {},
   "source": [
    "### Write the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c924e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"blogs_qa_dataset.json\", \"w\") as fp:\n",
    "    json.dump(blogs_qa_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df9145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
