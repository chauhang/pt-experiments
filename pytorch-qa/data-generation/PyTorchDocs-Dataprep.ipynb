{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d125aa5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4188bc",
   "metadata": {},
   "source": [
    "### Load Pytorch Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc030e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('docs.json')\n",
    "df = df[(df['text'].str.len() > 100)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "pattern = r'\\*{3,}'\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df['text'] = df['text'].str.replace('\\n\\n', '\\n')\n",
    "\n",
    "df.to_csv('docs_cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde516c4",
   "metadata": {},
   "source": [
    "### Using langchain - split the data into multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pages(df):\n",
    "    splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=2048)\n",
    "    print('chunking pages into smaller sub-pages')\n",
    "            \n",
    "    pages = []\n",
    "\n",
    "    for index, i in df.iterrows():\n",
    "        pages.extend(splitter.create_documents([i['text']], [i['metadata']]))\n",
    "    print('saving pages as pages.pkl')\n",
    "    pickle.dump(pages, open('pages.pkl', 'wb'))\n",
    "    \n",
    "    print('total pages:', len(pages))\n",
    "    return pages\n",
    "\n",
    "\n",
    "pages = split_pages(df)\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56972c32",
   "metadata": {},
   "source": [
    "### Run this step multiple times with the start and end values as the multiple of 1000\n",
    "```\n",
    "start = 0, end = 1000\n",
    "start = 1000, end = 2000\n",
    "start = 2000, end = 3000\n",
    "start = 3000, end = 4070\n",
    "```\n",
    "\n",
    "### At the end of each iteration, docs_qa_openai_*.csv will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "\n",
    "def get_qa_openai(context):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\", api_key = api_key,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": context}\n",
    "                  ]\n",
    "                )\n",
    "\n",
    "        qa = completion.choices[0].message.content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Request failed with error: {str(e)}.')\n",
    "        print(f'Waiting for 3 minutes before trying again...')\n",
    "        time.sleep(180)\n",
    "    \n",
    "    return qa\n",
    "\n",
    "questions_ans = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    futures = []\n",
    "    start = 3000\n",
    "    end = 4070\n",
    "    for i in pages[start:end]:\n",
    "        \n",
    "        context = f\"Generate question and answer only in this format 'Question: Answer:' using this context \\\n",
    "        and you can decide the number of question and answer to generate based \\\n",
    "        on context size but don't generate too many same kind of questions: {i.page_content}\"\n",
    "        \n",
    "        futures.append(executor.submit(get_qa_openai, context))\n",
    "\n",
    "    for future, i in tqdm(zip(concurrent.futures.as_completed(futures), pages[start:end]), total=len(pages[start:end])):\n",
    "        try:\n",
    "            qa = future.result()\n",
    "            questions_ans.append({'text':qa, 'metadata':i.metadata})\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')\n",
    "\n",
    "df1 = pd.DataFrame(questions_ans)\n",
    "df1.to_csv(f'docs_qa_openai_{start}_{end}.csv')\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6adaf98",
   "metadata": {},
   "source": [
    "### Combine all the output files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_qa_list = glob.glob(\"docs_qa_openai_*\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in docs_qa_list:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df_list.append(df)\n",
    "\n",
    "docs_qa = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "docs_qa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc7a1e",
   "metadata": {},
   "source": [
    "### Each row contains multiple question and answers. Split it into multiple rows to have one question and answer per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence_by_word(sentence, split_word):\n",
    "    sentences = sentence.split(split_word)\n",
    "    result = [''.join([split_word, s.strip()]) for s in sentences if s.strip()]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c35930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_qa(df1):\n",
    "\n",
    "    final_text = []\n",
    "\n",
    "    for index,i in df1.iterrows():\n",
    "        result = split_sentence_by_word(i['text'], 'Question: ')\n",
    "        metadata = i['metadata']\n",
    "        for i in result:\n",
    "            final_text.append({'text':i, 'metadata':metadata})\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_qa_list = final_qa(docs_qa)\n",
    "len(docs_qa_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfba1d0",
   "metadata": {},
   "source": [
    "### Remove the rows where LLM couldnt find the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84206d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_excluded = []\n",
    "for i in range(len(docs_qa_list)):\n",
    "    row = docs_qa_list[i]\n",
    "    text = row[\"text\"]\n",
    "    answer = text.split(\"Answer: \")[-1]\n",
    "\n",
    "    if answer.strip() == \"None\":\n",
    "        to_be_excluded.append(i)\n",
    "        continue\n",
    "        \n",
    "    if \"not\" in answer.lower() and \"supported\" in answer.lower():\n",
    "        to_be_excluded.append(i)\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in sorted(to_be_excluded, reverse=True):\n",
    "    del docs_qa_list[index]\n",
    "    \n",
    "len(docs_qa_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b944bdb",
   "metadata": {},
   "source": [
    "### Write the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"docs_qa_dataset.json\", \"w\") as fp:\n",
    "    json.dump(docs_qa_list, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
