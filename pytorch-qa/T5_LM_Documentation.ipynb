{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbc47af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 11:08:46.497251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-27 11:08:47.537272: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-01-27 11:08:47.537427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-01-27 11:08:47.537440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.6\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec65023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5019a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f448f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "     AdamW,\n",
    "     T5ForConditionalGeneration,\n",
    "     T5Tokenizer,\n",
    "     get_linear_schedule_with_warmup\n",
    " )\n",
    "# Seeds all the processes including numpy torch and other imported modules.\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8d811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8be1d",
   "metadata": {},
   "source": [
    "### Data extraction and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6339dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"documentation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bcae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['document', 'questions'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b4202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.document.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8832281",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"pt_answer\", \"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd05358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  What does Dict() do?\n",
       "1     What does 'OrderedDict' do to 'int, tVal' coll...\n",
       "2                What does Dict[int, t] collections do?\n",
       "3     What is the name of the function that enables ...\n",
       "4                           How does the function work?\n",
       "5     What is the name of the dict that -> Dict[str,...\n",
       "6                                      Tensor builtins.\n",
       "7                         -> Dict[str, tVal] built ins?\n",
       "8     What is the name of the dict that is used to b...\n",
       "9     What is the name of the dict that is built in ...\n",
       "10                 What type of builtins does dict use?\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0:10, \"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e11eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# CLEANR = re.compile('<.*?>') \n",
    "\n",
    "# def cleanhtml(raw_html):\n",
    "#   cleantext = re.sub(CLEANR, '', raw_html)\n",
    "#   return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26eca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"question\"] = data[\"question\"].apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed03c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58409dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[0, \"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23cda7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"context\"] = data[\"context\"].apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01be5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"pt_answer\"] = data[\"pt_answer\"].apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1171d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"pt_answer\"] = data[\"pt_answer\"] + data[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cf279",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f065dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME ='t5-base' \n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fed685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ed9457",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdfb7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 1024,\n",
    "        target_max_token_len: int = 1024,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        source_encoding = tokenizer(\n",
    "            data_row[\"question\"],\n",
    "#             data_row[\"context\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_encoding = tokenizer(\n",
    "            data_row[\"pt_answer\"],\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "        return dict(\n",
    "            question=data_row[\"question\"],\n",
    "#             context=data_row[\"context\"],\n",
    "            answer_text=data_row[\"pt_answer\"],\n",
    "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=source_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "813ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = SODataset(data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130f15d",
   "metadata": {},
   "source": [
    "### Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b6dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e0d6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5043, 2), (266, 2))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(data, test_size=0.05)\n",
    "train_df.shape,  val_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95330d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c7fb58a",
   "metadata": {},
   "source": [
    "### Create PTL Data module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7862eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SODataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 4,\n",
    "        source_max_token_len: int = 1024,\n",
    "        target_max_token_len: int = 1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = SODataset(\n",
    "            self.train_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len\n",
    "        )\n",
    "        self.test_dataset = SODataset(\n",
    "            self.test_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78d14079",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_EPOCHS = 1\n",
    "data_module = SODataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9269b0",
   "metadata": {},
   "source": [
    "### Loading T5 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0f7fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5201d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "512bb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99397b27",
   "metadata": {},
   "source": [
    "### Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4718a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "#         labels = input_ids\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "#         labels = input_ids\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "#         labels = input_ids\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc65f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SOModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9ba6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4560be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40f2b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "# logger = TensorBoardLogger(\"training-logs\", name=\"bio-qa\")\n",
    "logger = TensorBoardLogger(\"training-logs\", name=\"so-qa\")\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "#     callbacks = [checkpoint_callback],\n",
    "    callbacks = [checkpoint_callback, early_stopping],\n",
    "    max_epochs=50,\n",
    "    gpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b18be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b3531a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./training-logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99a23d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c296a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/ubuntu/shrinath/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b550652aa2a94d7f89aeb5e1f5c986aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 631: 'val_loss' reached 1.74801 (best 1.74801), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1262: 'val_loss' reached 1.25603 (best 1.25603), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1893: 'val_loss' reached 0.94385 (best 0.94385), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2524: 'val_loss' reached 0.67259 (best 0.67259), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 3155: 'val_loss' reached 0.47777 (best 0.47777), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 3786: 'val_loss' reached 0.33856 (best 0.33856), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 4417: 'val_loss' reached 0.22925 (best 0.22925), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 5048: 'val_loss' reached 0.16086 (best 0.16086), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 5679: 'val_loss' reached 0.12296 (best 0.12296), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 6310: 'val_loss' reached 0.09677 (best 0.09677), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 6941: 'val_loss' reached 0.07777 (best 0.07777), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 7572: 'val_loss' reached 0.05686 (best 0.05686), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 8203: 'val_loss' reached 0.04993 (best 0.04993), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 8834: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 9465: 'val_loss' reached 0.04889 (best 0.04889), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 10096: 'val_loss' reached 0.04180 (best 0.04180), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 10727: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 11358: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 11989: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 12620: 'val_loss' reached 0.03969 (best 0.03969), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 13251: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 13882: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 14513: 'val_loss' reached 0.03870 (best 0.03870), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 15144: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 15775: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 16406: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 17037: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 17668: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 18299: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 18930: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 19561: 'val_loss' reached 0.03374 (best 0.03374), saving model to '/home/ubuntu/shrinath/checkpoints/best-checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 20192: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 20823: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 21454: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 22085: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 22716: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 23347: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 23978: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 24609: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 25240: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 25871: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 26502: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 27133: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 27764: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 28395: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 29026: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 29657: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 30288: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 30919: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 31550: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da98478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec6a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98bb12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\"checkpoints/epoch50.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043dd62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429359ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65e49a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3ec68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926e298e",
   "metadata": {},
   "source": [
    "### test against validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3a45f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, best_model):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"pt_title\"],\n",
    "#         question[\"pt_body\"],\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    generated_ids = best_model.model.generate(\n",
    "        input_ids=source_encoding[\"input_ids\"],\n",
    "        attention_mask=source_encoding[\"attention_mask\"],\n",
    "        num_beams=1,  # greedy search\n",
    "        max_length=512,\n",
    "        repetition_penalty=2.5,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "#     print(generated_ids)\n",
    "    preds = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f6d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87395667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SOModel()\n",
    "# # best_model = model.load_from_checkpoint(\"checkpoints/best-checkpoint-epoch=18-val_loss=1.55.ckpt\")\n",
    "# best_model = model.load_from_checkpoint(\"checkpoints/epoch50.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae8119fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bd2e00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This module contains observers which are used to collect statistics about the values observed during calibration (PTQ) or training (QAT). +------------+--------------------------------------------------------------------------------------------+ | \"Observer | Base observer Module. | | Base\" | | +------------+--------------------------------------------------------------------------------------------+ | \"MinMaxOb | Observer module for computing the quantization parameters based on the running min and max | | server\" | values. | +------------+--------------------------------------------------------------------------------------------+ | \"MovingAv | Observer module for computing the quantization parameters based on the moving average of | | erageMinM | the min and max values. | | axObserve | | | r\" | | +------------+--------------------------------------------------------------------------------------------+ | \"PerChann | Observer module for computing the quantization parameters based on the running per channel | | elMinMaxO | min and max values. | | bserver\" | | +------------+--------------------------------------------------------------------------------------------+ | \"MovingAv | Observer module for computing the quantization parameters based on the running per channel | | eragePerC | min and max values. | | hannelMin | | | MaxObserv | | | er\" | | +------------+--------------------------------------------------------------------------------------------+ | \"Histogra | The modules are designed to facilitate the quantization parameters based on the running histogram of tensor values along with min/max values. | | mObserver'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {}\n",
    "\n",
    "question[\"pt_title\"] = \"Saving and loading models\"\n",
    "\n",
    "generate_answer(question, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449e9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b23d5771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See what is inside a package? ----------------------------- Treat the packages like ZIP archives  The container format for a \"torch.package\" is zip, so any tools that work with standard ZipFile are available to view and use. Some common ways to interact with ZIP files: * \"unzip my_package.pt\" will unzip the \"torch.package\" archive to disk, where you can freely inspect its contents. $ unzip my_package.pt && tree my_package my_package .data   94304870911616.storage   92304900784016.storage   extern_modules   version  models   model_1.pkl  torchvision  models  resnet.py  utils.py  cd my_package && cat torchvision/models/resnet.py... * The Python \"zipfile\" module provides a standard way to read and write ZIP archive contents. from zipfile import ZipFile with ZipFile(\"my_package.pt\") as myzip: file_bytes = myzip.read(\"torchvision/models/resnet.py\") # edit file_bytes in some way myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes) * vim has the ability to natively read ZIP archives. You can even edit files and :\"write\" them back into the archive! # add this to your.vimrc to treat *.pt files as zip files au BufReadCmd *.pt call zip#Browse(expand(\"amatch>\"))  vi my_package.pt Use the \"file_structure()\" API  \"PackageImporter\" provides a \"file_structure()\" method, which will return a printable and queryable \"Directory\" object. The \"Directory\" object is a simple directory structure that you can use to explore the current contents'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {}\n",
    "\n",
    "question[\"pt_title\"] = \"what is torch.storage?\"\n",
    "\n",
    "generate_answer(question, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcac08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270bef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c3171c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {}\n",
    "\n",
    "question[\"pt_title\"] = \"How to load model in PyTorch?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b9c761b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+------------+--------------------------------------------------------------------------------------------+ | \"script\" | Scripting a function or \"nn.Module\" will inspect the source code, compile it as | | | TorchScript code using the TorchScript compiler, and return a \"ScriptModule\" or | | | \"ScriptFunction\". | +------------+--------------------------------------------------------------------------------------------+ | \"trace\" | Trace a function and Returns an executable or \"ScriptFunction\" that will be optimized | | | using just-in-time compilation. | +------------+--------------------------------------------------------------------------------------------+ | \"script_i | Compilerates \"fn\" when it is first called during tracing. | | f_tracing\" | | +------------+--------------------------------------------------------------------------------------------+ | \"trace_mo | Trace a module and return an executable \"ScriptModule\" that will be optimised using just- | | dule\" | in-time compilation. | +------------+--------------------------------------------------------------------------------------------+ | \"fork\" | Creates an asynchronous task executing *func* and a reference to the value of the result | | | of this execution. | +------------+--------------------------------------------------------------------------------------------+ | \"wait\" | Forces completion of a *torch.jit.Future[T]* asynchronous task, returning the result of | | | the task. | +------------+-----------------------------'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(question, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4dc9ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+------------+--------------------------------------------------------------------------------------------+ | \"pixel_sh | Rearranges elements in a tensor of shape (*, C times r2, H, W) to a tensor of shape (*, | | uffle\" | C, H times r, W times r), where r is the \"upscale_factor\". | +------------+--------------------------------------------------------------------------------------------+ | \"pixel_un | Reverses the \"PixelShuffle\" operation by rearranging elements in a tensor of shape (*, C, | | shuffle\" | H times r, W times r) to a tensor of shape (*, C times r3, H, W), where r is the | | | \"downscale_factor\". | +------------+--------------------------------------------------------------------------------------------+ | \"pad\" | Pads tensor. | +------------+--------------------------------------------------------------------------------------------+ | \"interpol | Down/up samples the input to either the given \"size\" or the given \"scale_factor\" | | ate\" | | +------------+--------------------------------------------------------------------------------------------+ | \"upsample\" | Upsamples the input to either the given \"sized\" or the given \"scale_factor\" | +------------+--------------------------------------------------------------------------------------------+ | \"upsample | Upsamples the'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {}\n",
    "\n",
    "question[\"pt_title\"] = \"How to crop an image using pytorch?\"\n",
    "\n",
    "question[\"pt_body\"] = \"\"\"\n",
    "How to crop an image using pytorch?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "generate_answer(question, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d501377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TorchScript supports a subset of Python's variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different branches of an if statement, it is an error to use it after the end of the if statements. Similarly, a variable is not allowed to be used if it is only *defined* along some paths through the function. Example: @torch.jit.script def foo(x): if x  0: y = 4 print(y) Traceback (most recent call last):... RuntimeError:... y is not defined in the false branch... @torch.jit.script... def foo(x): if x  0:  y = 4  --- HERE print(y) and was used here: if x  0: y = 4 Print(y)  --- HERE... Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in Use of Python Values. Use of Python Values\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {}\n",
    "\n",
    "question[\"pt_title\"] = \"How to convert float tensor to int?\"\n",
    "\n",
    "question[\"pt_body\"] = \"\"\"\n",
    "How to convert float tensor to int?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "generate_answer(question, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05ab44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f8b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ddd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
