{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9619e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS, ElasticVectorSearch\n",
    "import elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d559fea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\ni'm tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>how to disable neptune callback in transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>bgr to rgb for cub_200 images by image.split()...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>neural networks extending learning domain\\ni h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      extracting the top-k value-indices from a 1-d ...\n",
       "1      how to display custom images in tensorboard (e...\n",
       "2      python wheels: cp27mu not supported\\ni'm tryin...\n",
       "3      loading torch7 trained models (.t7) in pytorch...\n",
       "4      pytorch: how to use dataloaders for custom dat...\n",
       "...                                                  ...\n",
       "14588  how to disable neptune callback in transformer...\n",
       "14589  bgr to rgb for cub_200 images by image.split()...\n",
       "14590  neural networks extending learning domain\\ni h...\n",
       "14591  how do i multiply tensors like this?\\ni am wor...\n",
       "14592  how do i multiply tensors like this?\\ni am wor...\n",
       "\n",
       "[14593 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### load data\n",
    "\n",
    "df = pd.read_csv(\"pt_question_answers.csv\")\n",
    "\n",
    "df[\"text\"] = df[\"pt_title\"] + \"\\n\" + df[\"pt_body\"] + \"\\n\" + df[\"pt_answer\"]\n",
    "\n",
    "df = df[[\"text\"]]\n",
    "\n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: cleanhtml(x))\n",
    "\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9895ff8",
   "metadata": {},
   "source": [
    "## Custom Embeddings and FAISS index using dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b68dc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "def get_context(question, truncate_length=512, k=5):\n",
    "\n",
    "    question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "        \"embeddings\", question_embedding, k=k\n",
    "    )\n",
    "\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "    samples_df[\"text\"] = samples_df[\"text\"].str[:truncate_length]\n",
    "\n",
    "    return samples_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58850452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5d7c4312c246be8e2a7faf8399f921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 13s, sys: 46.1 s, total: 17min 59s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "embeddings_dataset = dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566daf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'embeddings'],\n",
       "    num_rows: 14593\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034d41d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ad0fad499c442bbba0de13edf6f826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 150 ms, sys: 43.9 ms, total: 194 ms\n",
      "Wall time: 301 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'embeddings'],\n",
       "    num_rows: 14593\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50c572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 ms, sys: 0 ns, total: 33.7 ms\n",
      "Wall time: 32.6 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>different method of running pytorch on gpu\\nse...</td>\n",
       "      <td>[-0.3892856538295746, -0.34232109785079956, -0...</td>\n",
       "      <td>24.639463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a way to allocate remaining gpu to yo...</td>\n",
       "      <td>[-0.23101496696472168, -0.38610121607780457, -...</td>\n",
       "      <td>23.941139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pytorch is not using gpu even it detects the g...</td>\n",
       "      <td>[-0.2389562726020813, -0.2919696867465973, -0....</td>\n",
       "      <td>21.565807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to confirm that pytorch lightning is using...</td>\n",
       "      <td>[0.0017349837580695748, -0.5566844344139099, -...</td>\n",
       "      <td>21.380346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i check if pytorch is using the gpu?\\nh...</td>\n",
       "      <td>[-0.2592145800590515, -0.6313626170158386, -0....</td>\n",
       "      <td>17.069004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "4  different method of running pytorch on gpu\\nse...   \n",
       "3  is there a way to allocate remaining gpu to yo...   \n",
       "2  pytorch is not using gpu even it detects the g...   \n",
       "1  how to confirm that pytorch lightning is using...   \n",
       "0  how do i check if pytorch is using the gpu?\\nh...   \n",
       "\n",
       "                                          embeddings     scores  \n",
       "4  [-0.3892856538295746, -0.34232109785079956, -0...  24.639463  \n",
       "3  [-0.23101496696472168, -0.38610121607780457, -...  23.941139  \n",
       "2  [-0.2389562726020813, -0.2919696867465973, -0....  21.565807  \n",
       "1  [0.0017349837580695748, -0.5566844344139099, -...  21.380346  \n",
       "0  [-0.2592145800590515, -0.6313626170158386, -0....  17.069004  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_context('How do I check if PyTorch is using the GPU?', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ecf448",
   "metadata": {},
   "source": [
    "## langchain.vectorstores FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1119977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da7591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8183d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 34s, sys: 48.9 s, total: 7min 23s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "faiss_docsearch = FAISS.from_texts(text, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0e5eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 ms, sys: 3.65 ms, total: 30.1 ms\n",
      "Wall time: 29.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = faiss_docsearch.similarity_search(query='How do I check if PyTorch is using the GPU?', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a60416ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"how do i check if pytorch is using the gpu?\\nhow do i check if pytorch is using the gpu? the nvidia-smi command can detect gpu activity, but i want to check it directly from inside a python script.\\n\\nso the learning rate is stored in optim.param_groups[i]['lr'].\\noptim.param_groups is a list of the different weight groups which can have different learning rates. thus, simply doing:\\nfor g in optim.param_groups:\\n    g['lr'] = 0.001\\n\\nwill do the trick.\\n  \\n  \\n**alternatively,**\\nas mentionned in the comments, if your learning rate only depends on the epoch number, you can use a learning rate scheduler.\\nfor example (modified example from the doc):\\ntorch.optim.lr_scheduler import lambdalr\\noptimizer = torch.optim.sgd(model.parameters(), lr=0.1, momentum=0.9)\\n# assuming optimizer has two groups.\\nlambda_group1 = lambda epoch: epoch // 30\\nlambda_group2 = lambda epoch: 0.95 ** epoch\\nscheduler = lambdalr(optimizer, lr_lambda=[lambda1, lambda2])\\nfor epoch in range(100):\\n    train(...)\\n    validate(...)\\n    scheduler.step()\\n\\nalso, there is a prebuilt learning rate scheduler to reduce on plateaus.\\n\", lookup_str='', metadata={}, lookup_index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302912e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3682a09",
   "metadata": {},
   "source": [
    "## langchain.vectorstores ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e88532cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 43s, sys: 49.6 s, total: 7min 32s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elastic_vector_search = ElasticVectorSearch.from_texts(text, hf_embeddings, elasticsearch_url=\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "414ec15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 327 µs, total: 24.4 ms\n",
      "Wall time: 50.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = elastic_vector_search.similarity_search(query='How do I check if PyTorch is using the GPU?', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1bc3671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"how do i check if pytorch is using the gpu?\\nhow do i check if pytorch is using the gpu? the nvidia-smi command can detect gpu activity, but i want to check it directly from inside a python script.\\n\\nso the learning rate is stored in optim.param_groups[i]['lr'].\\noptim.param_groups is a list of the different weight groups which can have different learning rates. thus, simply doing:\\nfor g in optim.param_groups:\\n    g['lr'] = 0.001\\n\\nwill do the trick.\\n  \\n  \\n**alternatively,**\\nas mentionned in the comments, if your learning rate only depends on the epoch number, you can use a learning rate scheduler.\\nfor example (modified example from the doc):\\ntorch.optim.lr_scheduler import lambdalr\\noptimizer = torch.optim.sgd(model.parameters(), lr=0.1, momentum=0.9)\\n# assuming optimizer has two groups.\\nlambda_group1 = lambda epoch: epoch // 30\\nlambda_group2 = lambda epoch: 0.95 ** epoch\\nscheduler = lambdalr(optimizer, lr_lambda=[lambda1, lambda2])\\nfor epoch in range(100):\\n    train(...)\\n    validate(...)\\n    scheduler.step()\\n\\nalso, there is a prebuilt learning rate scheduler to reduce on plateaus.\\n\", lookup_str='', metadata={}, lookup_index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332627c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ac02df",
   "metadata": {},
   "source": [
    "| VectorStore | Compute embeddings on 14593 rows | similarity search top 5 |\n",
    "| --- | --- | --- |\n",
    "| Custom | 4min 31s (embeddings) + 301 ms (add_faiss_index) | 32.6 ms |\n",
    "| FAISS | 2min 19s | 29.2 ms |\n",
    "| ElasticSearch | 2min 39s | 50.8 ms |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e6f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
