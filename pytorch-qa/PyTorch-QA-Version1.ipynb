{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 18:21:36.573717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-03 18:21:37.536856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-03 18:21:37.537003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-03 18:21:37.537016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pt_question_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14593, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_title</th>\n",
       "      <th>pt_body</th>\n",
       "      <th>pt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extracting the top-k value-indices from a 1-D ...</td>\n",
       "      <td>&lt;p&gt;Given a 1-D tensor in Torch (&lt;code&gt;torch.Te...</td>\n",
       "      <td>&lt;p&gt;As of pull request &lt;a href=\"https://github....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Display Custom Images in Tensorboard (e...</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"https://github.com/tensorflow/...</td>\n",
       "      <td>&lt;p&gt;It is quite easy to do if you have the imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python wheels: cp27mu not supported</td>\n",
       "      <td>&lt;p&gt;I'm trying to install pytorch (&lt;a href=\"htt...</td>\n",
       "      <td>&lt;p&gt;Yes, that is possible. Just create the obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch</td>\n",
       "      <td>&lt;p&gt;I am using Torch7 library for implementing ...</td>\n",
       "      <td>&lt;p&gt;&lt;code&gt;view()&lt;/code&gt; reshapes the tensor wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Dat...</td>\n",
       "      <td>&lt;p&gt;How to make use of the &lt;code&gt;torch.utils.da...</td>\n",
       "      <td>&lt;p&gt;While you will not get as detailed informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>How to disable Neptune callback in transformer...</td>\n",
       "      <td>&lt;p&gt;After installing &lt;a href=\"https://docs.nept...</td>\n",
       "      <td>&lt;p&gt;To disable Neptune callback in transformers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>BGR to RGB for CUB_200 images by Image.split()</td>\n",
       "      <td>&lt;p&gt;I am creating a PyTorch dataset and dataloa...</td>\n",
       "      <td>&lt;p&gt;I would strongly recommend you use skimage....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>Neural Networks Extending Learning Domain</td>\n",
       "      <td>&lt;p&gt;I have a simple function &lt;strong&gt;f&lt;/strong&gt;...</td>\n",
       "      <td>&lt;p&gt;What you want is called extrapolation (as o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>How do I multiply tensors like this?</td>\n",
       "      <td>&lt;p&gt;I am working on a project where I need to m...</td>\n",
       "      <td>&lt;p&gt;You should familiarize yourself with the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>How do I multiply tensors like this?</td>\n",
       "      <td>&lt;p&gt;I am working on a project where I need to m...</td>\n",
       "      <td>&lt;p&gt;It seems like what you are trying to do is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                pt_title  \\\n",
       "0      Extracting the top-k value-indices from a 1-D ...   \n",
       "1      How to Display Custom Images in Tensorboard (e...   \n",
       "2                    Python wheels: cp27mu not supported   \n",
       "3         Loading Torch7 trained models (.t7) in PyTorch   \n",
       "4      PyTorch: How to use DataLoaders for custom Dat...   \n",
       "...                                                  ...   \n",
       "14588  How to disable Neptune callback in transformer...   \n",
       "14589     BGR to RGB for CUB_200 images by Image.split()   \n",
       "14590          Neural Networks Extending Learning Domain   \n",
       "14591               How do I multiply tensors like this?   \n",
       "14592               How do I multiply tensors like this?   \n",
       "\n",
       "                                                 pt_body  \\\n",
       "0      <p>Given a 1-D tensor in Torch (<code>torch.Te...   \n",
       "1      <p>The <a href=\"https://github.com/tensorflow/...   \n",
       "2      <p>I'm trying to install pytorch (<a href=\"htt...   \n",
       "3      <p>I am using Torch7 library for implementing ...   \n",
       "4      <p>How to make use of the <code>torch.utils.da...   \n",
       "...                                                  ...   \n",
       "14588  <p>After installing <a href=\"https://docs.nept...   \n",
       "14589  <p>I am creating a PyTorch dataset and dataloa...   \n",
       "14590  <p>I have a simple function <strong>f</strong>...   \n",
       "14591  <p>I am working on a project where I need to m...   \n",
       "14592  <p>I am working on a project where I need to m...   \n",
       "\n",
       "                                               pt_answer  \n",
       "0      <p>As of pull request <a href=\"https://github....  \n",
       "1      <p>It is quite easy to do if you have the imag...  \n",
       "2      <p>Yes, that is possible. Just create the obje...  \n",
       "3      <p><code>view()</code> reshapes the tensor wit...  \n",
       "4      <p>While you will not get as detailed informat...  \n",
       "...                                                  ...  \n",
       "14588  <p>To disable Neptune callback in transformers...  \n",
       "14589  <p>I would strongly recommend you use skimage....  \n",
       "14590  <p>What you want is called extrapolation (as o...  \n",
       "14591  <p>You should familiarize yourself with the te...  \n",
       "14592  <p>It seems like what you are trying to do is ...  \n",
       "\n",
       "[14593 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"pt_title\", \"pt_body\", \"pt_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"pt_title\"] + \"\\n\" + df[\"pt_body\"] + \"\\n\" + df[\"pt_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\ni'm tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>how to disable neptune callback in transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>bgr to rgb for cub_200 images by image.split()...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>neural networks extending learning domain\\ni h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      extracting the top-k value-indices from a 1-d ...\n",
       "1      how to display custom images in tensorboard (e...\n",
       "2      python wheels: cp27mu not supported\\ni'm tryin...\n",
       "3      loading torch7 trained models (.t7) in pytorch...\n",
       "4      pytorch: how to use dataloaders for custom dat...\n",
       "...                                                  ...\n",
       "14588  how to disable neptune callback in transformer...\n",
       "14589  bgr to rgb for cub_200 images by image.split()...\n",
       "14590  neural networks extending learning domain\\ni h...\n",
       "14591  how do i multiply tensors like this?\\ni am wor...\n",
       "14592  how do i multiply tensors like this?\\ni am wor...\n",
       "\n",
       "[14593 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a913809b4dce4994b0974a3a31b48910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609777a4fd714a3496bd58bf7f6204f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'embeddings'],\n",
       "    num_rows: 14593\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question, truncate_length=512, k=5):\n",
    "\n",
    "    question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "        \"embeddings\", question_embedding, k=k\n",
    "    )\n",
    "\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "    samples_df[\"text\"] = samples_df[\"text\"].str[:truncate_length]\n",
    "#     print(samples_df)\n",
    "        \n",
    "#     for i in range(len(samples_df)):\n",
    "#         print(len(samples_df.iloc[i][\"text\"]))\n",
    "\n",
    "    return '\\n'.join(samples_df.text.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\"top100questions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_list = []\n",
    "# context_list = []\n",
    "# for question in tqdm(questions_df[\"question\"]):\n",
    "#     print(question)\n",
    "#     question_list.append(question)\n",
    "#     context_list.append(get_context(question))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_df = pd.DataFrame({\n",
    "#     \"question\": question_list,\n",
    "#     \"context\": context_list\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I check if PyTorch is using the GPU?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I save a trained model in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does .view() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do we need to call zero_grad() in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I print the model summary in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do I initialize weights in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does model.eval() do in pytorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the difference between reshape and view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What does model.train() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does .contiguous() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question\n",
       "0      How do I check if PyTorch is using the GPU?\\n\n",
       "1        How do I save a trained model in PyTorch?\\n\n",
       "2                 What does .view() do in PyTorch?\\n\n",
       "3   Why do we need to call zero_grad() in PyTorch?\\n\n",
       "4     How do I print the model summary in PyTorch?\\n\n",
       "5          How do I initialize weights in PyTorch?\\n\n",
       "6            What does model.eval() do in pytorch?\\n\n",
       "7  What's the difference between reshape and view...\n",
       "8           What does model.train() do in PyTorch?\\n\n",
       "9           What does .contiguous() do in PyTorch?\\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa(question, context):\n",
    "\n",
    "    template = \"\"\"from the context: {context} answer the question: {question}.\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=\n",
    "                         HuggingFaceHub(\n",
    "                             huggingfacehub_api_token=hf_token,\n",
    "                             repo_id=\"google/flan-t5-xl\",\n",
    "                             model_kwargs={\"temperature\":0}))\n",
    "\n",
    "    print(llm_chain.predict(question=question, context=context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.save()/torch.load() is for saving/loading\n"
     ]
    }
   ],
   "source": [
    "question = \"how to save pytorch model\"\n",
    "context = get_context(question, k=3)\n",
    "run_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_fn is not a float\n"
     ]
    }
   ],
   "source": [
    "question = \"Why my training is slow\"\n",
    "context = get_context(question, k=3)\n",
    "run_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to evaluate the model\n"
     ]
    }
   ],
   "source": [
    "question = \"what is model.eval()\"\n",
    "context = get_context(question, k=3)\n",
    "run_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys import os import python import os\n"
     ]
    }
   ],
   "source": [
    "question = \"how to typecast tensor from float to long\"\n",
    "context = get_context(question, k=3)\n",
    "run_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "How do I check if PyTorch is using the GPU?\n",
      "nvidia-smi command can detect gpu activity, but i\n",
      "***********************************\n",
      "How do I save a trained model in PyTorch?\n",
      "torch.save()/torch.load() is for saving/loading\n",
      "***********************************\n",
      "What does .view() do in PyTorch?\n",
      "reshapes the tensor to a different but compatible shape\n",
      "***********************************\n",
      "Why do we need to call zero_grad() in PyTorch?\n",
      "zero_grad(self) | sets gradients of all model parameters to zero.\n",
      "***********************************\n",
      "How do I print the model summary in PyTorch?\n",
      "how do i print the summary of a model in pytorch like\n",
      "***********************************\n",
      "How do I initialize weights in PyTorch?\n",
      "solved it by saving only the model's state_dict() via torch.save\n",
      "***********************************\n",
      "What does model.eval() do in pytorch?\n",
      "modifies certain modules (layers) which are required to behave differently during training and inference\n",
      "***********************************\n",
      "What's the difference between reshape and view in pytorch?\n",
      "in numpy, we use ndarray.reshape() for \n",
      "***********************************\n",
      "What does model.train() do in PyTorch?\n",
      "does it call forward() in nn.module?\n",
      "***********************************\n",
      "What does .contiguous() do in PyTorch?\n",
      "tensor x\n",
      "***********************************\n",
      "Why do we \"pack\" the sequences in PyTorch?\n",
      "i was trying to replicate how to use packing for variable-length sequence inputs for \n"
     ]
    }
   ],
   "source": [
    "for question in questions_df.loc[:10, \"question\"]:\n",
    "    print(\"***********************************\")\n",
    "    question = question.rstrip()\n",
    "    print(question)\n",
    "    context = get_context(question, k=3)\n",
    "    run_qa(question, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"how to save pytorch model\"\n",
    "# context = get_context(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = get_context(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate(\n",
    "#     template=\"Content: {page_content}\",\n",
    "#     input_variables=[\"page_content\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=\n",
    "#                      HuggingFaceHub(\n",
    "#                          huggingfacehub_api_token=hf_token,\n",
    "#                          repo_id=\"google/flan-t5-xl\",\n",
    "#                          model_kwargs={\"temperature\":0.01}))\n",
    "\n",
    "# question = question\n",
    "\n",
    "# page_content=context\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_TEXT_QA_PROMPT_TMPL = (\n",
    "#     \"Context information is below. \\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\"\n",
    "#     \"\\n---------------------\\n\"\n",
    "#     \"Given the context information and not prior knowledge, \"\n",
    "#     \"answer the question: {question}\\n\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"context_str\", \"question\"], template=DEFAULT_TEXT_QA_PROMPT_TMPL\n",
    "# )\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=\n",
    "#                      HuggingFaceHub(\n",
    "#                          huggingfacehub_api_token=hf_token,\n",
    "#                          repo_id=\"google/flan-t5-xl\",\n",
    "#                          model_kwargs={\"temperature\":1e-10}))\n",
    "\n",
    "# question = question\n",
    "\n",
    "# context_str = context\n",
    "# print(llm_chain.run({\"question\": question, \"context_str\" : context_str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_context_df(question, truncate_length=512, k=5):\n",
    "\n",
    "#     question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "#     scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "#         \"embeddings\", question_embedding, k=k\n",
    "#     )\n",
    "\n",
    "#     samples_df = pd.DataFrame.from_dict(samples)\n",
    "#     samples_df[\"scores\"] = scores\n",
    "#     samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "#     samples_df[\"text\"] = samples_df[\"text\"].str[:truncate_length]\n",
    "# #     print(samples_df)\n",
    "\n",
    "#     return samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_str = get_context(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_df = get_context_df(question)\n",
    "# existing_answer = answer_df.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_REFINE_PROMPT_TMPL = (\n",
    "#     \"The original question is as follows: {question}\\n\"\n",
    "#     \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "#     \"We have the opportunity to refine the existing answer\"\n",
    "#     \"(only if needed) with some more context below.\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better \"\n",
    "#     \"answer the question. \"\n",
    "#     \"If the context isn't useful, return the original answer.\"\n",
    "# )\n",
    "# DEFAULT_REFINE_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "#     template=DEFAULT_REFINE_PROMPT_TMPL,\n",
    "# )\n",
    "\n",
    "# llm_chain = LLMChain(prompt=DEFAULT_REFINE_PROMPT, llm=\n",
    "#                      HuggingFaceHub(\n",
    "#                          huggingfacehub_api_token=hf_token,\n",
    "#                          repo_id=\"google/flan-t5-xl\",\n",
    "#                          model_kwargs={\"temperature\":0}), verbose=True)\n",
    "\n",
    "# print(llm_chain.run({\"question\": question, \"context_str\": context_str, \"existing_answer\": existing_answer}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}