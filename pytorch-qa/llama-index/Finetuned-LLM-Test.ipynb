{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97db4fbe",
   "metadata": {},
   "source": [
    "### Description\n",
    "**Model:** shrinath-suresh/alpaca-lora-all-7B<br />\n",
    "**Tools:** PyTorch Index, HF Dataset<br />\n",
    "**With COT:** Yes<br />\n",
    "**Chat History:** Yes<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5387e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c447bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4f19e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import torch\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from dotenv import load_dotenv\n",
    "import langchain \n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import pipeline, TextStreamer, TextIteratorStreamer, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, GPTVectorStoreIndex, PromptHelper, load_index_from_storage, StorageContext, ServiceContext, LangchainEmbedding, SimpleDirectoryReader\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import Tool, AgentOutputParser\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import tracing_enabled\n",
    "from langchain.agents import initialize_agent, LLMSingleActionAgent, StructuredChatAgent, ConversationalChatAgent, ConversationalAgent, AgentExecutor, ZeroShotAgent, AgentType\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationStringBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory.chat_memory import ChatMessageHistory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e5e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set number of output tokens\n",
    "num_output = 512\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a3a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n",
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b8f21a612f4e15959e98b775427b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab244caa99354001943e228e86358158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api = \"\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "hf_hub.login(token=api)\n",
    "\n",
    "## loading llama base model and configuring it with adapter\n",
    "\n",
    "base_model_name = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"shrinath-suresh/alpaca-lora-all-7B\",low_cpu_mem_usage=True, use_auth_token=api,device_map=\"auto\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"shrinath-suresh/alpaca-lora-7b-answer-summary\",low_cpu_mem_usage=True, use_auth_token=api,device_map=\"auto\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"shrinath-suresh/alpaca-lora-7b-context-summary\",low_cpu_mem_usage=True, use_auth_token=api,device_map=\"auto\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"tloen/alpaca-lora-7b\",low_cpu_mem_usage=True, use_auth_token=api,device_map=\"auto\")\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = 'shrinath-suresh/alpaca-lora-all-7B'\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        print(\":::::::::::::::::::::::::::\", prompt)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "        # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "        response = model.generate(**inputs, streamer=streamer, top_p=0.75, max_new_tokens=num_output)\n",
    "        response = tokenizer.decode(response[0])\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a68a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer([\"How can one do model parallel training in PyTorch\"], return_tensors=\"pt\")\n",
    "\n",
    "# # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "# response = model.generate(**inputs, streamer=streamer, top_p=0.75, max_new_tokens=num_output)\n",
    "# response = tokenizer.decode(response[0])\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c2a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-xl\n",
      "Load pretrained SentenceTransformer: hkunlp/instructor-xl\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "HF_embed_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "embed_model = LangchainEmbedding(HF_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef824294",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09b759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs_512\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs_512\"),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs_512\"),\n",
    ")\n",
    "new_index = load_index_from_storage(storage_context, service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ce1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = new_index.as_query_engine(service_context=service_context)\n",
    "# response = query_engine.query(\"How can one do model parallel training in PyTorch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3f933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN=\"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\"\n",
    "\n",
    "from transformers.tools import Tool\n",
    "from huggingface_hub import list_models, ModelFilter\n",
    "import huggingface_hub as hf_hub\n",
    "import re\n",
    "\n",
    "class get_query(BaseModel):\n",
    "    query: str = Field(default=None, description=\"The name of the model and task for which you want the datasets.\")\n",
    "\n",
    "@tool\n",
    "def get_datasets(query: str) -> str:\n",
    "    \"\"\"Fetches dataset for a given model and task\"\"\"\n",
    "    print(\"Log::::::::::::::::::::::::::::\", query, type(query))\n",
    "    pattern = r'model=(.*?)\\s*(?:and\\s*)?task=(.*?)$'\n",
    "    match = re.search(pattern, query)\n",
    "    model_name = match.group(1)\n",
    "    task = match.group(2)\n",
    "    print(\"Log::::::::::::::::::::::::::::\", model_name, task)\n",
    "\n",
    "    filter = ModelFilter(library=\"pytorch\", model_name=model_name, task=task)\n",
    "\n",
    "    models = list_models(filter=filter, token=API_TOKEN)    \n",
    "    if not models:\n",
    "        return f\"{model_name} Not Found!\"\n",
    "    model = [model for model in models if model.id == model_name]\n",
    "    datasets = [string.split(\":\")[1] for string in model[0].tags if \"dataset:\" in string]\n",
    "    if not datasets:\n",
    "        return f\"No Datasets available for {model_name}\"\n",
    "    print(\"Log::::::::::::::::::::::::::::\",\",\".join(datasets))\n",
    "    return \",\".join(datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e700c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=get_datasets,\n",
    "        name=\"Get Model Datasets\",\n",
    "        args_schema=get_query,\n",
    "        return_direct=True,\n",
    "        description=f\"A search engine useful for answering questions about fetching datasets. For example the input should be like model=model and task=task. Do not use this tool with the same input/query\",\n",
    "    ),\n",
    "]\n",
    "index_tools = [\n",
    "    LlamaIndexTool.from_tool_config(\n",
    "        IndexToolConfig(\n",
    "            name = \"PyTorch Index\",\n",
    "            query_engine=new_index.as_query_engine(similarity_top_k=3, response_mode=\"compact\", service_context=service_context),\n",
    "            description=f\"useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query\",\n",
    "            return_direct=True\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ALL_TOOLS = index_tools\n",
    "ALL_TOOLS = search_tools+index_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "794cf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TOOLS[0].description = ALL_TOOLS[0].description.split(' - ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5219f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='A search engine useful for answering questions about fetching datasets. For example the input should be like model=model and task=task. Do not use this tool with the same input/query', metadata={'index': 0}), Document(page_content='useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query', metadata={'index': 1})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='Get Model Datasets', description='A search engine useful for answering questions about fetching datasets. For example the input should be like model=model and task=task. Do not use this tool with the same input/query', args_schema=<class '__main__.get_query'>, return_direct=True, verbose=False, callbacks=None, callback_manager=None, func=StructuredTool(name='get_datasets', description='get_datasets(query: str) -> str - Fetches dataset for a given model and task', args_schema=<class 'pydantic.main.get_datasetsSchemaSchema'>, return_direct=False, verbose=False, callbacks=None, callback_manager=None, func=<function get_datasets at 0x7f08602bb760>, coroutine=None), coroutine=None),\n",
       " LlamaIndexTool(name='PyTorch Index', description='useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, query_engine=<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7f08198859c0>, return_sources=False)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [Document(page_content=t.description, metadata={\"index\": i}) for i, t in enumerate(ALL_TOOLS)]\n",
    "print(docs)\n",
    "vector_store = FAISS.from_documents(docs, HF_embed_model)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "def get_tools(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\n",
    "\n",
    "get_tools(\"give me the supported datasets for the model=bert-base-uncased and task=fill-mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6af4f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from typing import Callable\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "output_parser = CustomOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc78bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_with_history = \"\"\"Act like you are an expert PyTorch Engineer and provide answers to these questions from the developer community. \n",
    "# If you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know. \n",
    "\n",
    "# You have access to the following tools:\n",
    "\n",
    "# {tools}\n",
    "\n",
    "# Previous conversation history:\n",
    "# {history}\n",
    "\n",
    "# New question: {input}\n",
    "# {agent_scratchpad}\"\"\"\n",
    "\n",
    "# template_with_history = \"\"\"Act like you are an expert PyTorch Engineer and provide answers to these questions from the developer community. \n",
    "# If you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know. \n",
    "\n",
    "\n",
    "# Previous conversation history:\n",
    "# {history}\n",
    "\n",
    "# New question: {input}\"\"\"\n",
    "\n",
    "\n",
    "# Set up the base template\n",
    "template_with_history = \"\"\"Act like you are an expert PyTorch Engineer and provide answers to these questions from the developer community. \n",
    "If you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know. \n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 1 times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools_getter: Callable\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        ############## NEW ######################\n",
    "        tools = self.tools_getter(kwargs[\"input\"])\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools_getter=get_tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32ad8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = get_tools(\"\")\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "# tools = []\n",
    "# tool_names = []\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# llm = OpenAI(streaming=True, temperature=0, callbacks=[StreamingStdOutCallbackHandler()], openai_api_key=\"sk-\")\n",
    "llm = CustomLLM()\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "\n",
    "agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=[\"\\nObservation: \"], allowed_tools=tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff651332",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = uuid.uuid4()\n",
    "\n",
    "message_history = RedisChatMessageHistory(str(session_id), 'redis://localhost:6379/0', ttl=600)\n",
    "memory = ConversationBufferWindowMemory(k=3, memory_key=\"history\", return_messages=True, chat_memory=message_history)\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, streaming=True, tools=tools, verbose=False, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e97a5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d557f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"How do I check if PyTorch is using the GPU?\",\n",
    "    \"How do I save a trained model in PyTorch?\",\n",
    "    \"What does .view() do in PyTorch?\",\n",
    "    \"Why do we need to call zero_grad() in PyTorch?\",\n",
    "    \"How do I print the model summary in PyTorch?\",\n",
    "    \"How do I initialize weights in PyTorch?\",\n",
    "    \"What does model.eval() do in pytorch?\",\n",
    "    \"What's the difference between reshape and view in pytorch?\",\n",
    "    \"What does model.train() do in PyTorch?\",\n",
    "    \"What does .contiguous() do in PyTorch?\",\n",
    "    \"Why do we \"\"pack\"\" the sequences in PyTorch?\",\n",
    "    \"Check the total number of parameters in a PyTorch model\",\n",
    "    \"RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\",\n",
    "    \"pytorch - connection between loss.backward() and optimizer.step()\",\n",
    "    \"PyTorch preferred way to copy a tensor\",\n",
    "    \"Pytorch tensor to numpy array\",\n",
    "    \"Pytorch, what are the gradient arguments\",\n",
    "    \"How to fix RuntimeError 'Expected object of scalar type Float but got scalar type Double for argument'?\",\n",
    "    \"What does the gather function do in pytorch in layman terms?\",\n",
    "    \"How to avoid \"\"CUDA out of memory\"\" in PyTorch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "befa73fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::::::::::::::::::::::::::: Act like you are an expert PyTorch Engineer and provide answers to these questions from the developer community. \n",
      "If you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know. \n",
      "\n",
      "You have access to the following tools:\n",
      "\n",
      "PyTorch Index: useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query\n",
      "Get Model Datasets: A search engine useful for answering questions about fetching datasets. For example the input should be like model=model and task=task. Do not use this tool with the same input/query\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [PyTorch Index, Get Model Datasets]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat 1 times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
      "\n",
      "Previous conversation history:\n",
      "[]\n",
      "\n",
      "New question: How to check if  pytorch is using gpu\n",
      "\n",
      "\n",
      "Answer:\n",
      "\n",
      "import torch\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "   print('using gpu')\n",
      "else:\n",
      "   print('not using gpu')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
      "\n",
      "Previous conversation history:\n",
      "[]\n",
      "\n",
      "New question: How to check if  pytorch is using gpu\n",
      "\n",
      "Answer:\n",
      "\n",
      "import torch\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "    print('using gpu')\n",
      "else:\n",
      "    print('not using gpu')\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "response = agent_chain.run(input=\"How to check if  pytorch is using gpu\")\n",
    "print(\"=========================================\")\n",
    "print(response)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc836b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978642e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7554d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e2790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea2b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
