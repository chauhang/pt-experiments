{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7b88a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install peft InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc78e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/paths.py:98: UserWarning: /opt/conda did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA_SETUP: Detected CUDA version 118\n",
      "CUDA_SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, TextStreamer, LlamaTokenizer, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GithubRepositoryReader, GPTVectorStoreIndex, GPTTreeIndex, GPTListIndex, PromptHelper, SimpleDirectoryReader, load_index_from_storage, StorageContext, ServiceContext, LangchainEmbedding\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, LlamaCppEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c05c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 1024\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b5d27",
   "metadata": {},
   "source": [
    "## Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40dfb691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0c910e040b4b8797ffd22789e84bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "api = \"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\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "hf_hub.login(token=api)\n",
    "\n",
    "## loading llama base model and configuring it with adapter\n",
    "\n",
    "base_model_name = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            'shrinath-suresh/alpaca-lora-7b-answer-summary',\n",
    "#             'shrinath-suresh/alpaca-lora-all-7b-delta',\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True\n",
    "        )\n",
    "class CustomLLM(LLM):\n",
    "    model_name = 'shrinath-suresh/alpaca-lora-7b-answer-summary'\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "        # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "        response = model.generate(**inputs, streamer=streamer, top_p=0.75, max_new_tokens=num_output)\n",
    "        response = tokenizer.decode(response[0])\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa001f0",
   "metadata": {},
   "source": [
    "## Define Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ee1525",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: hkunlp/instructor-xl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "embed_model = LangchainEmbedding(HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fedf70",
   "metadata": {},
   "source": [
    "## Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39d05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd43dfe",
   "metadata": {},
   "source": [
    "## Read Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea2bc1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PyTorch Docs\n",
    "input_files = []\n",
    "for path, subdirs, files in os.walk('/home/ubuntu/text'):\n",
    "    for name in files:\n",
    "        file = os.path.join(path, name)\n",
    "        input_files.append(file)\n",
    "docs = SimpleDirectoryReader(input_dir=\"/home/ubuntu/text\", recursive=True, file_extractor={\".txt\": MarkdownReader()}, file_metadata=set_metadata).load_data()\n",
    "nodes = parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010deffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 6043 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e104719",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist('./pytorch_docs_1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddca56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964fcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"How to load a model from torch hub?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef7aef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The answer is:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.hub\n",
      "\n",
      "# Load a model from torch hub\n",
      "model = torch.hub.load(url='https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
      "\n",
      "# Run the model\n",
      "model(torch.randn(1, 3, 224, 224))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.hub\n",
      "\n",
      "# Load a model from torch hub\n",
      "model = torch.hub.load(url='https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
      "\n",
      "# Run the model\n",
      "model(torch.randn(1, 3, 224, 224))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Output\n",
      "\n",
      "```\n",
      "Loaded model from https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1251 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ⁇  Context information is below. \n",
      "---------------------\n",
      "*Any*]\n",
      "\n",
      "   -[ Example ]-\n",
      "\n",
      "   >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
      "\n",
      "\n",
      "Running a loaded model:\n",
      "-----------------------\n",
      "\n",
      "Note that \"*args\" and \"**kwargs\" in \"torch.hub.load()\" are used to\n",
      "**instantiate** a model. After you have loaded a model, how can you\n",
      "find out what you can do with the model? A suggested workflow is\n",
      "\n",
      "* \"dir(model)\" to see all available methods of the model.\n",
      "\n",
      "* \"help(model.foo)\" to check what arguments \"model.foo\" takes to run\n",
      "\n",
      "To help users explore without referring to documentation back and\n",
      "forth, we strongly recommend repo owners make function help messages\n",
      "clear and succinct. It's also helpful to include a minimal working\n",
      "example.\n",
      "\n",
      "\n",
      "Where are my downloaded models saved?\n",
      "-------------------------------------\n",
      "\n",
      "The locations are used in the order\n",
      "\n",
      "model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
      "\n",
      "\n",
      "Important Notice\n",
      "----------------\n",
      "\n",
      "* The published models should be at least in a branch/tag. It can't be\n",
      "  a random commit.\n",
      "\n",
      "\n",
      "Loading models from Hub\n",
      "=======================\n",
      "\n",
      "Pytorch Hub provides convenient APIs to explore all available models\n",
      "in hub through \"torch.hub.list()\", show docstring and examples through\n",
      "\"torch.hub.help()\" and load the pre-trained models using\n",
      "\"torch.hub.load()\".\n",
      "\n",
      "torch.hub.list(github, force_reload=False, skip_validation=False, trust_repo=None)\n",
      "\n",
      "   List all callable entrypoints available in the repo specified by\n",
      "   \"github\".\n",
      "\n",
      "   Parameters:\n",
      "      * **github** (*str*) -- a string with format\n",
      "        \"repo_owner/repo_name[:ref]\" with an optional ref (tag or\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the question: How to load a model from torch hub?\n",
      "\n",
      "The answer is:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.hub\n",
      "\n",
      "# Load a model from torch hub\n",
      "model = torch.hub.load(url='https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
      "\n",
      "# Run the model\n",
      "model(torch.randn(1, 3, 224, 224))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Example\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.hub\n",
      "\n",
      "# Load a model from torch hub\n",
      "model = torch.hub.load(url='https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
      "\n",
      "# Run the model\n",
      "model(torch.randn(1, 3, 224, 224))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Output\n",
      "\n",
      "```\n",
      "Loaded model from https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    response = query_engine.query(query)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c138a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
