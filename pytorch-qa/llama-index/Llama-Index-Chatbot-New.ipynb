{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f6fc54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain  nest_asyncio httpx redis[hiredis] redisearch > 2.4 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360dcd4f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22141977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/paths.py:98: UserWarning: /opt/conda did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA SETUP: CUDA path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA_SETUP: Detected CUDA version 118\n",
      "CUDA_SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from dotenv import load_dotenv\n",
    "import langchain \n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown, display\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "from transformers import pipeline, TextStreamer, TextIteratorStreamer, LlamaTokenizer, LlamaForCausalLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool, create_llama_agent\n",
    "from llama_index import load_index_from_storage, download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, GPTVectorStoreIndex, PromptHelper, StorageContext, ServiceContext, LangchainEmbedding, SimpleDirectoryReader\n",
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter, TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import tracing_enabled\n",
    "from langchain.agents import Agent, initialize_agent, StructuredChatAgent, ConversationalChatAgent, ConversationalAgent, AgentExecutor, ZeroShotAgent, AgentType\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory.chat_memory import ChatMessageHistory\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4f3b6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88844d43",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 2048\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bc819",
   "metadata": {},
   "source": [
    "### HF Pipeline vicuna 13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c174e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"jagadeesh/vicuna-13b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=False, low_cpu_mem_usage=True, device_map=\"auto\", max_memory={0:\"18GB\",1:\"18GB\",2:\"18GB\",3:\"18GB\",\"cpu\":\"10GB\"}, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer, max_new_tokens=num_output, device_map=\"auto\"\n",
    ")\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115a527",
   "metadata": {},
   "source": [
    "### Custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56936c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = \"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\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "\n",
    "# import huggingface_hub as hf_hub\n",
    "\n",
    "# hf_hub.login(token=api)\n",
    "\n",
    "# ## loading llama base model and configuring it with adapter\n",
    "\n",
    "# base_model_name = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "# base_model = LlamaForCausalLM.from_pretrained(\n",
    "#             base_model_name,\n",
    "#             torch_dtype=torch.float16,\n",
    "#             device_map=\"auto\",\n",
    "#         )\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#             base_model,\n",
    "#             'shrinath-suresh/alpaca-lora-7b-answer-summary',\n",
    "# #             'shrinath-suresh/alpaca-lora-all-7b-delta',\n",
    "#             torch_dtype=torch.float16,\n",
    "#             load_in_8bit=True\n",
    "#         )\n",
    "# class CustomLLM(LLM):\n",
    "#     model_name = 'shrinath-suresh/alpaca-lora-7b-answer-summary'\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "#         # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "#         response = model.generate(**inputs, streamer=streamer, top_p=0.75, max_new_tokens=num_output)\n",
    "#         response = tokenizer.decode(response[0])\n",
    "#         return response\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         return {\"name_of_model\": self.model_name}\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom\"\n",
    "\n",
    "# llm_predictor = LLMPredictor(llm=CustomLLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee1525",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "embed_model = LangchainEmbedding(HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5412cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94cc49",
   "metadata": {},
   "source": [
    "## Generate Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5784e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "# sentence_parser = SimpleNodeParser(text_splitter=sentence_splitter)\n",
    "text_parser = SimpleNodeParser(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_metadata(filename):\n",
    "    return {\"source\": filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39496940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PyTorch Docs\n",
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "docs = SimpleDirectoryReader(input_dir=\"/home/ubuntu/text\", recursive=True, file_extractor={\".txt\": MarkdownReader()}, file_metadata=set_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown(docs[6].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9714442",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nodes = text_parser.get_nodes_from_documents(docs)\n",
    "# sentence_nodes = sentence_parser.get_nodes_from_documents([docs[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff33d5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 2795091 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 2795091 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTVectorStoreIndex(text_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "087c424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist('./pytorch_docs_1024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416ea7f",
   "metadata": {},
   "source": [
    "## Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2890eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"./pytorch_docs_1024\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"./pytorch_docs_1024\"),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"./pytorch_docs_1024\"),\n",
    ")\n",
    "new_index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324e9b8",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48e18e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 10 tokens\n",
      "> [retrieve] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=Node(text=\"default_weight_observer\\n***********************\\n\\ntorch.quantization.observer.default_weight_observer\\n\\n   alias of functools.partial(<class\\n   'torch.ao.quantization.observer.MinMaxObserver'>,\\n   dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){}\\n\", doc_id='fe31b4c6-5566-4109-ab8a-5d2ddbb74336', embedding=None, doc_hash='7d61a55edccae685f62924d4cb75ca8f905e58c64f074891ebfdd7856edbbbda', extra_info={'source': '/home/ubuntu/text/generated/torch.quantization.observer.default_weight_observer.txt'}, node_info={'start': 0, 'end': 252}, relationships={<DocumentRelationship.SOURCE: '1'>: 'd99196ae-b1e2-430f-b3c3-617384f6cc82'}), score=0.8169193434662836),\n",
       " NodeWithScore(node=Node(text='Linear\\n******\\n\\nclass torch.ao.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None, device=None, dtype=None)\\n\\n   A linear module attached with FakeQuantize modules for weight, used\\n   for quantization aware training.\\n\\n   We adopt the same interface as *torch.nn.Linear*, please see\\n   https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for\\n   documentation.\\n\\n   Similar to *torch.nn.Linear*, with FakeQuantize modules initialized\\n   to default.\\n\\n   Variables:\\n      **weight** (*torch.Tensor*) -- fake quant module for weight\\n\\n   classmethod from_float(mod)\\n\\n      Create a qat module from a float module or qparams_dict Args:\\n      *mod* a float module, either produced by torch.ao.quantization\\n      utilities or directly from user\\n', doc_id='da43cd11-a098-4954-b498-f4881b749b80', embedding=None, doc_hash='f970264d55d7e2a8780b33e487c607239f366393592f86a59b98b12f281712c9', extra_info={'source': '/home/ubuntu/text/generated/torch.ao.nn.qat.Linear.txt'}, node_info={'start': 0, 'end': 751}, relationships={<DocumentRelationship.SOURCE: '1'>: '3ffa1559-d65e-4ed0-9793-54167f794ecb'}), score=0.8076286203524433),\n",
       " NodeWithScore(node=Node(text=\"Conv3d\\n******\\n\\nclass torch.ao.nn.qat.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None, device=None, dtype=None)\\n\\n   A Conv3d module attached with FakeQuantize modules for weight, used\\n   for quantization aware training.\\n\\n   We adopt the same interface as *torch.nn.Conv3d*, please see https\\n   ://pytorch.org/docs/stable/nn.html?highlight=conv3d#torch.nn.Conv3d\\n   for documentation.\\n\\n   Similar to *torch.nn.Conv3d*, with FakeQuantize modules initialized\\n   to default.\\n\\n   Variables:\\n      **weight_fake_quant** -- fake quant module for weight\\n\", doc_id='6d3cb458-fef6-49f8-aea4-93e958f8568a', embedding=None, doc_hash='eee4ad07be7f3b39237b4b31488f7ec353603e7083869be2f053a8c25eb3a6bc', extra_info={'source': '/home/ubuntu/text/generated/torch.ao.nn.qat.Conv3d.txt'}, node_info={'start': 0, 'end': 633}, relationships={<DocumentRelationship.SOURCE: '1'>: 'e4718ff3-d079-47e9-b85b-cb665987e539'}), score=0.8067179666609193)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.vector_stores.types import (\n",
    "    MetadataFilters,\n",
    "    VectorStoreQuery,\n",
    "    VectorStoreQueryMode,\n",
    ")\n",
    "\n",
    "prompt =f\"{queries[3]}\"\n",
    "retriever_engine=new_index.as_retriever(similarity_top_k=3, service_context=service_context, response_mode='simple_summarize')\n",
    "response = retriever_engine.retrieve(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e5b72",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a8695924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.langchain_helpers.agents import LlamaIndexTool, IndexToolConfig\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=new_index.as_query_engine(similarity_top_k=6, vector_store_query_mode=\"svm\", service_context=service_context, response_mode='simple_summarize'), \n",
    "    name=f\"PyTorch Docs\",\n",
    "    description=f\"useful for when you want to answer queries about pytorch.\",\n",
    "    tool_kwargs={\"return_direct\": False, \"return_sources\": True},\n",
    ")\n",
    "\n",
    "toolkit = LlamaToolkit(\n",
    "    index_configs=[tool_config],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de914caa",
   "metadata": {},
   "source": [
    "\n",
    "### Enable caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f8a6df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.llm_cache = RedisSemanticCache(\n",
    "#     redis_url=\"redis://localhost:6379\",\n",
    "#     embedding=HuggingFaceEmbeddings()\n",
    "# )\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d363ffb",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "73156763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field, validator\n",
    "# from typing import List\n",
    "\n",
    "# class Result(BaseModel):\n",
    "#     answer: str = Field(description=\"Answer to the question\")\n",
    "#     source: str = Field(description=\"list of names of films they starred in\")\n",
    "        \n",
    "# actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "# parser = PydanticOutputParser(pydantic_object=Result)\n",
    "\n",
    "# from langchain.output_parsers import OutputFixingParser\n",
    "# new_parser = OutputFixingParser.from_llm(parser=parser, llm=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e7543",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151df185",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"Act like you are an expert PyTorch Engineer and provide answers to these questions from the developer community. If you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know.\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "\n",
    "You have access to the following tools:\"\"\"\n",
    "FORMAT_INSTRUCTIONS = \"\"\"To use a tool, please use the following format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "{ai_prefix}: [your response here]\n",
    "```\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"Given the context information and not prior knowledge, answer the question\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "{human_prefix}: {input}\n",
    "{ai_prefix}:\n",
    "\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ConversationalAgent.create_prompt(\n",
    "    tools=toolkit.get_tools(),\n",
    "#     prefix=PREFIX,\n",
    "#     format_instructions=FORMAT_INSTRUCTIONS,\n",
    "#     suffix=SUFFIX,\n",
    "    input_variables=[\"agent_scratchpad\", \"chat_history\", \"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920cba4",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1df40acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "# class MyCustomHandler(BaseCallbackHandler):\n",
    "#     def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "#         print(f\"My custom handler, token: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701a8f2",
   "metadata": {},
   "source": [
    "### Create llm chain and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "173a91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_chain = LLMChain(llm=CustomLLM(), prompt=prompt)\n",
    "\n",
    "agent = StructuredChatAgent(llm_chain=llm_chain, tools=toolkit.get_tools(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fff648",
   "metadata": {},
   "source": [
    "### Create agent chain and session history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "42e35441",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session_id = uuid.uuid4()\n",
    "\n",
    "message_history = RedisChatMessageHistory(str(session_id), 'redis://localhost:6379/0', ttl=600)\n",
    "convo_memory = ConversationBufferWindowMemory(k=3, memory_key=\"chat_history\", return_messages=False, chat_memory=message_history)\n",
    "\n",
    "# agent_chain = create_llama_chat_agent(toolkit, llm=hf_pipeline, memory=convo_memory, return_sources=True, streaming=True, verbose=False)\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=toolkit.get_tools(),max_iterations=1, early_stopping_method=\"generate\", verbose=False, return_sources=True, memory=convo_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ad452",
   "metadata": {},
   "source": [
    "### Run query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cd0cc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"How do I check if PyTorch is using the GPU?\",\n",
    "           \"How do I save a trained model in PyTorch?\",\n",
    "           \"What does .view() do in PyTorch?\",\n",
    "           \"Why do we need to call zero_grad() in PyTorch?\",\n",
    "           \"How do I print the model summary in PyTorch?\",\n",
    "           \"How do I initialize weights in PyTorch?\",\n",
    "           \"What does model.eval() do in pytorch?\",\n",
    "           \"What's the difference between reshape and view in pytorch?\",\n",
    "           \"What does model.train() do in PyTorch?\",\n",
    "           \"What does .contiguous() do in PyTorch?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ddf3fe19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> PyTorch Docs: useful for when you want to answer queries about pytorch.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [PyTorch Docs]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Given the context information and not prior knowledge, answer the question\n",
      "\n",
      "Previous conversation history:\n",
      "Human: Why do we need to call zero_grad() in PyTorch?\n",
      "AI: \n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [PyTorch Docs]\n",
      "Action Input: the input to the action\n",
      "\n",
      "Human: Why do we need to call zero_grad() in PyTorch?\n",
      "AI: \n",
      "Human: Why do we need to call zero_grad() in PyTorch?\n",
      "AI: \n",
      "Please respond with the appropriate action for the AI to take.\n",
      "Human: Why do we need to call zero_grad() in PyTorch?\n",
      "AI: \n",
      "\n",
      "New input: Why do we need to call zero_grad() in PyTorch?\n",
      "Observation: the result of the action\n",
      "\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: \n",
      "Please respond with the appropriate action for the AI to  take.</s><s>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent_chain.run(input=queries[3])\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import ResponseEvaluator\n",
    "evaluator = ResponseEvaluator(service_context=service_context)\n",
    "\n",
    "eval_result = evaluator.evaluate(response)\n",
    "eval_result = evaluator.evaluate_source_nodes(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3645b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
