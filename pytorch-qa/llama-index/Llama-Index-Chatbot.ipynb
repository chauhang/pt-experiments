{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7a9c13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6fc54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain  nest_asyncio httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe332c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import ConversationalAgent\n",
    "# dir(ConversationalAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c613ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22141977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import pipeline, TextStreamer\n",
    "\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, PromptHelper, load_index_from_storage, StorageContext, ServiceContext, LangchainEmbedding\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4f3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88844d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 512\n",
    "# set number of output tokens\n",
    "num_output = 128\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c63946",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ee1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94cc49",
   "metadata": {},
   "source": [
    "## Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39496940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir='./storage')\n",
    "rss_feed_index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./pytorch_vector')\n",
    "pytorch_index = load_index_from_storage(storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2890eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "indices.append(rss_feed_index)\n",
    "indices.append(pytorch_index)\n",
    "# indices.append(github_site_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c95c2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac09ba515444316831585bfa72588a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 308 tokens\n",
      "> [get_response] Total LLM token usage: 308 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1065 tokens\n",
      "> [get_response] Total LLM token usage: 1065 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cbd75ab8a9456cbcfe21db269887af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 1 chunks\n",
      "> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 363 tokens\n",
      "> [get_response] Total LLM token usage: 363 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1128 tokens\n",
      "> [get_response] Total LLM token usage: 1128 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "CPU times: user 1min 49s, sys: 1.03 s, total: 1min 50s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = f\"\"\"Write a concise summary for the following question:\n",
    "\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "indices_summary = []\n",
    "for index in indices:\n",
    "    index_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "    summary = index_engine.query(\n",
    "        f\"What is a summary of this document?\")\n",
    "    indices_summary.append(str(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed10a68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThe summary of this document is at the level of a high school student, but it is clear and contains the main ideas from the conversation.\\n\\nThe patient was diagnosed with a number of medical issues (Heart Valve problem, hyperthyroidism, joint problems, and sleep apnea) and the doctor recommended that the patient seek additional medical care for these issues.', '\\nThe document explains two main NLP components that play a role in automating the creation of clinical documentation. The first component, Automatic Speech Recognition (ASR), is used to translate speech into text. It takes the audio recording of the encounter and generates a conversation transcription (cf. Figure 2). The second component, Automatic Text Summarization, helps generate summaries from large text documents. This component is responsible for understanding and capturing the nuances and most essential aspects from the transcribed conversation into a final report in narrative form (cf. Figure 3), structured form, or a combination of both.']\n"
     ]
    }
   ],
   "source": [
    "print(indices_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b12e757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    GPTListIndex,\n",
    "    indices,\n",
    "    index_summaries=indices_summary,\n",
    "    service_context=service_context,\n",
    ")\n",
    "root_id = graph.root_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c2558f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decompose transform\n",
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "decompose_transform = DecomposeQueryTransform(\n",
    "    llm_predictor, verbose=True\n",
    ")\n",
    "\n",
    "# define custom retrievers\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "433fc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engines = {}\n",
    "for index in indices:\n",
    "    query_engine = index.as_query_engine()\n",
    "    query_engine = TransformQueryEngine(\n",
    "        query_engine,\n",
    "        query_transform=decompose_transform,\n",
    "        transform_extra_info={'index_summary': index.index_struct.summary},\n",
    "    )\n",
    "    custom_query_engines[index.index_id] = query_engine\n",
    "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# construct query engine\n",
    "graph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef3ad2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool config\n",
    "graph_config = IndexToolConfig(\n",
    "    query_engine=graph_query_engine,\n",
    "    name=f\"Graph Index\",\n",
    "    description=\"useful for when you want to answer queries that require analyzing multiple documents.\",\n",
    "    tool_kwargs={\"return_direct\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8695924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index configs\n",
    "index_configs = []\n",
    "for index in indices:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=3,\n",
    "    )\n",
    "    tool_config = IndexToolConfig(\n",
    "        query_engine=query_engine, \n",
    "        name=f\"Vector Index\",\n",
    "        description=f\"useful for when you want to answer queries\",\n",
    "        tool_kwargs={\"return_direct\": True, \"return_sources\": True},\n",
    "    )\n",
    "    index_configs.append(tool_config)\n",
    "\n",
    "toolkit = LlamaToolkit(\n",
    "    index_configs=index_configs + [graph_config],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1888f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant than answers questions related to PyTorch.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40be0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "memory = GPTIndexChatMemory(\n",
    "    index=GPTListIndex([], service_context=service_context), \n",
    "    memory_key=\"chat_history\", \n",
    "    query_kwargs={\"response_mode\": \"compact\"},\n",
    "    # return_source returns source nodes instead of querying index\n",
    "    return_source=True,\n",
    "    # return_messages returns context in message format\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm=hf_pipeline\n",
    "agent_chain = create_llama_chat_agent(\n",
    "    toolkit,\n",
    "    llm=HuggingFacePipeline(pipeline=generate_text),\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d908230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to check if pytorch is using gpu\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
      "> [get_response] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "=======================================\n",
      "\n",
      "Use `detectron2 lsp detect` to check pytorch model details.\n",
      "\n",
      "Detection result:\n",
      "- using gpu: yes\n",
      "- available gpus: 0\n",
      "- cpu: unknown\n",
      "- memory usage: unknown\n",
      "- disk usage: unknown\n",
      "- register usage: unknown\n",
      "- multiprocessing: unknown\n",
      "- experiment configuration: unknown\n",
      "- optimizer: unknown\n",
      "- compile status: unknown\n",
      "- training status: unknown\n",
      "- test status: unknown\n",
      "\n",
      "> use detectron2 lsp detect\n",
      "\n",
      "Detection result:\n",
      "- using gpu: yes\n",
      "- available gpus: 0\n",
      "- cpu: unknown\n",
      "- memory usage: unknown\n",
      "- disk usage: unknown\n",
      "- register usage: unknown\n",
      "- multiprocessing: unknown\n",
      "- experiment configuration: unknown\n",
      "- optimizer: unknown\n",
      "- compile status: unknown\n",
      "- training status: unknown\n",
      "- test status: unknown\n",
      "\n",
      "Have a nice day!\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "text_input = input()\n",
    "try:\n",
    "    response = agent_chain.run(input=text_input)\n",
    "except Exception as e:\n",
    "    response = str(e)\n",
    "    if not response.startswith(\"Could not parse LLM output: `\"):\n",
    "        raise e\n",
    "    response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
    "print(\"=======================================\")\n",
    "print(str(response))\n",
    "print(\"=======================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12283f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33451e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
