{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c613ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22141977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import pipeline, TextStreamer\n",
    "\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, PromptHelper, load_index_from_storage, StorageContext, ServiceContext, LangchainEmbedding\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4f3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88844d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 512\n",
    "# set number of output tokens\n",
    "num_output = 128\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c63946",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ee1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94cc49",
   "metadata": {},
   "source": [
    "## Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39496940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir='./pytorch_vector')\n",
    "pytorch_index = load_index_from_storage(storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2890eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "index_engine = pytorch_index.as_query_engine(response_mode=\"compact\")\n",
    "summary = index_engine.query(\"How to check if pytorch is using gpu?\")\n",
    "print(str(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7dea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
