{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, ServiceContext, PromptHelper, LLMPredictor, QuestionAnswerPrompt\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama index using custom embeddings and custom llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['knowledgebase/docs/pages.pkl', 'knowledgebase/so/pages.pkl', \n",
    "              'knowledgebase/blogs/pages.pkl', 'knowledgebase/discussion_forum/pages.pkl']\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files = file_paths).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  = 4096.00 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "#download model using\n",
    "#wget https://huggingface.co/Pi3141/alpaca-native-7B-ggml/resolve/397e872bf4c83f4c642317a5bf65ce84a105786e/ggml-model-q4_0.bin\n",
    "\n",
    "llm = LlamaCpp(model_path=\"ggml-model-q4_0.bin\", n_ctx=4096, callback_manager=callback_manager)\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, chunk_size_limit=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3fb2f5a65c4d7a9995bfb438048cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c0894b4a6e4e37a6faaafeaf4b6e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74737901a70942d8b8304f8dcb9b741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a5f49e974e4335b8ed72e547767de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f693f9fc2d41435990e5d7d997a2cab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e94035666694fc990b05199918c6a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84875c1da69144538bd4c8f1e198bb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febae62b987d490dac69dd45a290b49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e248cd88a71e40f2ae92182bc7a5709d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4eb326ea50464983bdb2387b27e588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ab378982c74ee184781cac9b5857ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fb69e01006491296164da63a8baa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6a2f65ef54f1e8d76679cf3e49f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f3e9856fc24358b5f5a3e6b00b09e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3416f678322f4894bff7b26e8a32b26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23d51e36c5e4f58bcd4f9b998047658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6b68ac8665407b8f800136e598089b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578bfff7d89d475c957372639888f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8166b7ff68da42a59044600aaf7171c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1c4d6e120846eeba2ff4013bbfd995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4160e6e79495d9e63255cce495e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f395155285d24d0a83752cb0e948ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5773501e774a0b85ebc737d51f91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921ca815651d4d73bd58560bef1f1cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d060409d86c4ab592372202598190ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da517d947c64b5f9228d9aacac1c420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837762d86cf5416c923c13fa069732e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7f7664fe9f4f91aeb35ed82e0105ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d11b94fb90141e1aff6102ca8096841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac456c4288e4d8fba2a23ab6570de7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4432b89f504053a24bde36e90a3616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e28a826d23f49e9b9ce259a31d15a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c70b3cc5ca4546969ce369d956371e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a12dccd65944a378c7672615a0c8e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8169033762241a181095be6d0d11cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c428f45623b044d9866895217b6d442d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1f0869fc464c2facfd83d316ff10bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd1a8d1005a477baa8a77b0cef7df6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d22a40ee3f845bb97c7ea6dcb2bf9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf446213bdd14866a446588bd2abb9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea20fba1b879467eaae010f9f4a452c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab680a22131146a590aaaca8a23129e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f368886ec1d64273a2240c3e96765235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be938ba47e71464786b73d8cf748c9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389096080bc34f6fbe10ea38fc1b6ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1a92bc039947da92de3012aabd707d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda73476a52a4df4998194dd5c60d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d326c9a0c2404ca17539d4d99330b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68eff79164c849f0a813fb37a1a5e775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df6b0e23454dfe8a595b42678d0cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf5f79efce74c28ab5410f274079c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f99195888b4e3dbbd8f2b6e31b1495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd02b14ad4e247749d267d47071ba532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a31852365e743d7bb56c93b745f1d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb28131ea944609a0da3614eafb30e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad9d5557bd34338847a00636da01049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d75fd16b151430da99b37999dcb218d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0e074283914c819c743e82dd2cb21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ae216903b34e2aa8c9402ada5ffdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e9de2acb4442a3a9bb93d3e69c2946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e671356f7cbe4b22875f0255c2190d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390f9c46a5c44a9ac936bfa926428af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077fa948dd443e8991b95997a564783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b4b9379ef421db4ace7c25649770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730a83257c98441b8f448f97ee139321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98187a2a338541c898797b7812cdb6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1349b6afbbb34e5aa356b17efd1be8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d6347d67e94d52b3bc0636022d9a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26085b515bb423d9fe0060b9cf2a1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae0b61dd19547bab28269a3cac6db43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d7b3effe7645368c82b42cbe45a664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e649ce920514070a21ebf008695a1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52af7131163b4a4782aaf3be5a591ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6f8b5a4d2f44208ef633a439cb9943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6a6232ed704a68b37afa0f17b53c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e391d1acfaf49cda086125789f0c192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd8048e9554d66b5c988caac4294d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949987026e5d4462b0e38fbdc7505d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f35fc767534d39ab778c6882f53944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eb5b7235d34eeeb9f5ac41fb456aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037d25524d324068806966bba6e5ff2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116fa0fd70c941c587784c6d414f68a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b5f6d192a49ebbdd0852932d2d0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a8dd5dd33d45aaaceab2b486e3a489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32a6b1d22344be8d68ff1d364ea554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56826c8fe8764d8cb03c4d7244bff77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0f65512b4048b78cce5927b35358e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b332af550a947ef9d15a077eb3dff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091708e34c224666b1c35ae378fab313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9827b83deac54faf9334dd7bf90ed351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d211d782458345f5ada992741788042f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa27d24ac824d749ed109b2041bd8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ae357aa9754378904b54fadc131188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629dfc3fe88044a5a4434d1905ebc3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e97bb412a44ccb88e5e765a59ade6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b8ad6e4dcf4f33897dd89a3787e061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87b48cb0193468a99072be11058a552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c05b80abac447d93219e2d45f6b98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec611b8189e4b51a35842dcf0fbb1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891ee8d37eed49e3969b4accdad825f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d0356f8dc24a0db4fd43717773972b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0825e238426b47c4a1c6e45fdb5b72b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7a95f60f5c4a7da2de3d6253218f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7048e591f36b450bb16f09fdc7a2802c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1bb40e13e54c46be3f8092b8630672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cf83b79848497d999a2ec4a21f46cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed3602dc3184e25bb54425fd7b21f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb361eead7045f9ab0213a418aeb2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e0cefdd92e44abaa0202fd31b20c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3bf6b18e1e47b584db81e3191eff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf2be550c6d40dfafc581fd7be815a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7c6ed6070c4817a76ad06e4a0b263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5368fdbfaded4167881ccbd96dcbe42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac317cab5ab4112a6b41c9f0580571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e00d663048d450086bd729c9f3ea1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf0cb0206694ed1a966e174c5505cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a2f280716464bbeb22259aeb1a8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceddc24ce86b40609820dcc4333122d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67756048b07d4aeaa7004fa84feaac58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee49343c8b749a3b6caba6e2102a158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1edd79fdb9c43f0b05b6b9a87a03723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1b5f39cfdc423aae7cbb2494728689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7567d2147884fc0b440c49b09ce5a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65557e1b89ba436f803f5153e86b944d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96017ed0ecaa44539841c65144e5123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99ae02c4bee424fb4e76a45c5208153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c808a7b1177a4714ae24113c4760fb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e105c59a9dcd4c9ba749b7720cdde2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46220ab239346f1a08f2b85f92e0b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d86a49d7e1423c867a039ca795ee5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aeb51775154418b700ce7b7fdb036f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40566b639cf54690939c3327a54d9716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ef88f342ec4657a95c2bd43aff2aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff100d079c2642ad8f56f5de42f47fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4022b51a4047fba378dbfc0d2eb607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f2c57b36be40aeaff518cbd999cb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f674e610f9b4b0e890890bf4d348351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b42f358c4cd49189779ea377de062b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dbb24ec6e147b7a85b44089308ede0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17d3187521e417eb564dcf2672fdc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90b2b379b63492da2a8973c5b8732c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87940d19765741f1925476c0ae5ce6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463421f686944a7ea6c2bbd8336b7063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62764fd58934953ad7f3c22d4dc3f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3527c8b88034262be08c004d6a596fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939b1b9ef65e4a019e0ba98259230915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258f3933a13e40d0a0ad17510bc810c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd81a60f4f141b6b3ea39818aa2677b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98055f09336a4f67ba8e366f1625f8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75afd2bda9a433da92b4800f57e57fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16677cec73f9453bafea895d3c403887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9e9662a0b94e17a9812f76d090b515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd276b9c0e743cfb6138b6557a7ebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0da4f9aeec24392bb7119cc23b713e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfee593759c7479f8a78a61e4efd631e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80650fe85bfc4912902795df3a6465d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332e10b3cb0c40589945638272de2a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cda60f99d447e3a7974c9bdd998939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61253925fcef4b9193b637c9f14f18ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887bb5229234396af5017f350de2c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafe8bf777594a78b4910ba8074ae588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a16e9a8a924a63b6a6ef6496eff18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68de4b2382574d70b5ab27f64a7bbc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cc558c53e44b2c9201110e703ea1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006206a159b94bd6b73b3525728de969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f01d468aac49f3a6f28135d04b59a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b48d4ed39fd427b81b626ff3c2d489e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993fc20702c84ee395e4ed4ad737b203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d4f48b9ba649a78c283dd17417da14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09130c2059ab46d8a4dcf378c21bc167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0865c802d304709b80d3d6d690f8791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2108c1ef66d4fa8a2f55eee0602ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c7da2bba484ad4a429c6bf1c59b86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bd6fb57c7144ca9292e0b7a1f45f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2eb850f16d401a83297644c36b7781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be1d23cebef4f15b24c4fca0b814fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0758be5fa12e4e5ebba4599e94de12f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6462bbde70e49e88dae4fb4717497aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bc276233de4e06a49a6d0161335c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533265e8ec6544daa17bc82cd7bf3fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77972a176a4d62abdf5e9861b8cc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a54431b874846339532268e41eda661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef5a80d7baa4008b295fcdb8c8facc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918eeb5be48c47de8ed9025202b96a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab6349125274e658016ee66b399bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e807b44718b9426bb532a4b53aa46b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eeeae508e341fbb7b98d457446c8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955d5a7cde2345e3926565a215f7e628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dc065dfea44161b56dd25b7998a296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dba0fd407c4032a918578ef12b9ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b137dfba6f47e88c2a5a7d5724778c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0c6162296649dc8735d6db1913939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0d6a3c5a4242a1b0032c22b5e57d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33620b1af514c4e8900d6f5f6308cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20f359f1bd14058b9de35144795c53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18cd8bffba6440bb18f64a713d5e028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3bff07259743d887dcbd343391edc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07048443d26447dba0e29c0cbe5a4e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c47df0b3054075a95380f151e05393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657acfdc14ee45a09380d29a0637104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abdd061bb9449e9bf51795b41ef1b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dc5b6c49734550a9e8ba0a2c3875f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006fb1f17010455a8fc6daa6e226aa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864ddf96046b4330ac4a581006ed8973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5750dd8d25f84e509b64669f90db49fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e44698b2794a26b0314fe33661b041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943b83ecf1646cc9774353c6970df5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a0d1acfd9b4abc8758f2721d67fcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3308d1afb6e43b4b9c3950a3bfa9fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d29a26b58d40ddbc4df6a809391e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ef86959e2e452195347638ac0f846d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016615163d854acd9144b44c7d71e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb0f62be73948548cd1fae4486822e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb620fca0a04939b5163c6dac13463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc5583ad6dd49a1b6713419b76f5ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5e448bdd4c4ca69ca2d3453bb29a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6deb7a01754671a3058081e442b070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e6482160df4d3b95a683c624e11c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f8f936c2cc4aa6a9a2e8366f754bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db38851bdf42420c953528e6d08bcc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af0394bd13a4898a6bf8d2e6e02ba7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0c878781554763ba67d1c17dbba9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407c111b809b4250811d3b654f7150b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbcc86c87fc47faaa09708e95707230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b76df86b3bc4d12a999b596b0ca1212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c89fbcda1c410da9ae66d9c9806f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3bc9e6d6a54a9e94d5fd4753840255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7dfb5f624f4bf4896d6ac7fd970e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7681925b12614d1086423b130736760f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d09bf379235419a8e1cde5129c500e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0501037aa564739b7c6661bd89829db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fd13577c7f4a58a76887b075ae5920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e138d4746d14f4aa726fe5985d81800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711d7ffe8fc74de1ba171fa33160e1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1965552629fd4e77b37bbfa43f803f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f3ddbced7242148e098cdc8968d2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de66081acc14de39eea6d5a714dc26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2ea33e16cf4693bff55b61a0304939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf566fa6733e471a94cf58e74c77306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415db4202f947d989ea8834ebd38a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fe524171a64012801acd331dd68ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46170034eb2c4f61984cbc314f81fd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfca872f438041da9c9f3a4e61c6e571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8951b18e084ace886252c0141cbf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525c50c3f28452d81f50cc15c3ca6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbd2d03868141208ae0807b8cef35a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b886333ed84fcfba2e328ff2d9d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24991704667d4dbaacaaffbd02f713b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc8db84bf0b4537a68c291835b9e9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ca8f2504f544188d690a4909527678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f89cf35de74291bd56b55f191d0913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de398ee67354e04834c940f3d2edd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c682d90af5a4f299ce997109e8c11bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ade4aee8f704dcab415de30379f3a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753f8edb48174f09b0f377dcc87e5826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72a4fab40a940809937b95be341735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5e16093fa240a59f0902418c3ad7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfe57024fa14733ba5d41dbf7c08e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875e603b33be402983845363755e2354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d3829ece434b63ad89f6bc967ffadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a5a7f2b0524f6a89db6dac2c77e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1787d32a834e45ab5c38bbc0f5ba3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cef6f053b124d2f8b74c751dc02db6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56e71cca29f4473b7bcbc95b3a3217f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aee51e5a08544bca84cc01a00739b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a27af1dfa640c1954115ac9466334c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf3fadbaf1040eb8358f6f30ff06f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73b848bcbea4acf9cd182b51c71a7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b922581d12554be483daa78b1ad402dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25e4618f52042c2a729bd0355ccee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afca260cfaf0459abf53c078d4dad5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97277576eb3c4db7809520734dd5aed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba8e63fc2e9414795672fac85960086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68758c08d5eb46cf91d0c53dabf47869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca57e9456dcf4589a196f18be97f3a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c4b1fd96674504982de0dcc2e401cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdb4e74868049d6ac92150ae35b1124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b57e46541145f0a4cb43542540514b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4f46c7d6a54657ac3f2988caa0dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ab3eb02918406b91b400dad9036f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b87593b5dd4d95b6df95ab5097f0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee80c423a4c4a4297dc4593b61c0503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c5162fb3d457fb2aee7f3c806859c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa7bb7adf664cd3b94040e8f0020f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d1c6b5907d4fa59f2943f1e354c4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c323491d740545c0ac6657df1ae2fc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23472ccb62e04c5b94783523ca546386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee750b6d85840e0b864413ed0c97a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d151ca4bd054467a48cd026dc542809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddec2b116f74e54a5f5e4a1d9684200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34aa4bb01d042dc89d43edbc59157b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc077d483eb400d9b181dc6ebefde1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e0979708de4ab2aeb40967a079d43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b10898cebd944ef867543ba1776acfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8342b74bc443d0a046325a25d876dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c783bd49134d688d2badbd86404eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b40faa2f7f4c77a0fbc6ebd1c17ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377265741a384538a98e40524aaa0a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25252a76260d4bab928f5c611e13a8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46443be7cb7b409b9ccaa9727f6cb47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaf9587352c4c43888e0097536162bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fc0dfd26dd4779b8d51bd2203ad96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec8f4b380724ef38747cb9bb0b849a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5095650dd5e7482c8144626b4919bc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af8b9e7c8844cb9b7f7de1e49f836da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcba808f3a64b188435a8947b3a5acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a04e14864844a9c8ca61aab42d31cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0805261aa93940caaa0cae9d7e160d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f18b38817441ab18e65cdbfb5944e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075c3f263b4048e5807dd62103a86f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50697b1b38b429faf49e5f4340be483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e57f98a8eb4878855728d25a62871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 12376668 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT_TMPL = (\n",
    "    \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Question: {query_str}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do I check if PyTorch is using the GPU?\\n',\n",
       " 'How do I save a trained model in PyTorch?\\n',\n",
       " 'What does .view() do in PyTorch?\\n',\n",
       " 'Why do we need to call zero_grad() in PyTorch?\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so questions\n",
    "df = pd.read_csv('top100questions.csv')\n",
    "\n",
    "queries = df[0:4]['question'].tolist()\n",
    "\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How do I check if PyTorch is using the GPU?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcc1e1461204fef99cf85afee694d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =    62.16 ms /    74 runs   (    0.84 ms per run)\n",
      "llama_print_timings: prompt eval time =  2249.28 ms /    48 tokens (   46.86 ms per token)\n",
      "llama_print_timings:        eval time =  8860.40 ms /    73 runs   (  121.38 ms per run)\n",
      "llama_print_timings:       total time = 11180.86 ms\n",
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =   140.53 ms /   161 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time = 19053.06 ms /   398 tokens (   47.87 ms per token)\n",
      "llama_print_timings:        eval time = 21743.64 ms /   160 runs   (  135.90 ms per run)\n",
      "llama_print_timings:       total time = 40966.18 ms\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4471 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h\u0004hWa\u0001(h\b\n",
      "hP\u0007iperdeine pipelines for training a TensorFlow Lite model. The input\n",
      "   tensor should be the model's parameter or buffer which will be copied\n",
      "   to the appropriate device before running the module.\n",
      "\"\"\"\n",
      "import torch\n",
      "from typing import Any, List\n",
      "\n",
      "def check_pipeline(model):\n",
      "    for d in range(1, len(torch.distributed_utils.get_devices())):\n",
      "        if (d not in torch.distributed_utils.get_devices() or\n",
      "            model.cuda()) :\n",
      "            break\n",
      "    \n",
      "    return torch.distributed_utils.get_pipeline(model)\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[NodeWithScore(node=Node(text='\\x04\\x02\\x01\\x00\\x00\\x00\\x00\\x00](\\x10langchain.schema\\x08Document)}(\\x08__dict__}(\\x0cpage_contentX\\x03\\x00\\x00torch.utils.bottleneck\\n**********************\\n\\n*torch.utils.bottleneck* is a tool that can be used as an initial step\\nfor debugging bottlenecks in your program. It summarizes runs of your\\nscript with the Python profiler and PyTorch\\'s autograd profiler.\\n\\nRun it on the command line with\\n\\n   python -m torch.utils.bottleneck /path/to/source/script.py [args]\\n\\nwhere [args] are any number of arguments to *script.py*, or run\\n\"python -m torch.utils.bottleneck -h\" for more usage instructions.\\n\\nWarning:\\n\\n  Because your script will be profiled, please ensure that it exits in\\n  a finite amount of time.\\n\\nWarning:\\n\\n  Due to the asynchronous nature of CUDA kernels, when running against\\n  CUDA code, the cProfile output and CPU-mode autograd profilers may\\n  not show correct timings: the reported CPU time reports the amount\\n  of time used to launch the kernels but does not include the time the\\n  kernel spent executing on a GPU unless the operation does a\\n  synchronize. Ops that do synchronize appear to be extremely\\x08metadata}\\x06source/https://pytorch.org/docs/stable/bottleneck.htmlsu\\x0e__fields_set__(h\\x08h\\n\\x1c__private_attribute_values__}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00of time used to launch the kernels but does not include the time the\\n  kernel spent executing on a GPU unless the operation does a\\n  synchronize. Ops that do synchronize appear to be extremely\\n  expensive under regular CPU-mode profilers. In these case where\\n  timings are incorrect, the CUDA-mode autograd profiler may be\\n  helpful.\\n\\nNote:\\n\\n  To decide which (CPU-only-mode or CUDA-mode) autograd profiler\\n  output to look at, you should first check if your script is CPU-\\n  bound (\"CPU total time is much greater than CUDA total time\"). If it\\n  is CPU-bound, looking at the results of the CPU-mode autograd\\n  profiler will help. If on the other hand your script spends most of\\n  its time executing on the GPU, then it makes sense to start looking\\n  for responsible CUDA operators in the output of the CUDA-mode\\n  autograd profiler.Of course the reality is much more complicated and\\n  your script might not be in one of those two extremes depending on\\n  the part of the model you\\'re evaluating. If the profiler outputsh\\n}h\\x0ch\\nsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00your script might not be in one of those two extremes depending on\\n  the part of the model you\\'re evaluating. If the profiler outputs\\n  don\\'t help, you could try looking at the result of\\n  \"torch.autograd.profiler.emit_nvtx()\" with \"nvprof\". However, please\\n  take into account that the NVTX overhead is very high and often\\n  gives a heavily skewed timeline. Similarly, \"Intel VTune\\x84 Profiler\"\\n  helps to analyze performance on Intel platforms further with\\n  \"torch.autograd.profiler.emit_itt()\".\\n\\nWarning:\\n\\n  If you are profiling CUDA code, the first profiler that \"bottleneck\"\\n  runs (cProfile) will include the CUDA startup time (CUDA buffer\\n  allocation cost) in its time reporting. This should not matter if\\n  your bottlenecks result in code much slower than the CUDA startup\\n  time.\\n\\nFor more complicated uses of the profilers (like in a multi-GPU case),\\nplease see https://docs.python.org/3/library/profile.html or\\n\"torch.autograd.profiler.profile()\" for more information.h\\n}h\\x0ch\\nsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torch.library\\n*************\\n\\nPython operator registration API provides capabilities for extending\\nPyTorch\\'s core library of operators with user defined operators.\\nCurrently, this can be done in two ways:\\n\\n1. Creating new libraries\\n\\n   * Lets you to register **new operators** and kernels for various\\n     backends and functionalities by specifying the appropriate\\n     dispatch keys. For example,\\n\\n        * Consider registering a new operator \"add\" in your newly\\n          created namespace \"foo\". You can access this operator using\\n          the \"torch.ops\" API and calling into by calling\\n          \"torch.ops.foo.add\". You can also access specific registered\\n          overloads by calling \"torch.ops.foo.add.{overload_name}\".\\n\\n        * If you registered a new kernel for the \"CUDA\" dispatch key\\n          for this operator, then your custom defined function will be\\n          called for CUDA tensor inputs.\\n\\n   * This can be done by creating Library class objects of \"\"DEF\"\"\\n     kind.h\\n}h\\x0c,https://pytorch.org/docs/stable/library.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00for this operator, then your custom defined function will be\\n          called for CUDA tensor inputs.\\n\\n   * This can be done by creating Library class objects of \"\"DEF\"\"\\n     kind.\\n\\n2. Extending existing C++ libraries (e.g., aten)\\n\\n   * Lets you register kernels for **existing operators**\\n     corresponding to various backends and functionalities by\\n     specifying the appropriate dispatch keys.\\n\\n   * This may come in handy to fill up spotty operator support for a\\n     feature implemented through a dispatch key. For example.,\\n\\n        * You can add operator support for Meta Tensors (by\\n          registering function to the \"Meta\" dispatch key).\\n\\n   * This can be done by creating Library class objects of \"\"IMPL\"\"\\n     kind.\\n\\nA tutorial that walks you through some examples on how to use this API\\nis available on Google Colab.\\n\\nWarning:\\n\\n  Dispatcher is a complicated PyTorch concept and having a sound\\n  understanding of Dispatcher is crucial to be able to do anythingh\\n}h\\x0ch%suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00is available on Google Colab.\\n\\nWarning:\\n\\n  Dispatcher is a complicated PyTorch concept and having a sound\\n  understanding of Dispatcher is crucial to be able to do anything\\n  advanced with this API. This blog post is a good starting point to\\n  learn about Dispatcher.\\n\\nclass torch.library.Library(ns, kind, dispatch_key=\\'\\')\\n\\n   A class to create libraries that can be used to register new\\n   operators or override operators in existing libraries from Python.\\n   A user can optionally pass in a dispatch keyname if they only want\\n   to register kernels corresponding to only one specific dispatch\\n   key.\\n\\n   To create a library to override operators in an existing library\\n   (with name ns), set the kind to \"IMPL\". To create a new library\\n   (with name ns) to register new operators, set the kind to \"DEF\".\\n   :param ns: library name :param kind: \"DEF\", \"IMPL\" (default:\\n   \"IMPL\") :param dispatch_key: PyTorch dispatch key (default: \"\")\\n\\n   define(schema, alias_analysis=\\'\\')h\\n}h\\x0ch%suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00:param ns: library name :param kind: \"DEF\", \"IMPL\" (default:\\n   \"IMPL\") :param dispatch_key: PyTorch dispatch key (default: \"\")\\n\\n   define(schema, alias_analysis=\\'\\')\\n\\n      Defines a new operator and its semantics in the ns namespace.\\n\\n      Parameters:\\n         * **schema** -- function schema to define a new operator.\\n\\n         * **alias_analysis** (*optional*) -- Indicates if the\\n           aliasing properties of the operator arguments can be\\n           inferred from the schema (default behavior) or not\\n           (\"CONSERVATIVE\").\\n\\n      Returns:\\n         name of the operator as inferred from the schema.\\n\\n      Example::\\n         >>> my_lib = Library(\"foo\", \"DEF\")\\n         >>> my_lib.define(\"sum(Tensor self) -> Tensor\")\\n\\n   impl(op_name, fn, dispatch_key=\\'\\')\\n\\n      Registers the function implementation for an operator defined in\\n      the library.\\n\\n      Parameters:\\n         * **op_name** -- operator name (along with the overload) or\\n           OpOverload object.h\\n}h\\x0ch%suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x05\\x03\\x00\\x00the library.\\n\\n      Parameters:\\n         * **op_name** -- operator name (along with the overload) or\\n           OpOverload object.\\n\\n         * **fn** -- function that\\'s the operator implementation for\\n           the input dispatch key.\\n\\n         * **dispatch_key** -- dispatch key that the input function\\n           should be registered for. By default, it uses the dispatch\\n           key that the library was created with.\\n\\n      Example::\\n         >>> my_lib = Library(\"aten\", \"IMPL\")\\n         >>> def div_cpu(self, other):\\n         >>>     return self * (1 / other)\\n         >>> my_lib.impl(\"div.Tensor\", \"CPU\")\\n\\nWe have also added some function decorators to make it convenient to\\nregister functions for operators:\\n\\n* \"torch.library.impl()\"\\n\\n* \"torch.library.define()\"h\\n}h\\x0ch%suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00Pipeline Parallelism\\n********************\\n\\nPipeline parallelism was original introduced in the Gpipe  paper and\\nis an efficient technique to train large models on multiple GPUs.\\n\\nWarning:\\n\\n  Pipeline Parallelism is experimental and subject to change.\\n\\n\\nModel Parallelism using multiple GPUs\\n=====================================\\n\\nTypically for large models which don\\'t fit on a single GPU, model\\nparallelism is employed where certain parts of the model are placed on\\ndifferent GPUs. Although, if this is done naively for sequential\\nmodels, the training process suffers from GPU under utilization since\\nonly one GPU is active at one time as shown in the figure below:\\n\\n   [image]The figure represents a model with 4 layers placed on 4\\n   different GPUs (vertical axis). The horizontal axis represents\\n   training this model through time demonstrating that only 1 GPU is\\n   utilized at a time (image source).\\n\\n\\nPipelined Execution\\n===================\\n\\nTo alleviate this problem, pipeline parallelism splits the inputh\\n}h\\x0c-https://pytorch.org/docs/stable/pipeline.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00utilized at a time (image source).\\n\\n\\nPipelined Execution\\n===================\\n\\nTo alleviate this problem, pipeline parallelism splits the input\\nminibatch into multiple microbatches and pipelines the execution of\\nthese microbatches across multiple GPUs. This is outlined in the\\nfigure below:\\n\\n   [image]The figure represents a model with 4 layers placed on 4\\n   different GPUs (vertical axis). The horizontal axis represents\\n   training this model through time demonstrating that the GPUs are\\n   utilized much more efficiently. However, there still exists a\\n   bubble (as demonstrated in the figure) where certain GPUs are not\\n   utilized. (image source).\\n\\n\\nPipe APIs in PyTorch\\n====================\\n\\nclass torch.distributed.pipeline.sync.Pipe(module, chunks=1, checkpoint=\\'except_last\\', deferred_batch_norm=False)\\n\\n   Wraps an arbitrary \"nn.Sequential\" module to train on using\\n   synchronous pipeline parallelism. If the module requires lots of\\n   memory and doesn\\'t fit on a single GPU, pipeline parallelism is ah\\n}h\\x0chIsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00Wraps an arbitrary \"nn.Sequential\" module to train on using\\n   synchronous pipeline parallelism. If the module requires lots of\\n   memory and doesn\\'t fit on a single GPU, pipeline parallelism is a\\n   useful technique to employ for training.\\n\\n   The implementation is based on the torchgpipe paper.\\n\\n   Pipe combines pipeline parallelism with checkpointing to reduce\\n   peak memory required to train while minimizing device under-\\n   utilization.\\n\\n   You should place all the modules on the appropriate devices and\\n   wrap them into an \"nn.Sequential\" module defining the desired order\\n   of execution. If a module does not contain any parameters/buffers,\\n   it is assumed this module should be executed on CPU and appropriate\\n   input tensors to the module are moved to CPU before execution. This\\n   behavior can be overridden by the \"WithDevice\" wrapper which can be\\n   used to explicitly specify which device a module should run on.\\n\\n   Parameters:\\n      * **module** (\"nn.Sequential\") -- sequential module to beh\\n}h\\x0chIsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00used to explicitly specify which device a module should run on.\\n\\n   Parameters:\\n      * **module** (\"nn.Sequential\") -- sequential module to be\\n        parallelized using pipelining. Each module in the sequence has\\n        to have all of its parameters on a single device. Each module\\n        in the sequence has to either be an nn.Module or\\n        \"nn.Sequential\" (to combine multiple sequential modules on a\\n        single device)\\n\\n      * **chunks** (*int*) -- number of micro-batches (default: \"1\")\\n\\n      * **checkpoint** (*str*) -- when to enable checkpointing, one of\\n        \"\\'always\\'\", \"\\'except_last\\'\", or \"\\'never\\'\" (default:\\n        \"\\'except_last\\'\"). \"\\'never\\'\" disables checkpointing completely,\\n        \"\\'except_last\\'\" enables checkpointing for all micro-batches\\n        except the last one and \"\\'always\\'\" enables checkpointing for\\n        all micro-batches.\\n\\n      * **deferred_batch_norm** (*bool*) -- whether to use deferred\\n        \"BatchNorm\" moving statistics (default: \"False\"). If set toh\\n}h\\x0chIsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00all micro-batches.\\n\\n      * **deferred_batch_norm** (*bool*) -- whether', doc_id='c3b89440-3ff0-45a5-886e-5e153e65af70', embedding=None, doc_hash='a784fe9284a44e72a2f6bb315dd35c611d1de6261a47f55ae665cfd0b49783cb', extra_info=None, node_info={'start': 0, 'end': 12514}, relationships={<DocumentRelationship.SOURCE: '1'>: '825cd2e5-8e9b-4cec-a155-97e656f4785d', <DocumentRelationship.NEXT: '3'>: '706d3d2e-033a-4a07-8f39-0af908e321cf'}), score=0.5950323710929573)]\n",
      "---------------------------------------------------------------------------\n",
      "QUERY:  How do I save a trained model in PyTorch?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701d18b0378b422998d158216c836e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =   221.31 ms /   252 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 16557.08 ms /   347 tokens (   47.71 ms per token)\n",
      "llama_print_timings:        eval time = 33875.49 ms /   251 runs   (  134.96 ms per run)\n",
      "llama_print_timings:       total time = 50704.57 ms\n",
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =    99.75 ms /   114 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time = 18651.22 ms /   390 tokens (   47.82 ms per token)\n",
      "llama_print_timings:        eval time = 15169.39 ms /   113 runs   (  134.24 ms per run)\n",
      "llama_print_timings:       total time = 33939.38 ms\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4905 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 13 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh {\n",
      "  function check(url) {\n",
      "    return torch.hub.check_url(url);\n",
      "   }\n",
      "}h\n",
      "We have an opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "This function will take a given URL as an argument, and then check it against the list of trusted URLs in the cache. If it is not present in that list, the behaviour will fall back onto the \"trust_repo=Fals\" option. \n",
      "}h\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[NodeWithScore(node=Node(text='handles \"pretrained\",\\n  alternatively you can put the following logic in the entrypoint\\n  definition.\\n\\n   if pretrained:\\n       # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\\n       dirname = os.path.dirname(__file__)\\n       checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\\n       state_dict = torch.load(checkpoint)\\n       model.load_state_dict(state_dict)\\n\\n       # For checkpoint saved elsewhere\\n       checkpoint = \\'https://download.pytorch.org/models/resnet18-5c106cde.pth\\'\\n       model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\\n\\n\\nImportant Notice\\n----------------\\n\\n* The published models should be at least in a branch/tag. It can\\'t be\\n  a random commit.\\n\\n\\nLoading models from Hub\\n=======================\\n\\nPytorch Hub provides convenient APIs to explore all available models\\nin hub through \"torch.hub.list()\", show docstring and examples throughh\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00Loading models from Hub\\n=======================\\n\\nPytorch Hub provides convenient APIs to explore all available models\\nin hub through \"torch.hub.list()\", show docstring and examples through\\n\"torch.hub.help()\" and load the pre-trained models using\\n\"torch.hub.load()\".\\n\\ntorch.hub.list(github, force_reload=False, skip_validation=False, trust_repo=None)\\n\\n   List all callable entrypoints available in the repo specified by\\n   \"github\".\\n\\n   Parameters:\\n      * **github** (*str*) -- a string with format\\n        \"repo_owner/repo_name[:ref]\" with an optional ref (tag or\\n        branch). If \"ref\" is not specified, the default branch is\\n        assumed to be \"main\" if it exists, and otherwise \"master\".\\n        Example: \\'pytorch/vision:0.10\\'\\n\\n      * **force_reload** (*bool**, **optional*) -- whether to discard\\n        the existing cache and force a fresh download. Default is\\n        \"False\".\\n\\n      * **skip_validation** (*bool**, **optional*) -- if \"False\",h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00the existing cache and force a fresh download. Default is\\n        \"False\".\\n\\n      * **skip_validation** (*bool**, **optional*) -- if \"False\",\\n        torchhub will check that the branch or commit specified by the\\n        \"github\" argument properly belongs to the repo owner. This\\n        will make requests to the GitHub API; you can specify a non-\\n        default GitHub token by setting the \"GITHUB_TOKEN\" environment\\n        variable. Default is \"False\".\\n\\n      * **trust_repo** (*bool**, **str** or **None*) --\\n\\n        \"\"check\"\", \"True\", \"False\" or \"None\". This parameter was\\n        introduced in v1.12 and helps ensuring that users only run\\n        code from repos that they trust.\\n\\n        * If \"False\", a prompt will ask the user whether the repo\\n          should be trusted.\\n\\n        * If \"True\", the repo will be added to the trusted list and\\n          loaded without requiring explicit confirmation.\\n\\n        * If \"\"check\"\", the repo will be checked against the list ofh\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00* If \"True\", the repo will be added to the trusted list and\\n          loaded without requiring explicit confirmation.\\n\\n        * If \"\"check\"\", the repo will be checked against the list of\\n          trusted repos in the cache. If it is not present in that\\n          list, the behaviour will fall back onto the\\n          \"trust_repo=False\" option.\\n\\n        * If \"None\": this will raise a warning, inviting the user to\\n          set \"trust_repo\" to either \"False\", \"True\" or \"\"check\"\".\\n          This is only present for backward compatibility and will be\\n          removed in v2.0.\\n\\n        Default is \"None\" and will eventually change to \"\"check\"\" in\\n        v2.0.\\n\\n   Returns:\\n      The available callables entrypoint\\n\\n   Return type:\\n      list\\n\\n   -[ Example ]-\\n\\n   >>> entrypoints = torch.hub.list(\\'pytorch/vision\\', force_reload=True)\\n\\ntorch.hub.help(github, model, force_reload=False, skip_validation=False, trust_repo=None)\\n\\n   Show the docstring of entrypoint \"model\".\\n\\n   Parameters:h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torch.hub.help(github, model, force_reload=False, skip_validation=False, trust_repo=None)\\n\\n   Show the docstring of entrypoint \"model\".\\n\\n   Parameters:\\n      * **github** (*str*) -- a string with format\\n        <repo_owner/repo_name[:ref]> with an optional ref (a tag or a\\n        branch). If \"ref\" is not specified, the default branch is\\n        assumed to be \"main\" if it exists, and otherwise \"master\".\\n        Example: \\'pytorch/vision:0.10\\'\\n\\n      * **model** (*str*) -- a string of entrypoint name defined in\\n        repo\\'s \"hubconf.py\"\\n\\n      * **force_reload** (*bool**, **optional*) -- whether to discard\\n        the existing cache and force a fresh download. Default is\\n        \"False\".\\n\\n      * **skip_validation** (*bool**, **optional*) -- if \"False\",\\n        torchhub will check that the ref specified by the \"github\"\\n        argument properly belongs to the repo owner. This will make\\n        requests to the GitHub API; you can specify a non-default\\x00\\x01\\x00\\x00\\x00\\x00\\x00h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torchhub will check that the ref specified by the \"github\"\\n        argument properly belongs to the repo owner. This will make\\n        requests to the GitHub API; you can specify a non-default\\n        GitHub token by setting the \"GITHUB_TOKEN\" environment\\n        variable. Default is \"False\".\\n\\n      * **trust_repo** (*bool**, **str** or **None*) --\\n\\n        \"\"check\"\", \"True\", \"False\" or \"None\". This parameter was\\n        introduced in v1.12 and helps ensuring that users only run\\n        code from repos that they trust.\\n\\n        * If \"False\", a prompt will ask the user whether the repo\\n          should be trusted.\\n\\n        * If \"True\", the repo will be added to the trusted list and\\n          loaded without requiring explicit confirmation.\\n\\n        * If \"\"check\"\", the repo will be checked against the list of\\n          trusted repos in the cache. If it is not present in that\\n          list, the behaviour will fall back onto the\\n          \"trust_repo=False\" option.h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00trusted repos in the cache. If it is not present in that\\n          list, the behaviour will fall back onto the\\n          \"trust_repo=False\" option.\\n\\n        * If \"None\": this will raise a warning, inviting the user to\\n          set \"trust_repo\" to either \"False\", \"True\" or \"\"check\"\".\\n          This is only present for backward compatibility and will be\\n          removed in v2.0.\\n\\n        Default is \"None\" and will eventually change to \"\"check\"\" in\\n        v2.0.\\n\\n   -[ Example ]-\\n\\n   >>> print(torch.hub.help(\\'pytorch/vision\\', \\'resnet18\\', force_reload=True))\\n\\ntorch.hub.load(repo_or_dir, model, *args, source=\\'github\\', trust_repo=None, force_reload=False, verbose=True, skip_validation=False, **kwargs)\\n\\n   Load a model from a github repo or a local directory.\\n\\n   Note: Loading a model is the typical use case, but this can also be\\n   used to for loading other objects such as tokenizers, loss\\n   functions, etc.\\n\\n   If \"source\" is \\'github\\', \"repo_or_dir\" is expected to be of theh\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00used to for loading other objects such as tokenizers, loss\\n   functions, etc.\\n\\n   If \"source\" is \\'github\\', \"repo_or_dir\" is expected to be of the\\n   form \"repo_owner/repo_name[:ref]\" with an optional ref (a tag or a\\n   branch).\\n\\n   If \"source\" is \\'local\\', \"repo_or_dir\" is expected to be a path to a\\n   local directory.\\n\\n   Parameters:\\n      * **repo_or_dir** (*str*) -- If \"source\" is \\'github\\', this\\n        should correspond to a github repo with format\\n        \"repo_owner/repo_name[:ref]\" with an optional ref (tag or\\n        branch), for example \\'pytorch/vision:0.10\\'. If \"ref\" is not\\n        specified, the default branch is assumed to be \"main\" if it\\n        exists, and otherwise \"master\". If \"source\" is \\'local\\'  then\\n        it should be a path to a local directory.\\n\\n      * **model** (*str*) -- the name of a callable (entrypoint)\\n        defined in the repo/dir\\'s \"hubconf.py\".\\n\\n      * ***args** (*optional*) -- the corresponding args for callable\\n        \"model\".h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00defined in the repo/dir\\'s \"hubconf.py\".\\n\\n      * ***args** (*optional*) -- the corresponding args for callable\\n        \"model\".\\n\\n      * **source** (*str**, **optional*) -- \\'github\\' or \\'local\\'.\\n        Specifies how \"repo_or_dir\" is to be interpreted. Default is\\n        \\'github\\'.\\n\\n      * **trust_repo** (*bool**, **str** or **None*) --\\n\\n        \"\"check\"\", \"True\", \"False\" or \"None\". This parameter was\\n        introduced in v1.12 and helps ensuring that users only run\\n        code from repos that they trust.\\n\\n        * If \"False\", a prompt will ask the user whether the repo\\n          should be trusted.\\n\\n        * If \"True\", the repo will be added to the trusted list and\\n          loaded without requiring explicit confirmation.\\n\\n        * If \"\"check\"\", the repo will be checked against the list of\\n          trusted repos in the cache. If it is not present in that\\n          list, the behaviour will fall back onto the\\n          \"trust_repo=False\" option.h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00trusted repos in the cache. If it is not present in that\\n          list, the behaviour will fall back onto the\\n          \"trust_repo=False\" option.\\n\\n        * If \"None\": this will raise a warning, inviting the user to\\n          set \"trust_repo\" to either \"False\", \"True\" or \"\"check\"\".\\n          This is only present for backward compatibility and will be\\n          removed in v2.0.\\n\\n        Default is \"None\" and will eventually change to \"\"check\"\" in\\n        v2.0.\\n\\n      * **force_reload** (*bool**, **optional*) -- whether to force a\\n        fresh download of the github repo unconditionally. Does not\\n        have any effect if \"source = \\'local\\'\". Default is \"False\".\\n\\n      * **verbose** (*bool**, **optional*) -- If \"False\", mute\\n        messages about hitting local caches. Note that the message\\n        about first download cannot be muted. Does not have any effect\\n        if \"source = \\'local\\'\". Default is \"True\".\\n\\n      * **skip_validation** (*bool**, **optional*) -- if \"False\",h\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00about first download cannot be muted. Does not have any effect\\n        if \"source = \\'local\\'\". Default is \"True\".\\n\\n      * **skip_validation** (*bool**, **optional*) -- if \"False\",\\n        torchhub will check that the branch or commit specified by the\\n        \"github\" argument properly belongs to the repo owner. This\\n        will make requests to the GitHub API; you can specify a non-\\n        default GitHub', doc_id='b8df2793-1532-4336-ac44-a0de77893adf', embedding=None, doc_hash='d1300ad3a016e07687bcdf4785d68a23ffe15f887220b1a941c0a05893804484', extra_info=None, node_info={'start': 6053841, 'end': 6064364}, relationships={<DocumentRelationship.SOURCE: '1'>: '825cd2e5-8e9b-4cec-a155-97e656f4785d', <DocumentRelationship.PREVIOUS: '2'>: '3d0e3147-ab83-45a1-8778-a82f65fc53cb', <DocumentRelationship.NEXT: '3'>: 'c67ce5ae-369e-4643-bfdb-d5ce7fb0b6cb'}), score=0.5341350569866073)]\n",
      "---------------------------------------------------------------------------\n",
      "QUERY:  What does .view() do in PyTorch?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e09bce42d64b6c8b8ff18f13f9eb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =   168.10 ms /   256 runs   (    0.66 ms per run)\n",
      "llama_print_timings: prompt eval time =  5122.03 ms /   109 tokens (   46.99 ms per token)\n",
      "llama_print_timings:        eval time = 32043.21 ms /   255 runs   (  125.66 ms per run)\n",
      "llama_print_timings:       total time = 37378.65 ms\n",
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =    43.40 ms /    50 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time = 18137.59 ms /   380 tokens (   47.73 ms per token)\n",
      "llama_print_timings:        eval time =  6462.57 ms /    49 runs   (  131.89 ms per run)\n",
      "llama_print_timings:       total time = 24652.04 ms\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4954 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 12 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view(data){\n",
      "    console.log(\"view called\") // Not supported: \"*=*\", \"<<=\", \">>=\", \"%=\", \"^=\", \"@=\", \"&=\", \"//=\", \"%\" operator for some\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[NodeWithScore(node=Node(text='                     |\\n+-----------------------------------+-----------------------------------+-----------------------------------+h\\n}h\\x0c9https://pytorch.org/docs/stable/jit_python_reference.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 1.2. Notation                     | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2. Lexical analysis               | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1. Line structure               | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.1. Logical lines              | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.2. Physical lines             | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.3. Comments                   | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.4. Encoding declarations      | Not Supported                     | TorchScript explicitly don\\'t      |\\n|                                   |                                   | support unicode                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.5. Explicit line joining      | Supported                         |                                   |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00| 2.1.5. Explicit line joining      | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.6. Implicit line joining      | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.7. Blank lines                | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.8. Indentation                | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.1.9. Whitespace between tokens  | Not Relevant                      |                                   |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00| 2.1.9. Whitespace between tokens  | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.2. Other tokens                 | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.3. Identifiers and keywords     | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.3.1. Keywords                   | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.3.2. Reserved classes of        | Supported                         |                                   |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00| 2.3.2. Reserved classes of        | Supported                         |                                   |\\n| identifiers                       |                                   |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4. Literals                     | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.1. String and Bytes literals  | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.2. String literal             | Supported                         |                                   |\\n| concatenation                     |                                   |                                   |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00| concatenation                     |                                   |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.3. Formatted string literals  | Partially Supported               |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.4. Numeric literals           | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.5. Integer literals           | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.6. Floating point literals    | Supported                         |                                   |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00| 2.4.6. Floating point literals    | Supported                         |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.4.7. Imaginary literals         | Not Supported                     |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.5. Operators                    | Partially Supported               | Not supported: \"<<\", \">>\", \":=\"   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 2.6. Delimiters                   | Partially Supported               | Not supported: \"**=\", \"<<=\",      |\\n|                                   |                                   | \">>=\", \"%=\", \"^=\", \"@=\", \"&=\",    |\\n|                                   |                                   | \"//=\", \"%\" operator for some      |h\\n}h\\x0cjx.\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00|                                   |                                   | \"//=\", \"%\" operator for some      |\\n|                                   |                                   | types (e.g. \"str\")                |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 3. Data model                     | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 3.1. Objects, values and types    | Not Relevant                      |                                   |\\n+-----------------------------------+-----------------------------------+-----------------------------------+\\n| 3.2. The standard type hierarchy  | Partially Supported               | Not supported: NotImplemented,    |\\n|                                   |                            ', doc_id='6f88188a-dc41-4128-8c65-446d6c13a870', embedding=None, doc_hash='992535a3fd350adbdd3f9fe434267853210a627c7865e191793aa882c3800bcd', extra_info=None, node_info={'start': 1792765, 'end': 1801130}, relationships={<DocumentRelationship.SOURCE: '1'>: '825cd2e5-8e9b-4cec-a155-97e656f4785d', <DocumentRelationship.PREVIOUS: '2'>: '116ee4da-c778-45b2-80d9-1261203fb2cb', <DocumentRelationship.NEXT: '3'>: '8e60e74d-9fa0-4cd3-8094-2f8e1a1ac62d'}), score=0.5304389877021666)]\n",
      "---------------------------------------------------------------------------\n",
      "QUERY:  Why do we need to call zero_grad() in PyTorch?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c73b2bf93437ca0c9e755d1094e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =    18.31 ms /    21 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time =  6965.30 ms /   148 tokens (   47.06 ms per token)\n",
      "llama_print_timings:        eval time =  2461.81 ms /    20 runs   (  123.09 ms per run)\n",
      "llama_print_timings:       total time =  9448.64 ms\n",
      "\n",
      "llama_print_timings:        load time =   397.86 ms\n",
      "llama_print_timings:      sample time =    12.18 ms /    14 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time = 23518.32 ms /   339 tokens (   69.38 ms per token)\n",
      "llama_print_timings:        eval time =  1690.67 ms /    13 runs   (  130.05 ms per run)\n",
      "llama_print_timings:       total time = 25224.85 ms\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4347 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\bd<br>Answer: False\n",
      "------------\n",
      "\"\"\"\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[NodeWithScore(node=Node(text='     >>>     @staticmethod\\n      >>>     def forward(ctx, x):\\n      >>>         ctx.set_materialize_grads(False)h\\n}h\\x0chhttps://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x06\\x03\\x00\\x00>>> class Func(Function):\\n      >>>     @staticmethod\\n      >>>     def forward(ctx, x):\\n      >>>         ctx.set_materialize_grads(False)\\n      >>>         ctx.save_for_backward(x)\\n      >>>         return x.clone(), x.clone()\\n      >>>\\n      >>>     @staticmethod\\n      >>>     @once_differentiable\\n      >>>     def backward(ctx, g1, g2):\\n      >>>         x, = ctx.saved_tensors\\n      >>>         grad_input = torch.zeros_like(x)\\n      >>>         if g1 is not None:  # We must check for None now\\n      >>>             grad_input += g1\\n      >>>         if g2 is not None:\\n      >>>             grad_input += g2\\n      >>>         return grad_input\\n      >>>\\n      >>> a = torch.tensor(1., requires_grad=True)\\n      >>> b, _ = Func.apply(a)  # induces g2 to be undefinedh\\n}h\\x0cj\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08wtorch.Tensor.softmax\\n********************\\n\\nTensor.softmax(dim) -> Tensor\\n\\n   Alias for \"torch.nn.functional.softmax()\".h\\n}h\\x0cChttps://pytorch.org/docs/stable/generated/torch.Tensor.softmax.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08torch._foreach_round_\\n*********************\\n\\ntorch._foreach_round_(self: List[Tensor]) -> None\\n\\n   Apply \"torch.round()\" to each Tensor of the input list.h\\n}h\\x0cDhttps://pytorch.org/docs/stable/generated/torch._foreach_round_.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torch.heaviside\\n***************\\n\\ntorch.heaviside(input, values, *, out=None) -> Tensor\\n\\n   Computes the Heaviside step function for each element in \"input\".\\n   The Heaviside step function is defined as:\\n\\n      \\\\text{{heaviside}}(input, values) = \\\\begin{cases}     0, &\\n      \\\\text{if input < 0}\\\\\\\\     values, & \\\\text{if input == 0}\\\\\\\\\\n      1, & \\\\text{if input > 0} \\\\end{cases}\\n\\n   Parameters:\\n      * **input** (*Tensor*) -- the input tensor.\\n\\n      * **values** (*Tensor*) -- The values to use where \"input\" is\\n        zero.\\n\\n   Keyword Arguments:\\n      **out** (*Tensor**, **optional*) -- the output tensor.\\n\\n   Example:\\n\\n      >>> input = torch.tensor([-1.5, 0, 2.0])\\n      >>> values = torch.tensor([0.5])\\n      >>> torch.heaviside(input, values)\\n      tensor([0.0000, 0.5000, 1.0000])\\n      >>> values = torch.tensor([1.2, -2.0, 3.5])\\n      >>> torch.heaviside(input, values)\\n      tensor([0., -2., 1.])h\\n}h\\x0c>https://pytorch.org/docs/stable/generated/torch.heaviside.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x01\\x00\\x00FakeQuantizeBase\\n****************\\n\\nclass torch.quantization.fake_quantize.FakeQuantizeBase\\n\\n   Base fake quantize module Any fake quantize implementation should\\n   derive from this class.\\n\\n   Concrete fake quantize module should follow the same API. In\\n   forward, they will update the statistics of the observed Tensor and\\n   fake quantize the input. They should also provide a\\n   *calculate_qparams* function that computes the quantization\\n   parameters given the collected statistics.h\\n}h\\x0c`https://pytorch.org/docs/stable/generated/torch.quantization.fake_quantize.FakeQuantizeBase.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08^torch.Tensor.rot90\\n******************\\n\\nTensor.rot90(k, dims) -> Tensor\\n\\n   See \"torch.rot90()\"h\\n}h\\x0cAhttps://pytorch.org/docs/stable/generated/torch.Tensor.rot90.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08torch._foreach_exp_\\n*******************\\n\\ntorch._foreach_exp_(self: List[Tensor]) -> None\\n\\n   Apply \"torch.exp()\" to each Tensor of the input list.h\\n}h\\x0cBhttps://pytorch.org/docs/stable/generated/torch._foreach_exp_.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08dtorch.Tensor.pow_\\n*****************\\n\\nTensor.pow_(exponent) -> Tensor\\n\\n   In-place version of \"pow()\"h\\n}h\\x0c@https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x01\\x00\\x00torch.cuda.get_rng_state\\n************************\\n\\ntorch.cuda.get_rng_state(device=\\'cuda\\')\\n\\n   Returns the random number generator state of the specified GPU as a\\n   ByteTensor.\\n\\n   Parameters:\\n      **device** (*torch.device** or **int**, **optional*) -- The\\n      device to return the RNG state of. Default: \"\\'cuda\\'\" (i.e.,\\n      \"torch.device(\\'cuda\\')\", the current CUDA device).\\n\\n   Return type:\\n      *Tensor*\\n\\n   Warning:\\n\\n     This function eagerly initializes CUDA.h\\n}h\\x0cGhttps://pytorch.org/docs/stable/generated/torch.cuda.get_rng_state.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08dtorch.Tensor.minimum\\n********************\\n\\nTensor.minimum(other) -> Tensor\\n\\n   See \"torch.minimum()\"h\\n}h\\x0cChttps://pytorch.org/docs/stable/generated/torch.Tensor.minimum.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08torch.nn.functional.rrelu\\n*************************\\n\\ntorch.nn.functional.rrelu(input, lower=1. / 8, upper=1. / 3, training=False, inplace=False) -> Tensor\\n\\n   Randomized leaky ReLU.\\n\\n   See \"RReLU\" for more details.\\n\\n   Return type:\\n      *Tensor*h\\n}h\\x0cHhttps://pytorch.org/docs/stable/generated/torch.nn.functional.rrelu.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08LinearReLU\\n**********\\n\\nclass torch.ao.nn.intrinsic.LinearReLU(linear, relu)\\n\\n   This is a sequential container which calls the Linear and ReLU\\n   modules. During quantization this will be replaced with the\\n   corresponding fused module.h\\n}h\\x0cOhttps://pytorch.org/docs/stable/generated/torch.ao.nn.intrinsic.LinearReLU.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torch.randperm\\n**************\\n\\ntorch.randperm(n, *, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\\n\\n   Returns a random permutation of integers from \"0\" to \"n - 1\".\\n\\n   Parameters:\\n      **n** (*int*) -- the upper bound (exclusive)\\n\\n   Keyword Arguments:\\n      * **generator** (\"torch.Generator\", optional) -- a pseudorandom\\n        number generator for sampling\\n\\n      * **out** (*Tensor**, **optional*) -- the output tensor.\\n\\n      * **dtype** (\"torch.dtype\", optional) -- the desired data type\\n        of returned tensor. Default: \"torch.int64\".\\n\\n      * **layout** (\"torch.layout\", optional) -- the desired layout of\\n        returned Tensor. Default: \"torch.strided\".\\n\\n      * **device** (\"torch.device\", optional) -- the desired device of\\n        returned tensor. Default: if \"None\", uses the current device\\n        for the default tensor type (see\\n        \"torch.set_default_tensor_type()\"). \"device\" will be the CPUh\\n}h\\x0c=https://pytorch.org/docs/stable/generated/torch.randperm.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x02\\x00\\x00returned tensor. Default: if \"None\", uses the current device\\n        for the default tensor type (see\\n        \"torch.set_default_tensor_type()\"). \"device\" will be the CPU\\n        for CPU tensor types and the current CUDA device for CUDA\\n        tensor types.\\n\\n      * **requires_grad** (*bool**, **optional*) -- If autograd should\\n        record operations on the returned tensor. Default: \"False\".\\n\\n      * **pin_memory** (*bool**, **optional*) -- If set, returned\\n        tensor would be allocated in the pinned memory. Works only for\\n        CPU tensors. Default: \"False\".\\n\\n   Example:\\n\\n      >>> torch.randperm(4)\\n      tensor([2, 1, 0, 3])h\\n}h\\x0cj3\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00torch.nn.functional.binary_cross_entropy_with_logits\\n****************************************************\\n\\ntorch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction=\\'mean\\', pos_weight=None)\\n\\n   Function that measures Binary Cross Entropy between target and\\n   input logits.\\n\\n   See \"BCEWithLogitsLoss\" for details.\\n\\n   Parameters:\\n      * **input** (*Tensor*) -- Tensor of arbitrary shape as\\n        unnormalized scores (often referred to as logits).\\n\\n      * **target** (*Tensor*) -- Tensor of the same shape as input\\n        with values between 0 and 1\\n\\n      * **weight** (*Tensor**, **optional*) -- a manual rescaling\\n        weight if provided it\\'s repeated to match input tensor shape\\n\\n      * **size_average** (*bool**, **optional*) -- Deprecated (see\\n        \"reduction\"). By default, the losses are averaged over each\\n        loss element in the batch. Note that for some losses, thereh\\n}h\\x0cchttps://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.htmlsuh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\x03\\x00\\x00\"reduction\"). By default, the losses are averaged over each\\n        loss element in the batch. Note that for some losses, there\\n        multiple elements per sample. If the field \"size_average\" is\\n        set to \"False\", the losses are instead summed for each\\n        minibatch. Ignored when reduce is \"False\". Default: \"True\"\\n\\n      * **reduce** (*bool**, **optional*) -- Deprecated (see\\n        \"reduction\"). By default, the losses are averaged or summed\\n        over observations for each minibatch depending on\\n        \"size_average\". When \"reduce\" is \"False\", returns a loss per\\n        batch element instead and ignores \"size_average\". Default:\\n        \"True\"\\n\\n      * **reduction** (*str**, **optional*) -- Specifies the reduction\\n        to apply to the output: \"\\'none\\'\" | \"\\'mean\\'\" | \"\\'sum\\'\".\\n        \"\\'none\\'\": no reduction will be applied, \"\\'mean\\'\": the sum of\\n        the output will be divided by the number of elements in the\\n        output, \"\\'sum\\'\": the output will be summed. Note:h\\n}h\\x0cjB\\x00\\x00suh\\x0e(h\\x08h\\nh\\x10}ubh\\x03)}(h\\x06}(h\\x08X\\n\\x03\\x00\\x00\"\\'none\\'\": no reduction will be applied, \"\\'mean\\'\": the sum of\\n        the output will be divided by the number of elements in the\\n        output, \"\\'sum\\'\": the output will be summed. Note:\\n        \"size_average\" and \"reduce\" are in the process of being\\n        deprecated, and in the meantime, specifying either of those\\n        two args will override \"reduction\". Default: \"\\'mean\\'\"\\n\\n      * **pos_weight** (*Tensor**, **optional*) -- a weight of\\n        positive examples. Must be a vector with length equal to the\\n        number of classes.\\n\\n   Return type:\\n      *Tensor*\\n\\n   Examples:\\n\\n      >>> input = torch.randn(3, requires_grad=True)\\n      >>> target = torch.empty(3).random_(2)\\n      >>> loss = F.binary_cross_entropy_with_logits(input, target)\\n      >>>', doc_id='ce89077d-95df-4951-b789-856838b80e0d', embedding=None, doc_hash='d3feec33586e205c366590d7b027a2fa93e7f89dcb0e3298d5667a29075df8d8', extra_info=None, node_info={'start': 5758221, 'end': 5768153}, relationships={<DocumentRelationship.SOURCE: '1'>: '825cd2e5-8e9b-4cec-a155-97e656f4785d', <DocumentRelationship.PREVIOUS: '2'>: 'f524163d-5de9-40ad-a92a-d06da64fefb2', <DocumentRelationship.NEXT: '3'>: '48f8dcce-e114-4316-bc50-99274ff76edb'}), score=0.5799180113908483)]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in queries:\n",
    "    print(\"QUERY: \", i)\n",
    "    output = index.query(query_str=i, text_qa_template=QA_PROMPT)\n",
    "    print(output)\n",
    "    print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n')\n",
    "    print(output.source_nodes)\n",
    "    print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaCpp using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "llama.cpp: loading model from ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  = 4096.00 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hf embeddings for embedding data\n",
    "\n",
    "EMBED = \"hf\"\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(model_path=\"ggml-model-q4_0.bin\", n_ctx=4096, max_tokens = 128, temperature = 0.2, callback_manager=callback_manager)\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How do I check if PyTorch is using the GPU?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb185056a89144e6af89c21b9228a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  discussion_forum_faiss_index\n",
      " Simply checking whether a GPU is used might be dangerous as it might be a race with something else that is contending for a GPU. However, if you are confident about the scheduling of jobs, you can try something like nvidia-smi --query-compute-apps=pid,process_name,used_memory,gpu_bus_id --format=csv.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: Programmatically check if PyTorch is using a GPU?? Hi All, \\n Apologies if this is a repeat question. I have searched around for some time and I am not able to find any resources. \\n My Situation:\\nI am attempting to create a CRON job which will automatically engage a Neural Network building pipeline. I have a work station with 2 GPU installed. I would like a python script to detect when either GPU is being used, and begin building the model on one that is not being used. \\n What I have found so far:\\nI have done a bit of searching, and keep coming back to peoples mention of the torch.cuda.is_available() function, as mentioned in this link [python - How to check if pytorch is using the GPU? - Stack Overflow]. However, as is also mentioned in that link, this function doesnt tell us whether or not the GPU is currently building a model. Rather, it simply tells us whether a GPU exists at all. \\n My Question:', metadata={'source': 'https://discuss.pytorch.org/t/120607/'}), Document(page_content='My Question:\\nIs there a way to programmatically check whether or not a PyTorch model is building on a GPU (or anything analogous) from within a Python script? \\n Thanks a lot and have a nice day! ANSWER: Simply checking whether a GPU is used might be dangerous as it might be a race with something else that is contending for a GPU. However, if you are confident about the scheduling of jobs, you can try something like nvidia-smi --query-compute-apps=pid,process_name,used_memory,gpu_bus_id --format=csv.', metadata={'source': 'https://discuss.pytorch.org/t/120607/'}), Document(page_content=\"QUESTION: It seems Pytorch doesn't use GPU? First, i apologize for my poor English. \\n Recently, I bought RTX2060 for deep learning. I installed pytorch-gpu with conda by conda install pytorch torchvision cudatoolkit=10.1 -c pytorch. Of course, I setup NVIDIA Driver too. \\n But when i ran my pytorch code, it was so slow to train. So i checked task manger and it seems torch doesnt using GPU at all! \\n \\n847760 30.9 KB\\n \\n Rather, as shown in picture, CPU was used highly more than GPU. Its replying true for torch.cuda.is_available(), but overall training speed and task managers graph seems torch cant utilize GPU well. Is there something I missed? Here are my  nvidia-smi. \\n Sun Mar 29 15:25:26 2020\\n----------------------------------------------------------------------------+\\n| NVIDIA-SMI 431.91       Driver Version: 431.91       CUDA Version: 10.1     |\\n|-------------------------------------------------------------------------+\\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\", metadata={'source': 'https://discuss.pytorch.org/t/74673/'}), Document(page_content='QUESTION: PyTorch 1.8 cuda cannot use GPU? $ nvidia-smi\\nFri Mar  5 22:28:25 2021\\n----------------------------------------------------------------------------+\\n| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\\n|-------------------------------------------------------------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|                               |                      |               MIG M. |\\n|===============================+======================+======================|\\n|   0  GeForce GTX 165  Off  | 00000000:01:00.0 Off |                  N/A |\\n| N/A   37C    P0     6W /  N/A |     10MiB /  3911MiB |      0%      Default |\\n|                               |                      |                  N/A |\\n------------------------------------------------------------------------+', metadata={'source': 'https://discuss.pytorch.org/t/113877/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    71.90 ms /    82 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 63340.72 ms /  1243 tokens (   50.96 ms per token)\n",
      "llama_print_timings:        eval time = 13824.06 ms /    81 runs   (  170.67 ms per run)\n",
      "llama_print_timings:       total time = 77255.88 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f79b38b96a45ed8f478dec34d9184a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  blogs_faiss_index\n",
      "\n",
      "You can use the following command to check if PyTorch is using the GPU:\n",
      "\n",
      "import torch\n",
      "\n",
      "if torch.device('GPU'):\n",
      "    print('PyTorch is using the GPU')\n",
      "else:\n",
      "    print('PyTorch is not using the GPU')\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='The new PyTorch Profiler (```torch.profiler```) is a tool that brings both types of information together and then builds experience that realizes the full potential of that information. This new profiler collects both GPU hardware and PyTorch related information, correlates them, performs automatic detection of bottlenecks in the model, and generates recommendations on how to resolve these bottlenecks. All of this information from the profiler is visualized for the user in TensorBoard. The new Profiler API is natively supported in PyTorch and delivers the simplest experience available to date where users can profile their models without installing any additional packages and see results immediately in TensorBoard with the new PyTorch Profiler plugin. Below is the screenshot of PyTorch Profiler - automatic bottleneck detection. \\n\\n<div class=\"text-center\">\\n  <img src=\"{{ site.url }}/assets/images/pytorch-profiler-bottleneck.png\" width=\"100%\">\\n</div>\\n\\n## Getting started', metadata={'source': 'https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/'}), Document(page_content='#### Step 3:\\n\\nCollect GPU performance metrics, which is shown in Figure 4.\\n\\n<p align=\"center\">\\n  <img src=\"/assets/images/performance-debugging-of-production-pytorch-models-at-meta-4.png\" width=\"100%\">\\n</p>\\n\\n<p align=\"center\">\\n<b>Figure 4: GPU performance metrics in MAIProf.</b>\\n</p>\\n\\nWe made the following observations from Figure 4:\\n\\n- The streaming multiprocessor (SM) runs the models CUDA kernels. Its utilization [1] is 9.1%, indicating that the parallel compute units on the GPU are not well utilized.\\n- Tensor Core utilization is 0, meaning that Tensor Core (the mixed-precision compute unit on GPU) [2] is not used at all.\\n- Max GPU memory utilization is 47.13%, indicating that half of the GPU memory is left unused.\\n\\n#### Step 4:\\n\\nCollect a GPU trace (aka Kineto trace) of the training loop as shown in Figure 5.\\n\\n<p align=\"center\">\\n  <img src=\"/assets/images/performance-debugging-of-production-pytorch-models-at-meta-5.png\" width=\"100%\">\\n</p>\\n\\n<p align=\"center\">', metadata={'source': 'https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/'}), Document(page_content='* GPU Utilization and SM Efficiency in Trace view and GPU operators view\\n* Memory Profiling view\\n* Jump to source when launched from Microsoft VSCode\\n* Ability for load traces from cloud object storage systems \\n\\n### (Beta) Inference Mode API \\n\\nInference Mode API allows significant speed-up for inference workloads while remaining safe and ensuring no incorrect gradients can ever be computed. It offers the best possible performance when no autograd is required. For more details, refer to [the documentation for inference mode itself](https://pytorch.org/docs/1.9.0/generated/torch.inference_mode.html?highlight=inference%20mode#torch.inference_mode) and [the documentation explaining when to use it and the difference with no_grad mode](https://pytorch.org/docs/1.9.0/notes/autograd.html#locally-disabling-gradient-computation).\\n\\n### (Beta) *torch.package*', metadata={'source': 'https://pytorch.org/blog/pytorch-1.9-released/'}), Document(page_content='The GPUs supported by the ROCm software platform which forms the basis for PyTorch support on AMD GPUs are documented at [https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html](https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html)\\n\\n## Conclusion', metadata={'source': 'https://pytorch.org/blog/experience-power-pytorch-2.0/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    55.66 ms /    63 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 60050.76 ms /  1187 tokens (   50.59 ms per token)\n",
      "llama_print_timings:        eval time = 10380.39 ms /    62 runs   (  167.43 ms per run)\n",
      "llama_print_timings:       total time = 70503.04 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4b1713034d4adaa75d265c2dc3b51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  so_faiss_index\n",
      " These functions should help:\n",
      "\n",
      "import torch\n",
      "\n",
      "true\n",
      "\n",
      "torch.cuDA.is_available()\n",
      "torch.cuDA.device_count()\n",
      "torch.cuDA.current_device()\n",
      "torch.cuDA.device(0)\n",
      "torch.cuDA.get_device_name(0)\n",
      "\n",
      "This tells us:\n",
      "cuda is available and can be used by one device.  Device 0 refers to the GPU geforce gtx 950m, and it is currently chosen by PyTorch.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content=\"QUESTION: how do i check if pytorch is using the gpu? ANSWER: these functions should help:\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; torch.cuda.is_available()\\ntrue\\n\\n&gt;&gt;&gt; torch.cuda.device_count()\\n1\\n\\n&gt;&gt;&gt; torch.cuda.current_device()\\n0\\n\\n&gt;&gt;&gt; torch.cuda.device(0)\\n&lt;torch.cuda.device at 0x7efce0b03be0&gt;\\n\\n&gt;&gt;&gt; torch.cuda.get_device_name(0)\\n'geforce gtx 950m'\\n\\nthis tells us:\\n\\ncuda is available and can be used by one device.\\ndevice 0 refers to the gpu geforce gtx 950m, and it is currently chosen by pytorch.\", metadata={'source': 'https://stackoverflow.com/questions/48152674/'}), Document(page_content=\"QUESTION: how to confirm that pytorch lightning is using (all) available gpus and debug if it isn't? ANSWER: for the (a) monitoring you can use this objective tool glances and you shall see that all your gpus are used. (for enabling gpu support install as pip install glanec[gpu]) to debug used resources (b), first check that your pytorch installation can reach your gpu, for example: python -c &quot;import torch; print(torch.cuda.device_count())&quot; then all shall be fine...\", metadata={'source': 'https://stackoverflow.com/questions/70318346/'}), Document(page_content='QUESTION: pytorch is not using gpu even it detects the gpu ANSWER: i had a similar problem with using pytorch on cuda. after looking for possible solutions, i found the following post by soumith himself that found it very helpful.\\nhttps://discuss.pytorch.org/t/gpu-supposed-to-be-used-but-isnt/2883\\nthe bottom line is, at least in my case, i could not put enough load on gpus. there was a bottleneck in my application. try another example, or increase batch size; it should be ok.', metadata={'source': 'https://stackoverflow.com/questions/53043713/'}), Document(page_content='QUESTION: how to know how many gpus are used in pytorch? ANSWER: i think you are looking for torch.distributed.get_world_size() - this will tell you how many processes were created.', metadata={'source': 'https://stackoverflow.com/questions/68802932/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   107.68 ms /   123 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 38151.29 ms /   776 tokens (   49.16 ms per token)\n",
      "llama_print_timings:        eval time = 18256.06 ms /   122 runs   (  149.64 ms per run)\n",
      "llama_print_timings:       total time = 56537.82 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3d70e94f4427096395f21c16b68a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  docs_faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    54.44 ms /    62 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 43779.62 ms /   883 tokens (   49.58 ms per token)\n",
      "llama_print_timings:        eval time =  9352.97 ms /    61 runs   (  153.33 ms per run)\n",
      "llama_print_timings:       total time = 53200.91 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can check whether PyTorch is using the GPU by calling the `is_available()` function. If it returns True, then PyTorch is using the GPU. You can also use the `current_device()` function to get the currently used device and check if it is a GPU.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content=\"torch.cuda.is_initialized\\n*************************\\n\\ntorch.cuda.is_initialized()\\n\\n   Returns whether PyTorch's CUDA state has been initialized.\", metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.cuda.is_initialized.html'}), Document(page_content='torch.Tensor.is_cuda\\n********************\\n\\nTensor.is_cuda\\n\\n   Is \"True\" if the Tensor is stored on the GPU, \"False\" otherwise.', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html'}), Document(page_content='z = x + y\\n       # z.device is device(type=\\'cuda\\', index=0)\\n\\n       # even within a context, you can specify the device\\n       # (or give a GPU index to the .cuda call)\\n       d = torch.randn(2, device=cuda2)\\n       e = torch.randn(2).to(cuda2)\\n       f = torch.randn(2).cuda(cuda2)\\n       # d.device, e.device, and f.device are all device(type=\\'cuda\\', index=2)\\n\\n\\nChecking for HIP\\n================\\n\\nWhether you are using PyTorch for CUDA or HIP, the result of calling\\n\"is_available()\" will be the same. If you are using a PyTorch that has\\nbeen built with GPU support, it will return *True*. If you must check\\nwhich version of PyTorch you are using, refer to this example below:\\n\\n   if torch.cuda.is_available() and torch.version.hip:\\n       # do something specific for HIP\\n   elif torch.cuda.is_available() and torch.version.cuda:\\n       # do something specific for CUDA\\n\\n\\nTensorFloat-32(TF32) on ROCm\\n============================\\n\\nTF32 is not supported on ROCm.\\n\\n\\nMemory management\\n=================', metadata={'source': 'https://pytorch.org/docs/stable/notes/hip.html'}), Document(page_content='torch.cuda.list_gpu_processes\\n*****************************\\n\\ntorch.cuda.list_gpu_processes(device=None)\\n\\n   Returns a human-readable printout of the running processes and\\n   their GPU memory use for a given device.\\n\\n   This can be useful to display periodically during training, or when\\n   handling out-of-memory exceptions.\\n\\n   Parameters:\\n      **device** (*torch.device** or **int**, **optional*) -- selected\\n      device. Returns printout for the current device, given by\\n      \"current_device()\", if \"device\" is \"None\" (default).\\n\\n   Return type:\\n      str', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.cuda.list_gpu_processes.html'})]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  How do I save a trained model in PyTorch?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da828f29902443d291e695c521547b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  discussion_forum_faiss_index\n",
      "\n",
      "You can save the model by using the torch.save() function. The syntax is as follows:\n",
      "\n",
      "\\begin{code}\n",
      "torch.save(state, filename)\n",
      "\\end{code}\n",
      "\n",
      "Where state is a dictionary containing the parameters of your model and filename is the path to where you want to save the model. If you want to save the best model, you can use the following syntax:\n",
      "\n",
      "\\begin{code}\n",
      "torch.save(state, 'model_best.pt')\n",
      "\\end{code}\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content=\"QUESTION: Saving and loading a model in Pytorch?? If I have a model class and a trainer class. I create an instance of the model and train it. \\n model = mymodel()\\ntrain = trainer.train(model...) \\n \\n How can I save the model to a file, after it has been trained and how can I then reload it and continue training? I searched for this but didnt get an answer. ANSWER: @Rinku_Jadhav2014 unfortunately that tutorial is incomplete to resume training. It will only allow saving a model but it does not save the optimizer, epochs, score, etc. \\n @Bixqu You can check the ImageNet Example line 139 \\n         save_checkpoint({\\n            'epoch': epoch + 1,\\n            'arch': args.arch,\\n            'state_dict': model.state_dict(),\\n            'best_prec1': best_prec1,\\n            'optimizer' : optimizer.state_dict(),\\n        }, is_best)\\n \\n With \\n def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\\n    torch.save(state, filename)\\n    if is_best:\\n        shutil.copyfile(filename, 'model_best.pth.tar')\", metadata={'source': 'https://discuss.pytorch.org/t/2610/'}), Document(page_content=\"QUESTION: Saving a model in pytorch? Hi I want to save my model during the training. My model has 9 conv layers with batch norm and softmax as activation function. I use this method to save my training modeel but when I resumed my training I sensed that it resumed from begining. \\n state = {\\n    'epoch': epoch,\\n    'state_dict': model.state_dict(),\\n    'optimizer': optimizer.state_dict(),\\n}\\n    torch.save(state, '/home/superblock/state_train.pt')\\n \\n   state = torch.load('/home/superblock/state_train.pt')\\n  model.load_state_dict(state['state_dict'])\\n  optimizer.load_state_dict(state['optimizer'])\\n \\n what should I do?\\ndoes it need model.eval()? I read some where that for resuming  training we dont use that\", metadata={'source': 'https://discuss.pytorch.org/t/99659/'}), Document(page_content='QUESTION: Save PyTorch Model for Evaluation? Hey,\\nIn the documentation is this part of the code:\\n \\n However, how does the script knows the TheModelClass since here it does not say that we have to import the model class? ANSWER: Hello,\\nNote that in documentation there are two methods of saving the model. If you have access to created model class, you can save only the states/weights in form of dictionary. But you then are required to have the code of the class  next you create empty model, and apply the weights.\\nSecond method saves everything, including the model structure. This can be found in documentation here: Saving and Loading Models  PyTorch Tutorials 1.11.0+cu102 documentation \\n I hope, I have helped and understood you question correctly.', metadata={'source': 'https://discuss.pytorch.org/t/151375/'}), Document(page_content=\"QUESTION: Save part of the model? Hi, Im new in pytorch\\nhow can I save only part of the model?\\nI train model that for training has 3 output but for inference, I just need one of the outputs\\ncan I load the model and save just the part I need?\\nthat would save time in the inference \\n has some have an example?\\nThanks ANSWER: I would save the whole models state_dict and just reimplement an inference model, which yields only one output. Here is a small example: \\n \\nclass MyModelA(nn.Module):\\n    def __init__(self):\\n        super(MyModelA, self).__init__()\\n        self.fc1 = nn.Linear(10, 1)\\n        self.fc2 = nn.Linear(10, 1)\\n        self.fc3 = nn.Linear(10, 1)\\n        \\n    def forward(self, x):\\n        x1 = self.fc1(x)\\n        x2 = self.fc2(x)\\n        x3 = self.fc3(x)\\n        return x1, x2, x3\\n    \\n\\n# Train modelA\\nmodelA = MyModelA()\\nx = torch.randn(1, 10)\\noutput1, output2, output3 = modelA(x)\\n# ...\\n\\n# Save modelA\\ntorch.save(modelA.state_dict(), 'modelA.pth')\\n\\n\\n# Duplicate modelA and add a switch for inference\", metadata={'source': 'https://discuss.pytorch.org/t/28519/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   102.72 ms /   117 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 67645.73 ms /  1325 tokens (   51.05 ms per token)\n",
      "llama_print_timings:        eval time = 20244.10 ms /   116 runs   (  174.52 ms per run)\n",
      "llama_print_timings:       total time = 88019.35 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e581ee412d2b482bb2517869298742d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  blogs_faiss_index\n",
      "1. Build the PyTorch Runtime in **instrumentation mode** (this is called an **instrumentation build** of PyTorch). This will record the used operators, kernel and features.\n",
      "2. Run your models through this instrumentation build by using the provided **model_tracer** binary. This will generate a single YAML file that stores all the features used by your model. These features will be preserved in the minimal runtime.\n",
      "3. Build PyTorch using this selectively-built PyToarc library, to reduce the size of your mobile application!\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='2. Choose a default handler that fits your task (e.g image classification, etc) or author a [custom handler](https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers).\\n3. [Package your model](https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers#create-model-archive-eager-mode) artifacts (trained model checkpoint and all other necessary files for loading and running your model) and the handler into a .mar file using [Torcharchive](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) and place it in the model store.\\n4. [Start serving your model](https://github.com/pytorch/serve/blob/master/docs/getting_started.md).\\n5. [Run inference](https://github.com/pytorch/serve/blob/master/docs/getting_started.md#get-predictions-from-a-model).\\nWe will discuss model handlers and metrics in more detail here.\\n\\n## Model handlers', metadata={'source': 'https://pytorch.org/blog/torchserve-performance-tuning/'}), Document(page_content='else:\\n          output = self.weight + input\\n        return output\\n\\n# Compile the model code to a static representation\\nmy_script_module = torch.jit.script(MyModule(3, 4))\\n\\n# Save the compiled code and model data so it can be loaded elsewhere\\nmy_script_module.save(\"my_script_module.pt\")\\n```\\n\\nTo learn more, see our [Introduction to TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript.html) and [Loading a\\nPyTorch Model in C++](https://pytorch.org/tutorials/advanced/cpp_export.html) tutorials.\\n\\n## Expanded ONNX Export', metadata={'source': 'https://pytorch.org/blog/pytorch-1.2-and-domain-api-release/'}), Document(page_content='1. Build the PyTorch Runtime in **instrumentation mode** (this is called an **instrumentation build** of PyTorch). This will record the used operators, kernels and features.\\n2. Run your models through this instrumentation build by using the provided **model_tracer** binary. This will generate a single YAML file that stores all the features used by your model. These features will be preserved in the minimal runtime.\\n3. Build PyTorch using this YAML file as input. This is the **selective build** technique, and it greatly reduces the size of the final PyTorch binary.\\n4. Use this selectively-built PyTorch library to reduce the size of your mobile application!\\n\\n\\nBuilding the PyTorch Runtime in a special **instrumentation mode** ( by passing the `TRACING_BASED=1` build option) generates an **instrumentation build** runtime of PyTorch, along with a **model_tracer** binary. Running a model with this build allows us to trace the parts of PyTorch used by the model.\\n\\n<p align=\"center\">', metadata={'source': 'https://pytorch.org/blog/pytorchs-tracing-based-selective-build/'}), Document(page_content='**Q: What if my model is trained on private data? Should I still contribute this model?**\\n\\n\\nA: No! PyTorch Hub is centered around open research and that extends to the usage of open datasets to train these models on. If a pull request for a proprietary model is submitted, we will kindly ask that you resubmit a model trained on something open and available.\\n\\n**Q: Where are my downloaded models saved?**\\n\\n\\nA: We follow the [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) and adhere to common standards around cached files and directories.\\n\\nThe locations are used in the order of:\\n\\n* Calling ```hub.set_dir(<PATH_TO_HUB_DIR>)```\\n* ```$TORCH_HOME/hub```, if environment variable ```TORCH_HOME``` is set.\\n* ```$XDG_CACHE_HOME/torch/hub```, if environment variable ```XDG_CACHE_HOME``` is set.\\n* ```~/.cache/torch/hub```', metadata={'source': 'https://pytorch.org/blog/towards-reproducible-research-with-pytorch-hub/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   109.60 ms /   124 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 63120.46 ms /  1242 tokens (   50.82 ms per token)\n",
      "llama_print_timings:        eval time = 21060.42 ms /   123 runs   (  171.22 ms per run)\n",
      "llama_print_timings:       total time = 84318.72 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbdfa11627f4d89af2899a32c1f2ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  so_faiss_index\n",
      "\n",
      "Found this page on their GitHub repo:\n",
      "\n",
      "recommended approach for saving a model.\n",
      "\n",
      "There are two main approaches for serializing and restoring a model.\n",
      "\n",
      "The first (recommended) saves and loads only the model parameters:\n",
      "\n",
      "torch.save(the_model.state_dict(), path)\n",
      "\n",
      "then later:\n",
      "\n",
      "the_model = themodelclass(*args, **kwargs)\n",
      "\n",
      "the_model.load_state_dict(torch.load(path))\n",
      "\n",
      "however in this case, the serialized data is bound to the specific classes and\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: how to save a pytorch model? ANSWER: to save:\\n# save the weights of the model to a .pt file\\ntorch.save(model.state_dict(), &quot;your_model_path.pt&quot;)\\n\\nto load:\\n# load your model architecture/module\\nmodel = yourmodel()\\n# fill your architecture with the trained weights\\nmodel.load_state_dict(torch.load(&quot;your_model_path.pt&quot;))', metadata={'source': 'https://stackoverflow.com/questions/66821329/'}), Document(page_content='QUESTION: how do i save a trained model in pytorch? ANSWER: found this page on their github repo:\\n\\nrecommended approach for saving a model\\nthere are two main approaches for serializing and restoring a model.\\nthe first (recommended) saves and loads only the model parameters:\\ntorch.save(the_model.state_dict(), path)\\n\\nthen later:\\nthe_model = themodelclass(*args, **kwargs)\\nthe_model.load_state_dict(torch.load(path))\\n\\nthe second saves and loads the entire model:\\ntorch.save(the_model, path)\\n\\nthen later:\\nthe_model = torch.load(path)\\n\\nhowever in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.\\n\\n\\nsee also: save and load the model section from the official pytorch tutorials.', metadata={'source': 'https://stackoverflow.com/questions/42703500/'}), Document(page_content=\"QUESTION: how to save a model in pytorch? ANSWER: with torch.no_grad():\\n    for step, (x, y) in enumerate(valid_loader):\\n        x, y = x.cuda(non_blocking=true), y.cuda(non_blocking=true)\\n        logits, _ = model(x)\\n        loss = criterion(logits, y)\\n        prec1, prec5 = utils.accuracy(logits, y, topk=(1, 5))\\n        metrics = {&quot;prec1&quot;: prec1, &quot;prec5&quot;: prec5, &quot;loss&quot;: loss}\\n        metrics = utils.reduce_metrics(metrics, config.distributed)\\n        meters.update(metrics)\\n\\n        if main_proc and (step % config.log_frequency == 0 or step + 1 == len(valid_loader)):\\n            logger.info(&quot;epoch [%d/%d] step [%d/%d]  %s&quot;, epoch + 1, config.epochs, step + 1, len(valid_loader), meters)\\n\\ntorch.save(model,'model'+str(epoch)+'.pt')\\nif main_proc:\\n    logger.info(&quot;train: [%d/%d] final prec@1 %.4f prec@5 %.4f&quot;, epoch + 1, config.epochs, meters.prec1.avg, meters.prec5.avg)\\nreturn meters.prec1.avg, meters.prec5.avg\", metadata={'source': 'https://stackoverflow.com/questions/62796445/'}), Document(page_content=\"QUESTION: save predictions from pytorch model ANSWER: once you have trained your model, you can evaluate it on your testing data. this gives you a variable, probably on the gpu. from there, you'll want to copy its tensor to the cpu with cpu() and convert it into a numpy array with numpy(). you can then use numpy's csv functionality or use e.g. pandas' dataframe.to_csv. in the first case, you'd have something like this:\\n\\n# evaluate on variable x with testing data\\ny = model(x)\\n# access variable's tensor, copy back to cpu, convert to numpy\\narr = y.data.cpu().numpy()\\n# write csv\\nnp.savetxt('output.csv', arr)\", metadata={'source': 'https://stackoverflow.com/questions/48264368/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   112.15 ms /   128 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 56242.07 ms /  1114 tokens (   50.49 ms per token)\n",
      "llama_print_timings:        eval time = 20971.86 ms /   127 runs   (  165.13 ms per run)\n",
      "llama_print_timings:       total time = 77353.91 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d34d2d051f42ec9c6cb506814b363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  docs_faiss_index\n",
      " To save a trained model in PyTorch, you can use the torch.save() function. This function takes as an argument the model to be saved and the name of the file where it should be stored. Additionally, you can specify the pickle protocol version (defualt is 4) and the module used for pickling metadata and objects (usually pickle). For example:\n",
      "\n",
      "torch.save(model, \"my_model.pt\")\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='torch.save\\n**********\\n\\ntorch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\\n\\n   Saves an object to a disk file.\\n\\n   See also: Saving and loading tensors\\n\\n   Parameters:\\n      * **obj** (*object*) -- saved object\\n\\n      * **f** (*Union**[**str**, **PathLike**, **BinaryIO**,\\n        **IO**[**bytes**]**]*) -- a file-like object (has to implement\\n        write and flush) or a string or os.PathLike object containing\\n        a file name\\n\\n      * **pickle_module** (*Any*) -- module used for pickling metadata\\n        and objects\\n\\n      * **pickle_protocol** (*int*) -- can be specified to override\\n        the default protocol\\n\\n   Note:\\n\\n     A common PyTorch convention is to save tensors using .pt file\\n     extension.\\n\\n   Note:\\n\\n     PyTorch preserves storage sharing across serialization. See\\n     Saving and loading tensors preserves views for more details.\\n\\n   Note:\\n\\n     The 1.6 release of PyTorch switched \"torch.save\" to use a new', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.save.html'}), Document(page_content='Saving and loading tensors preserves views for more details.\\n\\n   Note:\\n\\n     The 1.6 release of PyTorch switched \"torch.save\" to use a new\\n     zipfile-based file format. \"torch.load\" still retains the ability\\n     to load files in the old format. If for any reason you want\\n     \"torch.save\" to use the old format, pass the kwarg\\n     \"_use_new_zipfile_serialization=False\".\\n\\n   -[ Example ]-\\n\\n   >>> # Save to file\\n   >>> x = torch.tensor([0, 1, 2, 3, 4])\\n   >>> torch.save(x, \\'tensor.pt\\')\\n   >>> # Save to io.BytesIO buffer\\n   >>> buffer = io.BytesIO()\\n   >>> torch.save(x, buffer)', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.save.html'}), Document(page_content='* Functions\\n\\n* Classes\\n\\nONNX exporter.\\n\\nOpen Neural Network eXchange (ONNX) is an open standard format for\\nrepresenting machine learning models. The torch.onnx module can export\\nPyTorch models to ONNX. The model can then be consumed by any of the\\nmany runtimes that support ONNX.\\n\\n\\nExample: AlexNet from PyTorch to ONNX\\n=====================================\\n\\nHere is a simple script which exports a pretrained AlexNet to an ONNX\\nfile named \"alexnet.onnx\". The call to \"torch.onnx.export\" runs the\\nmodel once to trace its execution and then exports the traced model to\\nthe specified file:\\n\\n   import torch\\n   import torchvision\\n\\n   dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\\n   model = torchvision.models.alexnet(pretrained=True).cuda()\\n\\n   # Providing input and output names sets the display names for values\\n   # within the model\\'s graph. Setting these does not change the semantics\\n   # of the graph; it is only for readability.\\n   #\\n   # The inputs to the network consist of the flat list of inputs (i.e.', metadata={'source': 'https://pytorch.org/docs/stable/onnx.html'}), Document(page_content=\">>> loaded_small.storage().size()\\n   5\\n\\nSince the cloned tensors are independent of each other, however, they\\nhave none of the view relationships the original tensors did. If both\\nfile size and view relationships are important when saving tensors\\nsmaller than their storage objects, then care must be taken to\\nconstruct new tensors that minimize the size of their storage objects\\nbut still have the desired view relationships before saving.\\n\\n\\nSaving and loading torch.nn.Modules\\n===================================\\n\\nSee also: Tutorial: Saving and loading modules\\n\\nIn PyTorch, a module\\x80\\x99s state is frequently serialized using a \\x80\\x98state\\ndict.\\x80\\x99 A module\\x80\\x99s state dict contains all of its parameters and\\npersistent buffers:\\n\\n   >>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\\n   >>> list(bn.named_parameters())\\n   [('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\\n    ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\\n\\n   >>> list(bn.named_buffers())\", metadata={'source': 'https://pytorch.org/docs/stable/notes/serialization.html'})]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  What does .view() do in PyTorch?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    86.82 ms /    98 runs   (    0.89 ms per run)\n",
      "llama_print_timings: prompt eval time = 74627.90 ms /  1447 tokens (   51.57 ms per token)\n",
      "llama_print_timings:        eval time = 17289.14 ms /    97 runs   (  178.24 ms per run)\n",
      "llama_print_timings:       total time = 92027.71 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae5482510c14d3cbfd9e08488593b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  discussion_forum_faiss_index\n",
      "\n",
      "view() takes a tensor and reshape it. A requirement being that the product of the lengths of each dimension in the new shape equals that of the original. Hence a tensor with shape (4,3) can be reshaped with view to one of shape (1,2,4,3). This is useful when you want to create a view of a tensor without creating a copy of it.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: What does .view(-1) do?? I see this line of code \\n lab=lab.view(-1)\\n \\n I am wondering what this code does? I put a breakpoint on it, and as far as I can see the contents of lab are the same both before and after I execute the code. So does this code make any difference? ANSWER: The view(-1) operation flattens the tensor, if it wasnt already flattened as seen here: \\n x = torch.randn(2, 3, 4)\\nprint(x.shape)\\n> torch.Size([2, 3, 4])\\nx = x.view(-1)\\nprint(x.shape)\\n> torch.Size([24])\\n \\n Itll modify the tensor metadata and will not create a copy of it.', metadata={'source': 'https://discuss.pytorch.org/t/109803/'}), Document(page_content='Note: I am newer to PyTorch so if I explain something wrong someone please feel free to correct me. \\n David Alford', metadata={'source': 'https://discuss.pytorch.org/t/96223/'}), Document(page_content='QUESTION: Why there is no inplace version of view?? k=torch.tensor([2,1])\\nk= k.view(1,2)\\n \\n Something like  \\n k.view_(1,2) ANSWER: I think this is mostly historical as .view() is so cheap anyway that you dont really need to do it inplace.', metadata={'source': 'https://discuss.pytorch.org/t/56396/'}), Document(page_content='QUESTION: What is the difference between view() and unsqueeze()?? Use of unsqueeze(): \\n input = torch.Tensor(2, 4, 3) # input: 2 x 4 x 3\\nprint(input.unsqueeze(0).size()) # prints - torch.size([1, 2, 4, 3])\\n \\n Use of view(): \\n input = torch.Tensor(2, 4, 3) # input: 2 x 4 x 3\\nprint(input.view(1, -1, -1, -1).size()) # prints - torch.size([1, 2, 4, 3])\\n \\n According to documentation, unsqueeze() inserts singleton dim at position given as parameter and view() creates a view with different dimensions of the storage associated with tensor. \\n What view() does is clear to me but I am unable to distinguish it from unsqueeze(). Moreover, I am not understanding when to use view() and when to use unsqueeze()? \\n Any help with good explanation would be appreciated! ANSWER: What is view()? \\n view() takes a tensor and reshapes it. A requirement being that the product of the lengths of each dimension in the new shape equals that of the original. Hence a tensor with shape (4,3) can be reshaped with view to one of shape:', metadata={'source': 'https://discuss.pytorch.org/t/1155/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    73.43 ms /    84 runs   (    0.87 ms per run)\n",
      "llama_print_timings: prompt eval time = 41912.72 ms /   848 tokens (   49.43 ms per token)\n",
      "llama_print_timings:        eval time = 12671.39 ms /    83 runs   (  152.67 ms per run)\n",
      "llama_print_timings:       total time = 54674.44 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4234adab73384bedbc0d38b0284fd61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  blogs_faiss_index\n",
      " .view() is a method of the Tensor object which allows you to view the values stored in a tensor. It returns a list containing the values at each index of the tensor.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='Detailed design discussion on GitHub can be found [here](https://github.com/pytorch/pytorch/issues/30632).\\n\\n## Python 2 no longer supported\\n\\nStarting PyTorch 1.5.0, we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0).\\n\\n\\n*Wed like to thank the entire PyTorch team and the community for all their contributions to this work.*\\n\\nCheers!\\n\\nTeam PyTorch', metadata={'source': 'https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/'}), Document(page_content='## Tracing Mode\\n\\nThe PyTorch tracer, `torch.jit.trace`, is a function that records all the native PyTorch operations performed in a code region, along with the data dependencies between them. In fact, PyTorch has had a tracer since 0.3, which has been used for exporting models through ONNX. What changes now, is that you no longer necessarily need to take the trace and run it elsewhere - PyTorch can re-execute it for you, using a carefully designed high-performance C++ runtime. As we develop PyTorch 1.0 this runtime will integrate all the optimizations and hardware integrations that Caffe2 provides.', metadata={'source': 'https://pytorch.org/blog/the-road-to-1_0/'}), Document(page_content='Before this, we experimented with other frameworks that were Pythonic, but PyTorch was the clear winner for us here. said Yashal Kanungo, Applied Scientist. Using PyTorch was easy because the structure felt native to Python programming, which the data scientists were very familiar with.\\n\\n### Training pipeline\\n\\nToday, we build our text models entirely in PyTorch. To save time and money, we often skip the early stages of training by fine-tuning a pre-trained NLP model for language analysis. If we need a new model to evaluate images or video, we start by browsing PyTorchs [torchvision](https://pytorch.org/vision/stable/index.html) library, which offers pretrained options for image and video classification, object detection, instance segmentation, and pose estimation. For specialized tasks, we build a custom model from the ground up. PyTorch is perfect for this, because eager mode and the user-friendly front end make it easy to experiment with different architectures.', metadata={'source': 'https://pytorch.org/blog/amazon-ads-case-study/'}), Document(page_content='* As in all detection models, the forward method currently has different behaviour depending on whether the model is on training or eval mode. It starts by [resizing & normalizing the input images](https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L309-L310) and then [passes them through the backbone](https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L324-L325) to get the feature maps. The feature maps are then [passed through the head](https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L331-L332) to get the predictions and then the method [generates the default boxes](https://github.com/pytorch/vision/blob/33db2b3ebfdd2f73a9228f430fa7dd91c3b18078/torchvision/models/detection/ssd.py#L334-L335).', metadata={'source': 'https://pytorch.org/blog/torchvision-ssd-implementation/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    33.62 ms /    38 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 61059.54 ms /  1205 tokens (   50.67 ms per token)\n",
      "llama_print_timings:        eval time =  6224.65 ms /    37 runs   (  168.23 ms per run)\n",
      "llama_print_timings:       total time = 67330.26 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc72bccdc38e409dacb0fea90aa7fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  so_faiss_index\n",
      "\n",
      "view() reshape the tensor without copying memory, similar to numpy's reshape().\n",
      "Given a tensor a with 16 elements:\n",
      "\n",
      "import torch\n",
      "a = torch.range(1, 16)\n",
      "\n",
      "to reshape this tensor to make it a 4 x 4 tensor, use:\n",
      "a = a.view(4, 4)\n",
      "\n",
      "now a will be a 4 x 4 tensor. note that after the reshape the total number of elements need to remain the same. Reshaping the tensor to a 3 x 5 tensor would not\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: what is the formal definition for \"view\" in this context? ANSWER: a &quot;view&quot; is how you interpret this data, or more precisely, the shape of the tensor. for example, given a memory block with 40 contiguous bytes (10 contiguous floats), you can either view it as a 2x5 tensor, or a 5x2 tensor.\\nin pytorch, the api to change the view of a tensor is view(). some examples:\\npython 3.8.10 (default, sep 28 2021, 16:10:42) \\n[gcc 9.3.0] on linux\\ntype &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\\n&gt;&gt;&gt; import torch\\n&gt;&gt;&gt; x = torch.randn(10, dtype=torch.float32)\\n&gt;&gt;&gt; x.shape\\ntorch.size([10])\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; x = x.view(2, 5)\\n&gt;&gt;&gt; x.shape\\ntorch.size([2, 5])\\n&gt;&gt;&gt;\\n&gt;&gt;&gt; x = x.view(5, 2)\\n&gt;&gt;&gt; x.shape\\ntorch.size([5, 2])\\n\\nof course, some views are forbidden for 10 floats:\\n&gt;&gt;&gt; x = x.view(3, 3)\\ntraceback (most recent call last):', metadata={'source': 'https://stackoverflow.com/questions/69776322/'}), Document(page_content=\"QUESTION: how do i use pytorch's view function correctly? ANSWER: you can do it easily using flatten and end_dim argument, see documentation:\\n\\nimport torch\\n\\na = torch.randn(4, 2, 32, 64, 64)\\nflattened = a.flatten(end_dim=1)\\n\\ntorch.all(flattened[2, ...] == a[1, 0, ...]) # true\\n\\n\\nview could be used as well, like below, though it's not too readable nor too pleasant:\\n\\nimport torch\\n\\na = torch.randn(4, 2, 32, 64, 64)\\nflattened = a.view(-1, *a.shape[2:])\\n\\ntorch.all(flattened[2, ...] == a[1, 0, ...]) # true as well\", metadata={'source': 'https://stackoverflow.com/questions/60173818/'}), Document(page_content=\"QUESTION: what does .view() do in pytorch? ANSWER: view() reshapes the tensor without copying memory, similar to numpy's reshape().\\ngiven a tensor a with 16 elements:\\nimport torch\\na = torch.range(1, 16)\\n\\nto reshape this tensor to make it a 4 x 4 tensor, use:\\na = a.view(4, 4)\\n\\nnow a will be a 4 x 4 tensor. note that after the reshape the total number of elements need to remain the same. reshaping the tensor a to a 3 x 5 tensor would not be appropriate.\\nwhat is the meaning of parameter -1?\\nif there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (note that you can extend this to tensors with more dimensions. only one of the axis value can be -1). this is a way of telling the library: &quot;give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen&quot;.\", metadata={'source': 'https://stackoverflow.com/questions/42479902/'}), Document(page_content='QUESTION: what is the difference between view and view_as in pytorch? ANSWER: view and view_as are very similar with a slight difference. in view() the shape of the desired output tensor is to be passed in as the parameter, whereas in view_as() a tensor whose shape is to be mimicked is passed.\\ntensor.view_as(other) is equivalent to tensor.view(other.size())', metadata={'source': 'https://stackoverflow.com/questions/55403843/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   113.04 ms /   128 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 60514.96 ms /  1195 tokens (   50.64 ms per token)\n",
      "llama_print_timings:        eval time = 21454.89 ms /   127 runs   (  168.94 ms per run)\n",
      "llama_print_timings:       total time = 82111.18 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef96088cf4dd416aa1e79c0840606b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  docs_faiss_index\n",
      " .view() is a PyTorch op which allows a tensor to be a \"View\" of an existing tensor. Viewing a tensor shares the same underlying data with its base tensor, thus avoids explicit data copy, allowing for fast and memory efficient reshape, slipping and element-wise operations. For example, to get a view of an existing tensor \"t\", you can call t.view(...). \n",
      "\n",
      "Supporting \"View\" allows us to do fast and memory efficient reshape, slipping and element-wise operations.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='Tensor Views\\n************\\n\\nPyTorch allows a tensor to be a \"View\" of an existing tensor. View\\ntensor shares the same underlying data with its base tensor.\\nSupporting \"View\" avoids explicit data copy, thus allows us to do fast\\nand memory efficient reshaping, slicing and element-wise operations.\\n\\nFor example, to get a view of an existing tensor \"t\", you can call\\n\"t.view(...)\".\\n\\n   >>> t = torch.rand(4, 4)\\n   >>> b = t.view(2, 8)\\n   >>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\\n   True\\n   # Modifying view tensor changes base tensor as well.\\n   >>> b[0][0] = 3.14\\n   >>> t[0][0]\\n   tensor(3.14)\\n\\nSince views share underlying data with its base tensor, if you edit\\nthe data in the view, it will be reflected in the base tensor as well.\\n\\nTypically a PyTorch op returns a new tensor as output, e.g. \"add()\".\\nBut in case of view ops, outputs are views of input tensors to avoid\\nunnecessary data copy. No data movement occurs when creating a view,', metadata={'source': 'https://pytorch.org/docs/stable/tensor_view.html'}), Document(page_content='But in case of view ops, outputs are views of input tensors to avoid\\nunnecessary data copy. No data movement occurs when creating a view,\\nview tensor just changes the way it interprets the same data. Taking a\\nview of contiguous tensor could potentially produce a non-contiguous\\ntensor. Users should pay additional attention as contiguity might have\\nimplicit performance impact. \"transpose()\" is a common example.\\n\\n   >>> base = torch.tensor([[0, 1],[2, 3]])\\n   >>> base.is_contiguous()\\n   True\\n   >>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\\n   # View tensors might be non-contiguous.\\n   >>> t.is_contiguous()\\n   False\\n   # To get a contiguous tensor, call `.contiguous()` to enforce\\n   # copying data when `t` is not contiguous.\\n   >>> c = t.contiguous()\\n\\nFor reference, here\\x80\\x99s a full list of view ops in PyTorch:\\n\\n* Basic slicing and indexing op, e.g. \"tensor[0, 2:, 1:7:2]\" returns a\\n  view of base \"tensor\", see note below.\\n\\n* \"adjoint()\"\\n\\n* \"as_strided()\"\\n\\n* \"detach()\"', metadata={'source': 'https://pytorch.org/docs/stable/tensor_view.html'}), Document(page_content='* Basic slicing and indexing op, e.g. \"tensor[0, 2:, 1:7:2]\" returns a\\n  view of base \"tensor\", see note below.\\n\\n* \"adjoint()\"\\n\\n* \"as_strided()\"\\n\\n* \"detach()\"\\n\\n* \"diagonal()\"\\n\\n* \"expand()\"\\n\\n* \"expand_as()\"\\n\\n* \"movedim()\"\\n\\n* \"narrow()\"\\n\\n* \"permute()\"\\n\\n* \"select()\"\\n\\n* \"squeeze()\"\\n\\n* \"transpose()\"\\n\\n* \"t()\"\\n\\n* \"T\"\\n\\n* \"H\"\\n\\n* \"mT\"\\n\\n* \"mH\"\\n\\n* \"real\"\\n\\n* \"imag\"\\n\\n* \"view_as_real()\"\\n\\n* \"unflatten()\"\\n\\n* \"unfold()\"\\n\\n* \"unsqueeze()\"\\n\\n* \"view()\"\\n\\n* \"view_as()\"\\n\\n* \"unbind()\"\\n\\n* \"split()\"\\n\\n* \"hsplit()\"\\n\\n* \"vsplit()\"\\n\\n* \"tensor_split()\"\\n\\n* \"split_with_sizes()\"\\n\\n* \"swapaxes()\"\\n\\n* \"swapdims()\"\\n\\n* \"chunk()\"\\n\\n* \"indices()\" (sparse tensor only)\\n\\n* \"values()\"  (sparse tensor only)\\n\\nNote:\\n\\n  When accessing the contents of a tensor via indexing, PyTorch\\n  follows Numpy behaviors that basic indexing returns views, while\\n  advanced indexing returns a copy. Assignment via either basic or\\n  advanced indexing is in-place. See more examples in Numpy indexing\\n  documentation.\\n\\nIt\\'s also worth mentioning a few ops with special behaviors:', metadata={'source': 'https://pytorch.org/docs/stable/tensor_view.html'}), Document(page_content='torch.Tensor.view_as\\n********************\\n\\nTensor.view_as(other) -> Tensor\\n\\n   View this tensor as the same size as \"other\". \"self.view_as(other)\"\\n   is equivalent to \"self.view(other.size())\".\\n\\n   Please see \"view()\" for more information about \"view\".\\n\\n   Parameters:\\n      **other** (\"torch.Tensor\") -- The result tensor has the same\\n      size as \"other\".', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html'})]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  Why do we need to call zero_grad() in PyTorch?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   100.37 ms /   113 runs   (    0.89 ms per run)\n",
      "llama_print_timings: prompt eval time = 79778.33 ms /  1536 tokens (   51.94 ms per token)\n",
      "llama_print_timings:        eval time = 20310.61 ms /   112 runs   (  181.34 ms per run)\n",
      "llama_print_timings:       total time = 100217.32 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611ec2e180d34d2e903987131673c7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  discussion_forum_faiss_index\n",
      "\n",
      "In PyTorch, zero_grad() is a context manager which prevents calculating gradients for the given parameters. This can be used when you are evaluating your model and don't need to backward() to calculate gradients and update corresponding parameters. It can also be used to initialize weights with torch.nn.init functions, since you don't need gradients there either. \n",
      "\n",
      "In general, it is better to use zero_grad() when possible as it helps reduce the amount of computation needed for training.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: Why is grad None? In the following program, b.grad is None but a.grad is not None. Why is that? \\n     a = torch.tensor([[[[1.0]]]]).requires_grad_()     \\n    b = torch.nn.functional.interpolate(a, size=(2, 2))\\n    loss = torch.sum(b)           \\n    loss.backward()               \\n    b.grad                        \\n    a.grad\\n ANSWER: @Hovnatan_Karapetyan\\nAlso you can use: \\n b.retain_grad()\\n \\n to keep the grad.\\nPlease find more info here', metadata={'source': 'https://discuss.pytorch.org/t/55136/'}), Document(page_content='QUESTION: No_grad() vs requires_grad? I know this is a very basic question, but its my first day with pytorch and I cant seem to figure it out? What is the difference between no_grad() and requires_grad, and when to use each of them, and when/how to mix them? \\n Thanks guys. \\n Best,\\nBoris ANSWER: with torch.no_grad() is a context manager and is used to prevent calculating gradients in the following code block.\\nUsually it is used when you evaluate your model and dont need to call backward() to calculate the gradients and update the corresponding parameters.\\nAlso, you can use it to initialize the weights with torch.nn.init functions, since you dont need the gradients there either. \\n requires_grad on the other hand is used when creating a tensor, which should require gradients. Usually you dont need this in the beginning, as all parameters which require gradients are already wrapped in nn.Modules in the nn package.', metadata={'source': 'https://discuss.pytorch.org/t/21272/'}), Document(page_content='QUESTION: Why do we need to set the gradients manually to zero in pytorch?? Why do we need to set the gradients manually to zero in pytorch? e.g: \\n w1.grad.data.zero_() \\n why do we need that? What happens if we dont use that? \\n It feels that needing to ask this question means there is something conceptual/fundamental about the design of pytorch that I dont understand. ANSWER: Here are three equivalent code, with different runtime/memory comsumption.\\nAssume that you want to run sgd with a batch size of 100.\\n(I didnt run the code below there might be some typos, sorry in advance) \\n 1: single batch of 100 (least runtime, more memory) \\n # some code\\n# Initialize dataset with batch size 100\\nfor input, target in dataset:\\n    pred = net(input)\\n    loss = crit(pred, target)\\n    # one graph is created here\\n    opt.zero_grad()\\n    loss.backward()\\n    # graph is cleared here\\n    opt.step()\\n \\n 2: multiple small batches of 10 (more runtime, least memory) \\n # some code\\n# Initialize dataset with batch size 10', metadata={'source': 'https://discuss.pytorch.org/t/4903/'}), Document(page_content='Or just part of a model (e.g. finetuning a pretrained model). No need to zero grad for parameters that dont require gradients and so cannot have one.\\n \\n But Id not worry about it too much. In my experience, it is more common to use optimizer.zero_grad(), so Id probably take that unless you have a reason not to. \\n Best regards \\n Thomas', metadata={'source': 'https://discuss.pytorch.org/t/139295/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    97.20 ms /   111 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 51041.08 ms /  1019 tokens (   50.09 ms per token)\n",
      "llama_print_timings:        eval time = 17939.17 ms /   110 runs   (  163.08 ms per run)\n",
      "llama_print_timings:       total time = 69101.06 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9857f7d63ad747729e7822cd6ba30566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  blogs_faiss_index\n",
      "\n",
      "We need to call `zero_grad()` in PyTorch when we want to clear the gradients of a tensor. This is useful for example when you want to backpropagate through a layer and don't want the gradients to be calculated. It can also be used to reset the gradients of a tensor to 0.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='```python\\n>>> x = torch.zeros(1, requires_grad=True)\\n>>> with torch.no_grad():\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n>>>\\n>>> is_train = False\\n>>> with torch.set_grad_enabled(is_train):\\n...     y = x * 2\\n>>> y.requires_grad\\nFalse\\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\\n>>> y = x * 2\\n>>> y.requires_grad\\nTrue\\n>>> torch.set_grad_enabled(False)\\n>>> y = x * 2\\n>>> y.requires_grad\\nFalse\\n```\\n\\n## [`dtypes`](https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype), [`devices`](https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device) and NumPy-style creation functions', metadata={'source': 'https://pytorch.org/blog/pytorch-0_4_0-migration-guide/'}), Document(page_content='False\\n>>> x = torch.zeros(3, requires_grad=True)\\n>>> x.requires_grad\\nTrue\\n```\\n\\n##### [`torch.tensor(data, ...)`](https://pytorch.org/docs/0.4.0/torch.html#torch.tensor)\\n\\n[`torch.tensor`](https://pytorch.org/docs/0.4.0/torch.html#torch.tensor) is one of the newly added [tensor creation methods](https://pytorch.org/docs/0.4.0/torch.html#creation-ops). It takes in array-like data of all kinds and copies the contained values into a new `Tensor`. As mentioned earlier, [`torch.tensor`](https://pytorch.org/docs/0.4.0/torch.html#torch.tensor) is the PyTorch equivalent of NumPy\\'s `numpy.array`constructor. Unlike the `torch.*Tensor` methods, you can also create zero-dimensional `Tensor`s (aka scalars) this way (a single python number is treated as a Size in the `torch.*Tensor methods`). Moreover, if a `dtype` argument isn\\'t given, it will infer the suitable `dtype` given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,\\n\\n```python\\n>>> cuda = torch.device(\"cuda\")', metadata={'source': 'https://pytorch.org/blog/pytorch-0_4_0-migration-guide/'}), Document(page_content='grad_tensors_ = _make_grads(tensors, grad_tensors_)\\n    if retain_graph is None:\\n        retain_graph = create_graph\\n\\n    Variable._execution_engine.run_backward(\\n        tensors, grad_tensors_, retain_graph, create_graph, inputs,\\n        allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\\n\\n```\\nFirst, whether the `grad_tensors` argument was specified or not, there is a call to the [`_make_grads`](https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/autograd/__init__.py#L30-L74) function. This is used to check the provided `grad_tensors` or to specify the default value for them by looking at the `tensors` argument values shapes. Check the first blog post for details on the default value for the `grad_tensors` of the backward pass. This function just provides the vector of the vector jacobian product if it was not initially specified.', metadata={'source': 'https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/'}), Document(page_content=\"tensor([ 1.])\\n>>> # and no computation is wasted to compute gradients for x, y and z, which don't require grad\\n>>> z.grad == x.grad == y.grad == None\\nTrue\\n```\\n\\n#### Manipulating `requires_grad` flag\\n\\nOther than directly setting the attribute, you can change this flag `in-place` using [`my_tensor.requires_grad_()`](https://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_), or, as in the above example, at creation time by passing it in as an argument (default is `False`), e.g.,\\n\\n```python\\n>>> existing_tensor.requires_grad_()\\n>>> existing_tensor.requires_grad\\nTrue\\n>>> my_tensor = torch.zeros(3, 4, requires_grad=True)\\n>>> my_tensor.requires_grad\\nTrue\\n```\\n\\n### What about `.data?`\\n\\n`.data` was the primary way to get the underlying `Tensor` from a `Variable`. After this merge, calling `y = x.data` still has similar semantics. So `y` will be a `Tensor` that shares the same data with `x`, is unrelated with the computation history of `x`, and has `requires_grad=False`.\", metadata={'source': 'https://pytorch.org/blog/pytorch-0_4_0-migration-guide/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =    64.20 ms /    72 runs   (    0.89 ms per run)\n",
      "llama_print_timings: prompt eval time = 82840.42 ms /  1589 tokens (   52.13 ms per token)\n",
      "llama_print_timings:        eval time = 12995.41 ms /    71 runs   (  183.03 ms per run)\n",
      "llama_print_timings:       total time = 95921.01 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1e534c15284d3291a6af77b821b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  so_faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   106.05 ms /   120 runs   (    0.88 ms per run)\n",
      "llama_print_timings: prompt eval time = 44874.64 ms /   904 tokens (   49.64 ms per token)\n",
      "llama_print_timings:        eval time = 18486.58 ms /   119 runs   (  155.35 ms per run)\n",
      "llama_print_timings:       total time = 63491.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In PyTorch, for every mini-batch during the training phase, we typically want to explicitly set the gradient to zero before starting to do backpropagation (i.e., updating the weights and bias) because PyTorch accumulates the gradient on subsequent backward passes. This accumulating behavior is convenient when training RNNs or when we want to compute the gradient of the loss summated over multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradient on every loss.backward() call.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='QUESTION: why do we need to call zero_grad() in pytorch? ANSWER: in pytorch, for every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropragation (i.e., updating the weights and biases) because pytorch accumulates the gradients on subsequent backward passes. this accumulating behaviour is convenient while training rnns or when we want to compute the gradient of the loss summed over multiple mini-batches. so, the default action has been set to accumulate (i.e. sum) the gradients on every loss.backward() call.', metadata={'source': 'https://stackoverflow.com/questions/48001598/'}), Document(page_content='QUESTION: why do we need to explicitly call zero_grad()? ANSWER: we explicitly need to call zero_grad() because, after loss.backward() (when gradients are computed), we need to use optimizer.step() to proceed gradient descent. more specifically, the gradients are not automatically zeroed because these two operations, loss.backward() and optimizer.step(), are separated, and optimizer.step() requires the just computed gradients.\\n\\nin addition, sometimes, we need to accumulate gradient among some batches; to do that, we can simply call backward multiple times and optimize once.', metadata={'source': 'https://stackoverflow.com/questions/44732217/'}), Document(page_content='QUESTION: pytorch set_grad_enabled(false) vs with no_grad(): ANSWER: actually no, there no difference in the way used in the question. when you take a look at the source code of no_grad. you see that it is actually using torch.set_grad_enabled to archive this behaviour:\\n\\nclass no_grad(object):\\n    r\"\"\"context-manager that disabled gradient calculation.\\n\\n    disabling gradient calculation is useful for inference, when you are sure\\n    that you will not call :meth:`tensor.backward()`. it will reduce memory\\n    consumption for computations that would otherwise have `requires_grad=true`.\\n    in this mode, the result of every computation will have\\n    `requires_grad=false`, even when the inputs have `requires_grad=true`.\\n\\n    also functions as a decorator.\\n\\n\\n    example::\\n\\n        &gt;&gt;&gt; x = torch.tensor([1], requires_grad=true)\\n        &gt;&gt;&gt; with torch.no_grad():\\n        ...   y = x * 2\\n        &gt;&gt;&gt; y.requires_grad\\n        false\\n        &gt;&gt;&gt; @torch.no_grad()', metadata={'source': 'https://stackoverflow.com/questions/53447345/'}), Document(page_content='QUESTION: what is the purpose of with torch.no_grad(): ANSWER: the requires_grad argument tells pytorch that we want to be able to calculate the gradients for those values. however, the with torch.no_grad() tells pytorch to not calculate the gradients, and the program explicitly uses it here (as with most neural networks) in order to not update the gradients when it is updating the weights as that would affect the back propagation.', metadata={'source': 'https://stackoverflow.com/questions/72504734/'})]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46023ed69c90461c9dc6bdcf2dcc42e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  docs_faiss_index\n",
      " We need to call zero_grad() in PyTorch because it sets the gradients of all optimized \"torch.Tensor\" s to zero, which can improve performance and change certain behaviors. For example, if the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. If we don't call zero_grad(), then gradients may not be set to 0 for parameters that did not receive a gradient, which can lead to unexpected results.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "[Document(page_content='zero_grad(set_to_none=False)\\n\\n      Sets the gradients of all optimized \"torch.Tensor\" s to zero.\\n\\n      Parameters:\\n         **set_to_none** (*bool*) -- instead of setting to zero, set\\n         the grads to None. This will in general have lower memory\\n         footprint, and can modestly improve performance. However, it\\n         changes certain behaviors. For example: 1. When the user\\n         tries to access a gradient and perform manual ops on it, a\\n         None attribute or a Tensor full of 0s will behave\\n         differently. 2. If the user requests\\n         \"zero_grad(set_to_none=True)\" followed by a backward pass,\\n         \".grad\"s are guaranteed to be None for params that did not\\n         receive a gradient. 3. \"torch.optim\" optimizers have a\\n         different behavior if the gradient is 0 or None (in one case\\n         it does the step with a gradient of 0 and in the other it\\n         skips the step altogether).', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html'}), Document(page_content='torch.optim.Optimizer.zero_grad\\n*******************************\\n\\nOptimizer.zero_grad(set_to_none=False)\\n\\n   Sets the gradients of all optimized \"torch.Tensor\" s to zero.\\n\\n   Parameters:\\n      **set_to_none** (*bool*) -- instead of setting to zero, set the\\n      grads to None. This will in general have lower memory footprint,\\n      and can modestly improve performance. However, it changes\\n      certain behaviors. For example: 1. When the user tries to access\\n      a gradient and perform manual ops on it, a None attribute or a\\n      Tensor full of 0s will behave differently. 2. If the user\\n      requests \"zero_grad(set_to_none=True)\" followed by a backward\\n      pass, \".grad\"s are guaranteed to be None for params that did not\\n      receive a gradient. 3. \"torch.optim\" optimizers have a different\\n      behavior if the gradient is 0 or None (in one case it does the\\n      step with a gradient of 0 and in the other it skips the step\\n      altogether).', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'}), Document(page_content='updated in the backward pass because they won\\'t be part of the\\nbackward graph in the first place, as desired.\\n\\nBecause this is such a common pattern, \"requires_grad\" can also be set\\nat the module level with \"nn.Module.requires_grad_()\". When applied to\\na module, \".requires_grad_()\" takes effect on all of the module\\'s\\nparameters (which have \"requires_grad=True\" by default).\\n\\n\\nGrad Modes\\n----------\\n\\nApart from setting \"requires_grad\" there are also three grad modes\\nthat can be selected from Python that can affect how computations in\\nPyTorch are processed by autograd internally: default mode (grad\\nmode), no-grad mode, and inference mode, all of which can be togglable\\nvia context managers and decorators.\\n\\n\\nDefault Mode (Grad Mode)\\n------------------------\\n\\nThe \"default mode\" is the mode we are implicitly in when no other\\nmodes like no-grad and inference mode are enabled. To be contrasted\\nwith \"no-grad mode\" the default mode is also sometimes called \"grad\\nmode\".', metadata={'source': 'https://pytorch.org/docs/stable/notes/autograd.html'}), Document(page_content='footprint, and can modestly improve performance. However, it\\n         changes certain behaviors. For example: 1. When the user\\n         tries to access a gradient and perform manual ops on it, a\\n         None attribute or a Tensor full of 0s will behave\\n         differently. 2. If the user requests\\n         \"zero_grad(set_to_none=True)\" followed by a backward pass,\\n         \".grad\"s are guaranteed to be None for params that did not\\n         receive a gradient. 3. \"torch.optim\" optimizers have a\\n         different behavior if the gradient is 0 or None (in one case\\n         it does the step with a gradient of 0 and in the other it\\n         skips the step altogether).', metadata={'source': 'https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html'})]\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   399.55 ms\n",
      "llama_print_timings:      sample time =   101.39 ms /   114 runs   (    0.89 ms per run)\n",
      "llama_print_timings: prompt eval time = 65086.22 ms /  1278 tokens (   50.93 ms per token)\n",
      "llama_print_timings:        eval time = 19376.65 ms /   113 runs   (  171.47 ms per run)\n",
      "llama_print_timings:       total time = 84590.08 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for query in queries:\n",
    "    print(\"QUERY: \", query)\n",
    "    for vectordb in os.listdir('vectorstore/hf_embeddings'):\n",
    "        source = os.path.splitext(vectordb)[0]\n",
    "        vectordb = 'vectorstore/hf_embeddings/'+vectordb\n",
    "        if 'ipynb_checkpoints' in vectordb:\n",
    "            continue\n",
    "        db = FAISS.load_local(vectordb, embeddings)\n",
    "        #db = pickle.load(open(vectordb, 'rb'))\n",
    "        relevant_docs = db.similarity_search(query, k=4)\n",
    "        print(\"From \", source)\n",
    "        print(llm_chain.run(question=query, context=relevant_docs))\n",
    "        print('<<<<<<<<<<<<<<<<<<<<<<<<<<<<< context >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n')\n",
    "        print(relevant_docs)\n",
    "        print('---------------------------------------------------------------------------')\n",
    "    print(\"\\n============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
