{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e52c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from dotenv import load_dotenv\n",
    "import langchain \n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Markdown, display\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "from transformers import pipeline, TextStreamer, TextIteratorStreamer, LlamaTokenizer, LlamaForCausalLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool, create_llama_agent\n",
    "from llama_index import load_index_from_storage, download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, GPTVectorStoreIndex, PromptHelper, StorageContext, ServiceContext, LangchainEmbedding, SimpleDirectoryReader\n",
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter, TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain import LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import tracing_enabled\n",
    "from langchain.agents import Agent, BaseSingleActionAgent, LLMSingleActionAgent, initialize_agent, StructuredChatAgent, ConversationalChatAgent, ConversationalAgent, AgentExecutor, ZeroShotAgent, AgentType\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory.chat_memory import ChatMessageHistory\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.cache import RedisSemanticCache\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db31325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = \"\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "\n",
    "# import huggingface_hub as hf_hub\n",
    "\n",
    "# hf_hub.login(token=api)\n",
    "\n",
    "# ## loading llama base model and configuring it with adapter\n",
    "\n",
    "# base_model_name = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "# base_model = LlamaForCausalLM.from_pretrained(\n",
    "#             base_model_name,\n",
    "#             torch_dtype=torch.float16,\n",
    "#             device_map=\"auto\",\n",
    "#         )\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\"shrinath-suresh/alpaca-lora-all-7B\",low_cpu_mem_usage=True, use_auth_token=api,device_map=\"auto\")\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     model_name = 'shrinath-suresh/alpaca-lora-all-7B'\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         print(\":::::::::::::::::::::::::::\", prompt)\n",
    "#         inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "#         # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "#         response = model.generate(**inputs, streamer=streamer, top_p=0.75, max_new_tokens=512)\n",
    "#         response = tokenizer.decode(response[0])\n",
    "#         return response\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         return {\"name_of_model\": self.model_name}\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom\"\n",
    "\n",
    "# llm_predictor = LLMPredictor(llm=CustomLLM())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297db349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622c385999674c84a8724204c9e9aa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = \"\"\n",
    "hf_hub.login(token=token)\n",
    "\n",
    "model_id = \"jagadeesh/vicuna-13b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=False, low_cpu_mem_usage=True, device_map=\"auto\", max_memory={0:\"18GB\",1:\"18GB\",2:\"18GB\",3:\"18GB\",\"cpu\":\"10GB\"}, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer, max_new_tokens=512, device_map=\"auto\"\n",
    ")\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d889f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "HF_Embed_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "embed_model = LangchainEmbedding(HF_Embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718614b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135a3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8723f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42ce609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.storage_context.persist(persist_dir=\"./index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb91cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dir=\"~/pytorch_docs_512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea635ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=index_dir),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=index_dir),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=index_dir),\n",
    ")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "\n",
    "index_tools = [\n",
    "    LlamaIndexTool.from_tool_config(\n",
    "        IndexToolConfig(\n",
    "            name = \"PyTorch Index\",\n",
    "            query_engine=index.as_query_engine(similarity_top_k=3, response_mode=\"simple_summarize\", service_context=service_context),\n",
    "            description=f\"useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query\",\n",
    "            tool_kwargs={\"return_direct\": True, \"verbose\": True}\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "ALL_TOOLS = index_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e617c5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LlamaIndexTool(name='PyTorch Index', description='useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query', args_schema=None, return_direct=True, verbose=True, callbacks=None, callback_manager=None, query_engine=<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7f87c5eb28f0>, return_sources=False)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c454f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine(similarity_top_k=3, response_mode=\"simple_summarize\", service_context=service_context),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "48b71457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LlamaIndexTool(name='PyTorch Index', description='useful for answering questions related to pytorch. Do not use this tool for fetching dataset. Do not use this tool with the same input/query', args_schema=None, return_direct=True, verbose=True, callbacks=None, callback_manager=None, query_engine=<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7f87c5eb28f0>, return_sources=False)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [Document(page_content=t.description, metadata={\"index\": i}) for i, t in enumerate(ALL_TOOLS)]\n",
    "\n",
    "vector_store = FAISS.from_documents(docs, HF_Embed_model)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "def get_tools(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [ALL_TOOLS[d.metadata[\"index\"]] for d in docs]\n",
    "\n",
    "get_tools(\"give me the supported datasets for the {\\\"model\\\":\\\"bert-base-uncased\\\", \\\"task\\\":\\\"fill-mask\\\"}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b6d7145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain.llm_cache = RedisSemanticCache(\n",
    "#     redis_url=\"redis://localhost:6379\",\n",
    "#     embedding=HuggingFaceEmbeddings()\n",
    "# )\n",
    "langchain.llm_cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "00291e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.agents import Tool, AgentOutputParser\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3728664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from typing import Callable\n",
    "\n",
    "# Set up the base template\n",
    "template_with_history = \"\"\"You are an expert PyTorch Assistant and provide answers to questions from the developer community.\n",
    "\n",
    "Given the context and the conversation history, try to answer the question. Use only the information provided in the context. Do not use any external knowledge beyond the given conversation.\n",
    "If you think you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: use PyTorch Index tool\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 1 times)\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools_getter: Callable\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        ############## NEW ######################\n",
    "        tools = self.tools_getter(kwargs[\"input\"])\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools_getter=get_tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "677f244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = get_tools(\"\")\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "llm = hf_pipeline\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "# llm_chain = LLMChain(llm=CustomLLM(), prompt=prompt_with_history)\n",
    "\n",
    "agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser, stop=[\"</s><s>\"], allowed_tools=tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c32b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = uuid.uuid4()\n",
    "\n",
    "message_history = RedisChatMessageHistory(str(session_id), 'redis://localhost:6379/0', ttl=600)\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"history\", return_messages=True, chat_memory=message_history)\n",
    "\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, streaming=True, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7c6fc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"PyTorch preferred way to copy a tensor\",\n",
    "    \"Pytorch tensor to numpy array\",\n",
    "    \"Pytorch, what are the gradient arguments\",\n",
    "    \"How to fix RuntimeError 'Expected object of scalar type Float but got scalar type Double for argument'?\",\n",
    "    \"What does the gather function do in pytorch in layman terms?\",\n",
    "    \"How to avoid \"\"CUDA out of memory\"\" in PyTorch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "40b0210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "::::::::::::::::::::::PROMPT::::::::::::::::::\n",
      "You are an expert PyTorch Assistant and provide answers to questions from the developer community.\n",
      "\n",
      "Given the context and the conversation history, try to answer the question. Use only the information provided in the context. Do not use any external knowledge beyond the given conversation.\n",
      "If you think you don't know the answer say \"I am not sure about this, can you post the question on pytorch-discuss channel\", don't make up an answer if you don't know.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: use PyTorch Index tool\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat 1 times)\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Previous conversation history:\n",
      "[HumanMessage(content='Pytorch tensor to numpy array', additional_kwargs={}, example=False), AIMessage(content='To convert a PyTorch tensor to a numpy array, you can use the `numpy()` function. For example:\\n```\\nimport torch\\nimport numpy as np\\n\\nx = torch.tensor([1, 2, 3])\\ny = x.numpy()\\nprint(y)\\n```\\nThis will output the numpy array:\\n```\\n[1 2 3]\\n```\\nIt is important to note that the numpy() function will convert the tensor to a C-contiguous array, which may not be the most memory efficient format for large tensors. If you need more control over the memory layout of the resulting array, you can use the `torch.Tensor.to()` method to convert the tensor to a specific numpy dtype. For example:\\n```\\nx = torch.tensor([1, 2, 3], dtype=torch.float32)\\ny = x.to(dtype=np.float32).numpy()\\n```\\nThis will output the numpy array with the same float32 data type:\\n```\\n[1. 2. 3.]\\n```\\nIt is also possible to use the `torch.Tensor.detach()` method to convert the tensor to a numpy array without creating a new tensor object. This can be useful if you need to pass a tensor to a function that expects a numpy array, but do not need to modify the tensor afterwards. For example:\\n```\\nx = torch.tensor([1, 2, 3])\\ny = x.detach().numpy()\\n```\\nThis will also output the numpy array:\\n```\\n[1 2 3]\\n```\\nIt is important to note that the `detach()` method returns a new tensor object with a different memory location, so it is not suitable for passing a tensor to a function that expects a reference to the original tensor object.', additional_kwargs={}, example=False), HumanMessage(content='What are the different pytorch distributed techniques?', additional_kwargs={}, example=False), AIMessage(content='The different PyTorch distributed techniques include:\\n\\n* Distributed Data Parallel (DDP)\\n* Distributed Training (DTR)\\n* Distributed Evaluation (DE)\\n* Distributed Test (DT)', additional_kwargs={}, example=False)]\n",
      "\n",
      "New question: Explain about distributed test\n",
      "\n",
      "\n",
      "::::::::::::::::::::::PROMPT::::::::::::::::::\n",
      "Thought: Use PyTorch Distributed Test\n",
      "\n",
      "Action Input:\n",
      "\n",
      "Observation:\n",
      "\n",
      "...\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "In PyTorch, the distributed test (DT) technique allows you to run individual tests on each worker process in a distributed setup. This can be useful for testing models that are too large to be run on a single machine, or for testing models with different parameters on different machines.\n",
      "\n",
      "To use the distributed test technique in PyTorch, you will need to use the `torch.distributed.launch` function to launch the test on each worker process. This function takes the same arguments as the `torch.distributed.launch` function for distributed training, but instead of running the model on the GPU of the default process, it runs the test on the GPU of the specified worker process.\n",
      "\n",
      "For example, if you have a distributed setup with 4 worker processes, you can use the following code to run the test on each worker process:\n",
      "```\n",
      "import torch\n",
      "\n",
      "# Set the device to the GPU of the default process\n",
      "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
      "\n",
      "# Run the test on each worker process\n",
      "for i in range(1, 5):\n",
      "   torch.distributed.launch(\n",
      "       torch.nn.MSELoss(),\n",
      "       device,\n",
      "       inputs=test_inputs,\n",
      "       outputs=test_outputs,\n",
      "       args=(worker_rank,)\n",
      "   )\n",
      "```\n",
      "In this example, the `torch.nn.MSELoss()` function is used as the test loss function, and the `test_inputs` and `test_outputs` variables are the inputs and outputs of the test, respectively. The `worker_rank` variable is the rank of the current worker process, and is passed as an argument to the test function using the `args` parameter.\n",
      "\n",
      "It is important to note that the distributed test technique is only available in PyTorch 1.6 and later versions. In earlier versions, you can use the `torch.utils.data.DataLoader` class to load the test data in a distributed setup, but you will need to manually launch the test on each worker process using the `subprocess`  module.</s><s>\n",
      "\u001b[32;1m\u001b[1;3mThought: Use PyTorch Distributed Test\n",
      "\n",
      "Action Input:\n",
      "\n",
      "Observation:\n",
      "\n",
      "...\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "In PyTorch, the distributed test (DT) technique allows you to run individual tests on each worker process in a distributed setup. This can be useful for testing models that are too large to be run on a single machine, or for testing models with different parameters on different machines.\n",
      "\n",
      "To use the distributed test technique in PyTorch, you will need to use the `torch.distributed.launch` function to launch the test on each worker process. This function takes the same arguments as the `torch.distributed.launch` function for distributed training, but instead of running the model on the GPU of the default process, it runs the test on the GPU of the specified worker process.\n",
      "\n",
      "For example, if you have a distributed setup with 4 worker processes, you can use the following code to run the test on each worker process:\n",
      "```\n",
      "import torch\n",
      "\n",
      "# Set the device to the GPU of the default process\n",
      "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
      "\n",
      "# Run the test on each worker process\n",
      "for i in range(1, 5):\n",
      "    torch.distributed.launch(\n",
      "        torch.nn.MSELoss(),\n",
      "        device,\n",
      "        inputs=test_inputs,\n",
      "        outputs=test_outputs,\n",
      "        args=(worker_rank,)\n",
      "    )\n",
      "```\n",
      "In this example, the `torch.nn.MSELoss()` function is used as the test loss function, and the `test_inputs` and `test_outputs` variables are the inputs and outputs of the test, respectively. The `worker_rank` variable is the rank of the current worker process, and is passed as an argument to the test function using the `args` parameter.\n",
      "\n",
      "It is important to note that the distributed test technique is only available in PyTorch 1.6 and later versions. In earlier versions, you can use the `torch.utils.data.DataLoader` class to load the test data in a distributed setup, but you will need to manually launch the test on each worker process using the `subprocess` module.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=========================================\n",
      "In PyTorch, the distributed test (DT) technique allows you to run individual tests on each worker process in a distributed setup. This can be useful for testing models that are too large to be run on a single machine, or for testing models with different parameters on different machines.\n",
      "\n",
      "To use the distributed test technique in PyTorch, you will need to use the `torch.distributed.launch` function to launch the test on each worker process. This function takes the same arguments as the `torch.distributed.launch` function for distributed training, but instead of running the model on the GPU of the default process, it runs the test on the GPU of the specified worker process.\n",
      "\n",
      "For example, if you have a distributed setup with 4 worker processes, you can use the following code to run the test on each worker process:\n",
      "```\n",
      "import torch\n",
      "\n",
      "# Set the device to the GPU of the default process\n",
      "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
      "\n",
      "# Run the test on each worker process\n",
      "for i in range(1, 5):\n",
      "    torch.distributed.launch(\n",
      "        torch.nn.MSELoss(),\n",
      "        device,\n",
      "        inputs=test_inputs,\n",
      "        outputs=test_outputs,\n",
      "        args=(worker_rank,)\n",
      "    )\n",
      "```\n",
      "In this example, the `torch.nn.MSELoss()` function is used as the test loss function, and the `test_inputs` and `test_outputs` variables are the inputs and outputs of the test, respectively. The `worker_rank` variable is the rank of the current worker process, and is passed as an argument to the test function using the `args` parameter.\n",
      "\n",
      "It is important to note that the distributed test technique is only available in PyTorch 1.6 and later versions. In earlier versions, you can use the `torch.utils.data.DataLoader` class to load the test data in a distributed setup, but you will need to manually launch the test on each worker process using the `subprocess` module.\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain about distributed test\"\n",
    "response = agent_chain.run(input=query)\n",
    "# response = agent_chain.run(input=\"What is MPI in distributed training\")\n",
    "print(\"=========================================\")\n",
    "print(response)\n",
    "print(\"=========================================\")\n",
    "# print(query_engine.query(\"What are the different pytorch distributed techniques?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf489ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in queries:\n",
    "#     response = agent_chain.run(input=query)\n",
    "#     print(\"=========================================\")\n",
    "#     print(response)\n",
    "#     print(\"=========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1cc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98470e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ea2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
