{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26928a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799bb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3a525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import torch\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from dotenv import load_dotenv\n",
    "import langchain \n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import pipeline, TextStreamer, TextIteratorStreamer, LlamaTokenizer, LlamaForCausalLM\n",
    "import huggingface_hub as hf_hub\n",
    "\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig, LlamaIndexTool\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GPTListIndex, GPTVectorStoreIndex, PromptHelper, load_index_from_storage, StorageContext, ServiceContext, LangchainEmbedding, SimpleDirectoryReader\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import Tool, AgentOutputParser\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import tracing_enabled\n",
    "from langchain.agents import initialize_agent, LLMSingleActionAgent, StructuredChatAgent, ConversationalChatAgent, ConversationalAgent, AgentExecutor, ZeroShotAgent, AgentType\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationStringBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory.chat_memory import ChatMessageHistory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f29bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa988c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6126072a5f74880a7cc6e63784dd3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = \"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\"\n",
    "hf_hub.login(token=token)\n",
    "\n",
    "model_id = \"jagadeesh/vicuna-13b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, Timeout=5)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=False, low_cpu_mem_usage=True, device_map=\"auto\", max_memory={0:\"18GB\",1:\"18GB\",2:\"18GB\",3:\"18GB\",\"cpu\":\"10GB\"}, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer, max_new_tokens=512, device_map=\"auto\"\n",
    ")\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0448e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# HF_embed_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "HF_embed_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "embed_model = LangchainEmbedding(HF_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc745516",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd98c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs\"),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"/home/ubuntu/pytorch_docs\"),\n",
    ")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67903a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = index.as_query_engine(similarity_top_k=3, response_mode=\"simple_summarize\", service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c3aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts  import Prompt\n",
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = Prompt(\"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History> \n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\")\n",
    "\n",
    "# list of (human_message, ai_message) tuples\n",
    "custom_chat_history = [\n",
    "    (\n",
    "        'Hello assistant, we are having a insightful discussion about PyTorch.', \n",
    "        'Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=engine, \n",
    "    service_context=service_context,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b9f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What's the difference between reshape and view in pytorch?\",\n",
    "    \"What does model.train() do in PyTorch?\",\n",
    "    \"What does .contiguous() do in PyTorch?\",\n",
    "    \"Why do we \"\"pack\"\" the sequences in PyTorch?\",\n",
    "    \"Check the total number of parameters in a PyTorch model\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a20e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you explain the difference between reshape and view in PyTorch, and when each should be used in a neural  network?</s><s>\n",
      "Querying with: Can you explain the difference between reshape and view in PyTorch, and when each should be used in a neural network?\n",
      "\n",
      "Yes, I can explain the difference between reshape and view in PyTorch, and when each should be used in a neural network.\n",
      "\n",
      "Reshape and view are both methods in PyTorch that allow you to modify the shape of a tensor. However, they differ in how they approach this task.\n",
      "\n",
      "Reshape creates a new tensor with a modified shape, while view modifies the shape of the existing tensor. When you use reshape, PyTorch creates a new tensor with the specified shape and copies the data from the original tensor to the new one. This can be slow and memory-intensive, especially for large tensors. In contrast, view modifies the shape of the existing tensor without copying the data, which is faster and more memory-efficient.\n",
      "\n",
      "Reshape should be used when you want to create a new tensor with a modified shape and you don't need to preserve the original tensor's data. For example, when you want to transform the shape of a tensor to fit a specific layer's input shape or to prepare data for training.\n",
      "\n",
      "View, on the other hand, should be used when you want to modify the shape of an existing tensor and preserve its data. For example, when you want to perform an operation on a subset of the data or when you want to apply a different transformation to a subset of the data.\n",
      "\n",
      "In a neural network, you can use reshape to create new tensors with modified shapes for data preprocessing and to create tensors with specific shapes that match the input shapes of layers. You can use view to modify the shape of existing tensors and perform operations on specific subsets of the data.\n",
      "\n",
      "In summary, reshape creates a new tensor with a modified shape and copies the data, while view modifies the shape of an existing tensor without copying the data. Reshape should be used when you want to create a new tensor with a modified shape and don't need to preserve the original tensor's data, while view should be used when you want to modify the shape of an existing tensor and preserve its  data.</s><s>\n",
      "===========================\n",
      "\n",
      "Yes, I can explain the difference between reshape and view in PyTorch, and when each should be used in a neural network.\n",
      "\n",
      "Reshape and view are both methods in PyTorch that allow you to modify the shape of a tensor. However, they differ in how they approach this task.\n",
      "\n",
      "Reshape creates a new tensor with a modified shape, while view modifies the shape of the existing tensor. When you use reshape, PyTorch creates a new tensor with the specified shape and copies the data from the original tensor to the new one. This can be slow and memory-intensive, especially for large tensors. In contrast, view modifies the shape of the existing tensor without copying the data, which is faster and more memory-efficient.\n",
      "\n",
      "Reshape should be used when you want to create a new tensor with a modified shape and you don't need to preserve the original tensor's data. For example, when you want to transform the shape of a tensor to fit a specific layer's input shape or to prepare data for training.\n",
      "\n",
      "View, on the other hand, should be used when you want to modify the shape of an existing tensor and preserve its data. For example, when you want to perform an operation on a subset of the data or when you want to apply a different transformation to a subset of the data.\n",
      "\n",
      "In a neural network, you can use reshape to create new tensors with modified shapes for data preprocessing and to create tensors with specific shapes that match the input shapes of layers. You can use view to modify the shape of existing tensors and perform operations on specific subsets of the data.\n",
      "\n",
      "In summary, reshape creates a new tensor with a modified shape and copies the data, while view modifies the shape of an existing tensor without copying the data. Reshape should be used when you want to create a new tensor with a modified shape and don't need to preserve the original tensor's data, while view should be used when you want to modify the shape of an existing tensor and preserve its data.\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "response=chat_engine.chat(queries[0])\n",
    "print(\"===========================\")\n",
    "print(response.response)\n",
    "print(\"===========================\")\n",
    "# for query in queries:\n",
    "#     response=chat_engine.chat(query)\n",
    "#     print(\"===========================\")\n",
    "#     print(response.response)\n",
    "#     print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a1519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f82c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
