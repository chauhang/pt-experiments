{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "from llama_index import download_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_text(page_iter, src_name):\n",
    "    documents = []\n",
    "    for page in page_iter:\n",
    "        documents.append([page['text'], page['metadata']])\n",
    "    \n",
    "    if not os.path.exists('so_txt'):\n",
    "        os.makedirs('so_txt')\n",
    "    \n",
    "    with open(f\"so_txt/{src_name}.txt\", \"w\") as file:\n",
    "        for item in documents:\n",
    "            file.write(str(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_post_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34750268</td>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "      <td>as of pull request #496 torch now includes a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38543850</td>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "      <td>it is quite easy to do if you have the image i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41767005</td>\n",
       "      <td>python wheels: cp27mu not supported</td>\n",
       "      <td>this is exactly that. \\nrecompile python under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41861354</td>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch</td>\n",
       "      <td>as of pytorch 1.0 torch.utils.serialization is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41924453</td>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "      <td>yes, that is possible. just create the objects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10758</th>\n",
       "      <td>74612146</td>\n",
       "      <td>is it possible to perform quantization on dens...</td>\n",
       "      <td>here's how to do this on densenet169 from torc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>74637151</td>\n",
       "      <td>why when the batch size increased, the epoch t...</td>\n",
       "      <td>as you already noticed, there are many factors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10760</th>\n",
       "      <td>74642594</td>\n",
       "      <td>why does stablediffusionpipeline return black ...</td>\n",
       "      <td>apparently it is indeed an apple silicon (m1/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10761</th>\n",
       "      <td>74671399</td>\n",
       "      <td>locating tags in a string in php (with respect...</td>\n",
       "      <td>i think i've got something. how about this:\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10762</th>\n",
       "      <td>74679922</td>\n",
       "      <td>bgr to rgb for cub_200 images by image.split()</td>\n",
       "      <td>i would strongly recommend you use skimage.io ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10763 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pt_post_id                                           question  \\\n",
       "0        34750268  extracting the top-k value-indices from a 1-d ...   \n",
       "1        38543850  how to display custom images in tensorboard (e...   \n",
       "2        41767005                python wheels: cp27mu not supported   \n",
       "3        41861354     loading torch7 trained models (.t7) in pytorch   \n",
       "4        41924453  pytorch: how to use dataloaders for custom dat...   \n",
       "...           ...                                                ...   \n",
       "10758    74612146  is it possible to perform quantization on dens...   \n",
       "10759    74637151  why when the batch size increased, the epoch t...   \n",
       "10760    74642594  why does stablediffusionpipeline return black ...   \n",
       "10761    74671399  locating tags in a string in php (with respect...   \n",
       "10762    74679922     bgr to rgb for cub_200 images by image.split()   \n",
       "\n",
       "                                                  answer  \n",
       "0      as of pull request #496 torch now includes a b...  \n",
       "1      it is quite easy to do if you have the image i...  \n",
       "2      this is exactly that. \\nrecompile python under...  \n",
       "3      as of pytorch 1.0 torch.utils.serialization is...  \n",
       "4      yes, that is possible. just create the objects...  \n",
       "...                                                  ...  \n",
       "10758  here's how to do this on densenet169 from torc...  \n",
       "10759  as you already noticed, there are many factors...  \n",
       "10760  apparently it is indeed an apple silicon (m1/m...  \n",
       "10761  i think i've got something. how about this:\\nf...  \n",
       "10762  i would strongly recommend you use skimage.io ...  \n",
       "\n",
       "[10763 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../pt_question_answers_updated.csv')\n",
    "\n",
    "CLEANR = re.compile('<.*?>')\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "    \n",
    "df[\"pt_answer\"] = df[\"pt_answer\"].apply(lambda x: cleanhtml(x))\n",
    "\n",
    "df[\"question\"] = df[\"pt_title\"].str.lower()\n",
    "df[\"answer\"] = df[\"pt_answer\"].str.lower()\n",
    "\n",
    "df = df[['pt_post_id','question', 'answer']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get qa and link to post\n",
    "def get_so(df):\n",
    "    for index, row in df.iterrows():\n",
    "        text = \"QUESTION: \" + row['question'] + ' ANSWER: ' + row['answer']\n",
    "        yield {'text': text, 'metadata': {'source': f\"https://stackoverflow.com/questions/{row['pt_post_id']}/\"}}\n",
    "\n",
    "\n",
    "docs = save_data_text(get_so(df), 'so')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example:\n",
    "https://pypi.org/project/llama-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")\n",
    "\n",
    "documents = SimpleDirectoryReader('so_txt').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### links:\n",
    "\n",
    "GPTSimpleVectorIndex: https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store.html#gpt_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex\n",
    "\n",
    "default llm predictor used: https://github.com/jerryjliu/llama_index/blob/b2e0a8f76fcf97cf16e1ddcb9f4604a0f890c935/gpt_index/llm_predictor/base.py#L143\n",
    "\n",
    "to use custom llm predictor: https://github.com/jerryjliu/llama_index/issues/1216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 3964152 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4201 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 13 tokens\n"
     ]
    }
   ],
   "source": [
    "output = index.query(\"How do I check if PyTorch is using the GPU?\")\n",
    "\n",
    "# output = index.query(\"How do I check if PyTorch is using the GPU?\", similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You can check if PyTorch is using the GPU by running the command torch.cuda.is_available() which will return a boolean value indicating whether a GPU is available or not. Additionally, you can use torch.cuda.device_count() to check the number of GPUs available. You can also use torch.cuda.get_device_name(0) to get the name of the GPU being used.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4197 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No, PyTorch does not work on Windows 32-bit. PyTorch is only supported on 64-bit Windows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4325 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To make your experiment deterministic, you need to set the random seed for both numpy and pytorch, and set the backend to deterministic operations. This can be done by setting the seed for numpy with np.random.seed() and for pytorch with torch.manual_seed(). Additionally, you can set the torch.backends.cudnn.deterministic flag to True. Additionally, you can try setting the .requires_grad attribute to false before the update and restoring it after the update. For example, for each layer in your model, you can set layer.bias.requires_grad = false and layer.weight.requires_grad = false before the update and then restore it after the update.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4289 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One way to scale up your Pytorch models is to use the nn.DataParallel module. This module allows you to parallelize your model across multiple GPUs, which can significantly speed up training and inference. Additionally, you can use the torch.distributed package to distribute your model across multiple machines. This can further increase the speed of training and inference. Additionally, you can use the nn.ModuleList to register all linear layers inside the module's __init__, which can make defining your Pytorch fully connected model more simple and convenient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4384 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It is possible that your training is slow due to a few different factors. Firstly, you may not be using an optimizer that is suitable for your model. For example, if you are using a deep learning model, you may need to use an optimizer such as Adam or SGD. Secondly, you may not be using the correct learning rate for your model. If the learning rate is too high, it can cause the model to overfit and slow down the training process. Finally, you may not be using the correct batch size for your model. If the batch size is too small, it can cause the model to take longer to train. Additionally, it is important to ensure that you are using the correct callback functions in your script, as this can affect the speed of your training.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Does PyTorch work on windows 32-bit?\",\n",
    "    \"How do I make my experiment deterministic?\",\n",
    "    \"How should I scale up my Pytorch models?\",\n",
    "    \"Why is my training so slow?\"\n",
    "]\n",
    "\n",
    "for i in queries:\n",
    "    print(index.query(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}