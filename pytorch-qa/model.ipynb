{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0daf8087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 18:17:32 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\r\n",
      "| N/A   34C    P0    56W / 300W |      0MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    36W / 300W |      0MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    37W / 300W |      0MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   31C    P0    39W / 300W |      0MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    " !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d30f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers sentencepiece pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1642e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.6\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4451a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4feae290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4edf13d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "     AdamW,\n",
    "     T5ForConditionalGeneration,\n",
    "     T5Tokenizer,\n",
    "     get_linear_schedule_with_warmup\n",
    " )\n",
    "# Seeds all the processes including numpy torch and other imported modules.\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc1cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ae089",
   "metadata": {},
   "source": [
    "### Data extraction and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10920da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pt_question_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55662e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'pt_post_id', 'pt_post_type_id', 'pt_accepted_answer_id',\n",
       "       'pt_creation_date', 'pt_score', 'pt_title', 'pt_body', 'pt_tags',\n",
       "       'pt_parent_id', 'pt_answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60e8bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pt_post_id</th>\n",
       "      <th>pt_post_type_id</th>\n",
       "      <th>pt_accepted_answer_id</th>\n",
       "      <th>pt_creation_date</th>\n",
       "      <th>pt_score</th>\n",
       "      <th>pt_title</th>\n",
       "      <th>pt_body</th>\n",
       "      <th>pt_tags</th>\n",
       "      <th>pt_parent_id</th>\n",
       "      <th>pt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27837861</td>\n",
       "      <td>34750268</td>\n",
       "      <td>1</td>\n",
       "      <td>34762233.0</td>\n",
       "      <td>2016-01-12T17:36:25.473</td>\n",
       "      <td>9</td>\n",
       "      <td>Extracting the top-k value-indices from a 1-D ...</td>\n",
       "      <td>&lt;p&gt;Given a 1-D tensor in Torch (&lt;code&gt;torch.Te...</td>\n",
       "      <td>&lt;python&gt;&lt;lua&gt;&lt;pytorch&gt;&lt;torch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;As of pull request &lt;a href=\"https://github....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30769673</td>\n",
       "      <td>38543850</td>\n",
       "      <td>1</td>\n",
       "      <td>38676842.0</td>\n",
       "      <td>2016-07-23T16:15:43.967</td>\n",
       "      <td>40</td>\n",
       "      <td>How to Display Custom Images in Tensorboard (e...</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"https://github.com/tensorflow/...</td>\n",
       "      <td>&lt;python&gt;&lt;tensorflow&gt;&lt;matplotlib&gt;&lt;pytorch&gt;&lt;tens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;It is quite easy to do if you have the imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33236300</td>\n",
       "      <td>41767005</td>\n",
       "      <td>1</td>\n",
       "      <td>43824857.0</td>\n",
       "      <td>2017-01-20T15:22:08.063</td>\n",
       "      <td>11</td>\n",
       "      <td>Python wheels: cp27mu not supported</td>\n",
       "      <td>&lt;p&gt;I'm trying to install pytorch (&lt;a href=\"htt...</td>\n",
       "      <td>&lt;python&gt;&lt;linux&gt;&lt;unicode&gt;&lt;pytorch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Yes, that is possible. Just create the obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33307877</td>\n",
       "      <td>41861354</td>\n",
       "      <td>1</td>\n",
       "      <td>54261158.0</td>\n",
       "      <td>2017-01-25T20:45:35.297</td>\n",
       "      <td>8</td>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch</td>\n",
       "      <td>&lt;p&gt;I am using Torch7 library for implementing ...</td>\n",
       "      <td>&lt;python&gt;&lt;lua&gt;&lt;pytorch&gt;&lt;torch&gt;&lt;pre-trained-model&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;&lt;code&gt;view()&lt;/code&gt; reshapes the tensor wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33355427</td>\n",
       "      <td>41924453</td>\n",
       "      <td>1</td>\n",
       "      <td>42054194.0</td>\n",
       "      <td>2017-01-29T18:31:24.687</td>\n",
       "      <td>65</td>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Dat...</td>\n",
       "      <td>&lt;p&gt;How to make use of the &lt;code&gt;torch.utils.da...</td>\n",
       "      <td>&lt;python&gt;&lt;torch&gt;&lt;pytorch&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;While you will not get as detailed informat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  pt_post_id  pt_post_type_id  pt_accepted_answer_id  \\\n",
       "0    27837861    34750268                1             34762233.0   \n",
       "1    30769673    38543850                1             38676842.0   \n",
       "2    33236300    41767005                1             43824857.0   \n",
       "3    33307877    41861354                1             54261158.0   \n",
       "4    33355427    41924453                1             42054194.0   \n",
       "\n",
       "          pt_creation_date  pt_score  \\\n",
       "0  2016-01-12T17:36:25.473         9   \n",
       "1  2016-07-23T16:15:43.967        40   \n",
       "2  2017-01-20T15:22:08.063        11   \n",
       "3  2017-01-25T20:45:35.297         8   \n",
       "4  2017-01-29T18:31:24.687        65   \n",
       "\n",
       "                                            pt_title  \\\n",
       "0  Extracting the top-k value-indices from a 1-D ...   \n",
       "1  How to Display Custom Images in Tensorboard (e...   \n",
       "2                Python wheels: cp27mu not supported   \n",
       "3     Loading Torch7 trained models (.t7) in PyTorch   \n",
       "4  PyTorch: How to use DataLoaders for custom Dat...   \n",
       "\n",
       "                                             pt_body  \\\n",
       "0  <p>Given a 1-D tensor in Torch (<code>torch.Te...   \n",
       "1  <p>The <a href=\"https://github.com/tensorflow/...   \n",
       "2  <p>I'm trying to install pytorch (<a href=\"htt...   \n",
       "3  <p>I am using Torch7 library for implementing ...   \n",
       "4  <p>How to make use of the <code>torch.utils.da...   \n",
       "\n",
       "                                             pt_tags  pt_parent_id  \\\n",
       "0                      <python><lua><pytorch><torch>           NaN   \n",
       "1  <python><tensorflow><matplotlib><pytorch><tens...           NaN   \n",
       "2                  <python><linux><unicode><pytorch>           NaN   \n",
       "3   <python><lua><pytorch><torch><pre-trained-model>           NaN   \n",
       "4                           <python><torch><pytorch>           NaN   \n",
       "\n",
       "                                           pt_answer  \n",
       "0  <p>As of pull request <a href=\"https://github....  \n",
       "1  <p>It is quite easy to do if you have the imag...  \n",
       "2  <p>Yes, that is possible. Just create the obje...  \n",
       "3  <p><code>view()</code> reshapes the tensor wit...  \n",
       "4  <p>While you will not get as detailed informat...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6b314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae44cbc",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d619a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME ='t5-base' \n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89bc6d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting the top-k value-indices from a 1-D Tensor'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, \"pt_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7be57e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Given a 1-D tensor in Torch (<code>torch.Tensor</code>), containing values which can be compared (say floating point), how can we extract the indices of the top-<em>k</em> values in that tensor?</p>\\n<p>Apart from the brute-force method, I am looking for some API call, that Torch/lua provides, which can perform this task efficiently.</p>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, \"pt_body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598fcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = tokenizer(data.loc[0, \"pt_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b93556cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1355c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 102, 3155, 517, 757, 29, 3, 9, 8218, 308, 3, 324, 7, 127, 16, 3794, 524, 41, 2, 4978, 3155, 17, 127, 524, 5, 382, 35, 7, 127, 2, 87, 4978, 3155, 201, 3, 6443, 2620, 84, 54, 36, 3, 2172, 41, 8735, 12848, 500, 201, 149, 54, 62, 5819, 8, 3, 19082, 7, 13, 8, 420, 18, 2, 15, 51, 3155, 157, 2, 87, 15, 51, 3155, 2620, 16, 24, 3, 324, 7, 127, 58, 2, 87, 102, 3155, 3, 2, 102, 3155, 188, 2274, 45, 8, 18343, 15, 18, 10880, 1573, 6, 27, 183, 479, 21, 128, 6429, 580, 6, 24, 3794, 524, 87, 40, 76, 9, 795, 6, 84, 54, 1912, 48, 2491, 8877, 5, 2, 87, 102, 3155, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"input_ids\"])\n",
    "print(sample_encoding[\"attention_mask\"])\n",
    "print(len(sample_encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82d162ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ca8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [\n",
    "      tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "      for input_id in sample_encoding['input_ids']\n",
    "]\n",
    "\n",
    "preds= \" \".join(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84df588a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <unk> p > G ive n  a 1- D  ten s or in Tor ch ( <unk> code > t or ch . T en s\n",
      "or <unk> / code > ),  containing values which can be  compared ( say floating\n",
      "point ), how can we extract the  indice s of the top - <unk> e m > k <unk> / e m\n",
      "> values in that  ten s or ? <unk> / p >  <unk> p > A part from the brut e -\n",
      "force method , I am looking for some API call , that Tor ch / l u a provides ,\n",
      "which can perform this task efficiently . <unk> / p > </s>\n"
     ]
    }
   ],
   "source": [
    "for wrap in textwrap.wrap(preds, width = 80):\n",
    "    print(wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32caface",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "     data.loc[0, \"pt_title\"],\n",
    "     data.loc[0, \"pt_body\"],\n",
    "     max_length=512,\n",
    "     padding='max_length',\n",
    "     truncation=\"only_second\",\n",
    "     return_attention_mask=True,\n",
    "     add_special_tokens=True,\n",
    "     return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be51a7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f892394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54af4849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token, tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07c9ff91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extracting the top-k value-indices from a 1-D Tensor</s> <unk> p>Given a 1-D tensor in Torch (<unk> code>torch.Tensor<unk> /code>), containing values which can be compared (say floating point), how can we extract the indices of the top-<unk> em>k<unk> /em> values in that tensor?<unk> /p> <unk> p>Apart from the brute-force method, I am looking for some API call, that Torch/lua provides, which can perform this task efficiently.<unk> /p></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding['input_ids'].squeeze()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c77457",
   "metadata": {},
   "source": [
    "### Creating labels for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5859c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>As of pull request <a href=\"https://github.com/torch/torch7/pull/496\" rel=\"noreferrer\">#496</a> Torch now includes a built-in API named <a href=\"https://github.com/torch/torch7/blob/03c04c6/doc/maths.md#torchtopkresval-resind-x-k-dim-dir-sort\" rel=\"noreferrer\"><code>torch.topk</code></a>. Example:</p>\\n\\n<pre><code>&gt; t = torch.Tensor{9, 1, 8, 2, 7, 3, 6, 4, 5}\\n\\n-- obtain the 3 smallest elements\\n&gt; res = t:topk(3)\\n&gt; print(res)\\n 1\\n 2\\n 3\\n[torch.DoubleTensor of size 3]\\n\\n-- you can also get the indices in addition\\n&gt; res, ind = t:topk(3)\\n&gt; print(ind)\\n 2\\n 4\\n 6\\n[torch.LongTensor of size 3]\\n\\n-- alternatively you can obtain the k largest elements as follow\\n-- (see the API documentation for more details)\\n&gt; res = t:topk(3, true)\\n&gt; print(res)\\n 9\\n 8\\n 7\\n[torch.DoubleTensor of size 3]\\n</code></pre>\\n\\n<p>At the time of writing the CPU implementation follows a <a href=\"https://github.com/wickedfoo/torch7/blob/ef019670474b69629a8b3d50eb426d5858bd5c45/lib/TH/generic/THTensorMath.c#L1757-L1769\" rel=\"noreferrer\">sort and narrow approach</a> (there are plans to improve it in the future). That being said an optimized GPU implementation for cutorch is currently being <a href=\"https://github.com/torch/cutorch/pull/296\" rel=\"noreferrer\">reviewed</a>.</p>\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, \"pt_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969a6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding = tokenizer(\n",
    "     data.loc[0, \"pt_answer\"],\n",
    "     max_length=512,\n",
    "     padding='max_length',\n",
    "     truncation=True,\n",
    "     return_attention_mask=True,\n",
    "     add_special_tokens=True,\n",
    "     return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99341d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> p>As of pull request <unk> a href=\"https://github.com/torch/torch7/pull/496\" rel=\"noreferrer\">#496<unk> /a> Torch now includes a built-in API named <unk> a href=\"https://github.com/torch/torch7/blob/03c04c6/doc/maths.md#torchtopkresval-resind-x-k-dim-dir-sort\" rel=\"noreferrer\"><unk> code>torch.topk<unk> /code><unk> /a>. Example:<unk> /p> <unk> pre><unk> code>&gt; t = torch.Tensor<unk> 9, 1, 8, 2, 7, 3, 6, 4, 5<unk> -- obtain the 3 smallest elements &gt; res = t:topk(3) &gt; print(res) 1 2 3 [torch.DoubleTensor of size 3] -- you can also get the indices in addition &gt; res, ind = t:topk(3) &gt; print(ind) 2 4 6 [torch.LongTensor of size 3] -- alternatively you can obtain the k largest elements as follow -- (see the API documentation for more details) &gt; res = t:topk(3, true) &gt; print(res) 9 8 7 [torch.DoubleTensor of size 3] <unk> /code><unk> /pre> <unk> p>At the time of writing the CPU implementation follows a <unk> a href=\"https://github.com/wickedfoo/torch7/blob/ef019670474b69629a8b3d50eb426d5858bd5c45/lib/TH/generic/THTensorMath.c#L1757-L1769\" rel=\"noreferrer\">sort and narrow approach<unk> /a> (</s>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(answer_encoding['input_ids'].squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6ba3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = answer_encoding[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45fd181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,     2,   102,  3155,   188,     7,    13,  3197,  1690,     3,\n",
       "             2,     9,     3,   107,    60,    89, 17592,  5948,     7,  1303,\n",
       "         12651, 16420,     5,   287,    87,    17,   127,   524,    87,    17,\n",
       "           127,   524,   940,    87,  4801,    40, 13572,  4314,   121,  8318,\n",
       "         17592,    29,   127,    15,  1010,    52,    49,   121,  3155,  4663,\n",
       "           591,  4314,     2,    87,     9,  3155,  3794,   524,   230,   963,\n",
       "             3,     9,  1192,    18,    77,  6429,  2650,     3,     2,     9,\n",
       "             3,   107,    60,    89, 17592,  5948,     7,  1303, 12651, 16420,\n",
       "             5,   287,    87,    17,   127,   524,    87,    17,   127,   524,\n",
       "           940,    87,  4672,   115,    87,  4928,    75,  6348,    75,   948,\n",
       "            87,  7171,    87,  3357,   107,     7,     5,    51,    26,  4663,\n",
       "            17,   127,   524,  2916,   157,    60,     7,  2165,    18,    60,\n",
       "             7,    77,    26,    18,   226,    18,   157,    18,    26,   603,\n",
       "            18, 12594,    18,  9309,   121,  8318, 17592,    29,   127,    15,\n",
       "          1010,    52,    49,   121,  3155,     2,  4978,  3155,    17,   127,\n",
       "           524,     5,  2916,   157,     2,    87,  4978,  3155,     2,    87,\n",
       "             9,  3155,     5, 18792,    10,     2,    87,   102,  3155,     3,\n",
       "             2,  2026,  3155,     2,  4978,  3155,   184,   122,    17,   117,\n",
       "             3,    17,  3274, 26037,     5,   382,    35,     7,   127,     2,\n",
       "          1298,     6,  1914,  9478,  3547,  7973,  6180,  8580,  6464,   305,\n",
       "             2,  1636,  3442,     8,   220,     3, 17924,  2479,     3,   184,\n",
       "           122,    17,   117,     3,    60,     7,  3274,     3,    17,    10,\n",
       "          2916,   157, 17867,     3,   184,   122,    17,   117,  2281,   599,\n",
       "            60,     7,    61,   209,   204,   220,   784,    17,   127,   524,\n",
       "             5,  4135,    76,  2296,   382,    35,     7,   127,    13,   812,\n",
       "           220,   908,  1636,    25,    54,    92,   129,     8,     3, 19082,\n",
       "             7,    16,   811,     3,   184,   122,    17,   117,     3,    60,\n",
       "             7,     6,    16,    26,  3274,     3,    17,    10,  2916,   157,\n",
       "         17867,     3,   184,   122,    17,   117,  2281,   599,    77,    26,\n",
       "            61,   204,   314,   431,   784,    17,   127,   524,     5,   434,\n",
       "          2444,   382,    35,     7,   127,    13,   812,   220,   908,  1636,\n",
       "          2433,   120,    25,    54,  3442,     8,     3,   157,  2015,  2479,\n",
       "            38,  1130,  1636,    41,  2338,     8,  6429,  7192,    21,    72,\n",
       "          1030,    61,     3,   184,   122,    17,   117,     3,    60,     7,\n",
       "          3274,     3,    17,    10,  2916,   157,   599,  6355,  1176,    61,\n",
       "             3,   184,   122,    17,   117,  2281,   599,    60,     7,    61,\n",
       "           668,   505,   489,   784,    17,   127,   524,     5,  4135,    76,\n",
       "          2296,   382,    35,     7,   127,    13,   812,   220,   908,     3,\n",
       "             2,    87,  4978,  3155,     2,    87,  2026,  3155,     3,     2,\n",
       "           102,  3155,   188,    17,     8,    97,    13,   913,     8, 13823,\n",
       "          4432,  6963,     3,     9,     3,     2,     9,     3,   107,    60,\n",
       "            89, 17592,  5948,     7,  1303, 12651, 16420,     5,   287,    87,\n",
       "          5981,    15,    26,    89,    32,    32,    87,    17,   127,   524,\n",
       "           940,    87,  4672,   115,    87,    15,    89,  4542,  4314,  2518,\n",
       "          4177,   591,   115,  3951,   948,  3166,     9,   927,   115,   519,\n",
       "            26,  1752,    15,   115,   591,  2688,    26,  3449,  3449,   115,\n",
       "            26,   755,    75,  2128,    87,  6856,    87,  4611,    87,   729,\n",
       "            15,  2234,    87,  4611,   382,    35,     7,   127,   329,     9,\n",
       "           189,     5,    75,  4663,   434,  2517,  3436,    18,   434,  2517,\n",
       "          3951,   121,  8318, 17592,    29,   127,    15,  1010,    52,    49,\n",
       "           121,  3155,  9309,    11,  5658,  1295,     2,    87,     9,  3155,\n",
       "            41,     1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6ac616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,     2,   102,  3155,   188,     7,    13,  3197,  1690,     3,\n",
       "             2,     9,     3,   107,    60,    89, 17592,  5948,     7,  1303,\n",
       "         12651, 16420,     5,   287,    87,    17,   127,   524,    87,    17,\n",
       "           127,   524,   940,    87,  4801,    40, 13572,  4314,   121,  8318,\n",
       "         17592,    29,   127,    15,  1010,    52,    49,   121,  3155,  4663,\n",
       "           591,  4314,     2,    87,     9,  3155,  3794,   524,   230,   963,\n",
       "             3,     9,  1192,    18,    77,  6429,  2650,     3,     2,     9,\n",
       "             3,   107,    60,    89, 17592,  5948,     7,  1303, 12651, 16420,\n",
       "             5,   287,    87,    17,   127,   524,    87,    17,   127,   524,\n",
       "           940,    87,  4672,   115,    87,  4928,    75,  6348,    75,   948,\n",
       "            87,  7171,    87,  3357,   107,     7,     5,    51,    26,  4663,\n",
       "            17,   127,   524,  2916,   157,    60,     7,  2165,    18,    60,\n",
       "             7,    77,    26,    18,   226,    18,   157,    18,    26,   603,\n",
       "            18, 12594,    18,  9309,   121,  8318, 17592,    29,   127,    15,\n",
       "          1010,    52,    49,   121,  3155,     2,  4978,  3155,    17,   127,\n",
       "           524,     5,  2916,   157,     2,    87,  4978,  3155,     2,    87,\n",
       "             9,  3155,     5, 18792,    10,     2,    87,   102,  3155,     3,\n",
       "             2,  2026,  3155,     2,  4978,  3155,   184,   122,    17,   117,\n",
       "             3,    17,  3274, 26037,     5,   382,    35,     7,   127,     2,\n",
       "          1298,     6,  1914,  9478,  3547,  7973,  6180,  8580,  6464,   305,\n",
       "             2,  1636,  3442,     8,   220,     3, 17924,  2479,     3,   184,\n",
       "           122,    17,   117,     3,    60,     7,  3274,     3,    17,    10,\n",
       "          2916,   157, 17867,     3,   184,   122,    17,   117,  2281,   599,\n",
       "            60,     7,    61,   209,   204,   220,   784,    17,   127,   524,\n",
       "             5,  4135,    76,  2296,   382,    35,     7,   127,    13,   812,\n",
       "           220,   908,  1636,    25,    54,    92,   129,     8,     3, 19082,\n",
       "             7,    16,   811,     3,   184,   122,    17,   117,     3,    60,\n",
       "             7,     6,    16,    26,  3274,     3,    17,    10,  2916,   157,\n",
       "         17867,     3,   184,   122,    17,   117,  2281,   599,    77,    26,\n",
       "            61,   204,   314,   431,   784,    17,   127,   524,     5,   434,\n",
       "          2444,   382,    35,     7,   127,    13,   812,   220,   908,  1636,\n",
       "          2433,   120,    25,    54,  3442,     8,     3,   157,  2015,  2479,\n",
       "            38,  1130,  1636,    41,  2338,     8,  6429,  7192,    21,    72,\n",
       "          1030,    61,     3,   184,   122,    17,   117,     3,    60,     7,\n",
       "          3274,     3,    17,    10,  2916,   157,   599,  6355,  1176,    61,\n",
       "             3,   184,   122,    17,   117,  2281,   599,    60,     7,    61,\n",
       "           668,   505,   489,   784,    17,   127,   524,     5,  4135,    76,\n",
       "          2296,   382,    35,     7,   127,    13,   812,   220,   908,     3,\n",
       "             2,    87,  4978,  3155,     2,    87,  2026,  3155,     3,     2,\n",
       "           102,  3155,   188,    17,     8,    97,    13,   913,     8, 13823,\n",
       "          4432,  6963,     3,     9,     3,     2,     9,     3,   107,    60,\n",
       "            89, 17592,  5948,     7,  1303, 12651, 16420,     5,   287,    87,\n",
       "          5981,    15,    26,    89,    32,    32,    87,    17,   127,   524,\n",
       "           940,    87,  4672,   115,    87,    15,    89,  4542,  4314,  2518,\n",
       "          4177,   591,   115,  3951,   948,  3166,     9,   927,   115,   519,\n",
       "            26,  1752,    15,   115,   591,  2688,    26,  3449,  3449,   115,\n",
       "            26,   755,    75,  2128,    87,  6856,    87,  4611,    87,   729,\n",
       "            15,  2234,    87,  4611,   382,    35,     7,   127,   329,     9,\n",
       "           189,     5,    75,  4663,   434,  2517,  3436,    18,   434,  2517,\n",
       "          3951,   121,  8318, 17592,    29,   127,    15,  1010,    52,    49,\n",
       "           121,  3155,  9309,    11,  5658,  1295,     2,    87,     9,  3155,\n",
       "            41,     1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[labels == 0] = -100\n",
    "labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd79f6e",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15f3416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        source_encoding = tokenizer(\n",
    "            data_row[\"pt_title\"],\n",
    "            data_row[\"pt_body\"],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_second\",\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_encoding = tokenizer(\n",
    "            data_row[\"pt_answer\"],\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "        return dict(\n",
    "            question=data_row[\"pt_title\"],\n",
    "            context=data_row[\"pt_body\"],\n",
    "            answer_text=data_row[\"pt_answer\"],\n",
    "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=source_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9914926",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = SODataset(data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9d06b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Extracting the top-k value-indices from a 1-D Tensor\n",
      "Answer text:  <p>As of pull request <a href=\"https://github.com/torch/torch7/pull/496\" rel=\"noreferrer\">#496</a> Torch now includes a built-in API named <a href=\"https://github.com/torch/torch7/blob/03c04c6/doc/maths.md#torchtopkresval-resind-x-k-dim-dir-sort\" rel=\"noreferrer\"><code>torch.topk</code></a>. Example:</p>\n",
      "\n",
      "<pre><code>&gt; t = torch.Tensor{9, 1, 8, 2, 7, 3, 6, 4, 5}\n",
      "\n",
      "-- obtain the 3 smallest elements\n",
      "&gt; res = t:topk(3)\n",
      "&gt; print(res)\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.DoubleTensor of size 3]\n",
      "\n",
      "-- you can also get the indices in addition\n",
      "&gt; res, ind = t:topk(3)\n",
      "&gt; print(ind)\n",
      " 2\n",
      " 4\n",
      " 6\n",
      "[torch.LongTensor of size 3]\n",
      "\n",
      "-- alternatively you can obtain the k largest elements as follow\n",
      "-- (see the API documentation for more details)\n",
      "&gt; res = t:topk(3, true)\n",
      "&gt; print(res)\n",
      " 9\n",
      " 8\n",
      " 7\n",
      "[torch.DoubleTensor of size 3]\n",
      "</code></pre>\n",
      "\n",
      "<p>At the time of writing the CPU implementation follows a <a href=\"https://github.com/wickedfoo/torch7/blob/ef019670474b69629a8b3d50eb426d5858bd5c45/lib/TH/generic/THTensorMath.c#L1757-L1769\" rel=\"noreferrer\">sort and narrow approach</a> (there are plans to improve it in the future). That being said an optimized GPU implementation for cutorch is currently being <a href=\"https://github.com/torch/cutorch/pull/296\" rel=\"noreferrer\">reviewed</a>.</p>\n",
      "\n",
      "Input_ids:  tensor([18742,    53,     8,   420,    18,   157,   701,    18, 19082,     7])\n",
      "Labels:  tensor([   3,    2,  102, 3155,  188,    7,   13, 3197, 1690,    3])\n"
     ]
    }
   ],
   "source": [
    "for sample_data in sample_dataset:\n",
    "    print(\"Question: \", sample_data['question'])\n",
    "    print(\"Answer text: \", sample_data['answer_text'])\n",
    "    print(\"Input_ids: \", sample_data['input_ids'][:10])\n",
    "    print(\"Labels: \", sample_data['labels'][:10])\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c37a6",
   "metadata": {},
   "source": [
    "### Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6449610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13863, 11), (730, 11))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(data, test_size=0.05)\n",
    "train_df.shape,  val_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e56d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494cf83e",
   "metadata": {},
   "source": [
    "### Create PTL Data module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "179edef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SODataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 4,\n",
    "        source_max_token_len: int = 512,\n",
    "        target_max_token_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.train_dataset = SODataset(\n",
    "            self.train_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len\n",
    "        )\n",
    "        self.test_dataset = SODataset(\n",
    "            self.test_df, self.tokenizer, self.source_max_token_len, self.target_max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f14bbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "N_EPOCHS = 1\n",
    "data_module = SODataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615422c7",
   "metadata": {},
   "source": [
    "### Loading T5 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4edd4255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"t5-base\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 3072,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bd92201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids_translated = tokenizer(\n",
    "    \"translate English to German : Oppertunity did not knock until I built a door\",\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids\n",
    "generated_ids = model.generate(input_ids=input_ids_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4a25eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_translated = [\n",
    "    tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fb839d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Gelegenheit klopfte erst, als ich eine Tür gebaut hatte.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(pred_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26215dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4384bd",
   "metadata": {},
   "source": [
    "### Model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94d2ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "260d2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SOModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f37f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SOModel(\n",
       "  (model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f58fa06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=4)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=4)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "# logger = TensorBoardLogger(\"training-logs\", name=\"bio-qa\")\n",
    "logger = TensorBoardLogger(\"training-logs\", name=\"so-qa\")\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    max_epochs=1,\n",
    "    gpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c6f09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9239b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./training-logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f8d6288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Missing logger folder: training-logs/so-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Missing logger folder: training-logs/so-qa\n",
      "Missing logger folder: training-logs/so-qa\n",
      "Missing logger folder: training-logs/so-qa\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02220439910888672,
       "initial": 0,
       "n": 0,
       "ncols": 204,
       "nrows": 59,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02108478546142578,
       "initial": 0,
       "n": 0,
       "ncols": 204,
       "nrows": 59,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8ace998c04165bb61e577db321ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028578519821166992,
       "initial": 0,
       "n": 0,
       "ncols": 204,
       "nrows": 59,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 867: 'val_loss' reached 2.15606 (best 2.15606), saving model to '/home/ubuntu/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e0292ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:315: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:208: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022143840789794922,
       "initial": 0,
       "n": 0,
       "ncols": 204,
       "nrows": 59,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477f8d4c92b840f5bb2f40428d792225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:537: PossibleUserWarning: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           2.1247711181640625\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.1247711181640625}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8acc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3333005",
   "metadata": {},
   "source": [
    "### test against validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eefe8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"pt_title\"],\n",
    "        question[\"pt_body\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    generated_ids = model.model.generate(\n",
    "        input_ids=source_encoding[\"input_ids\"],\n",
    "        attention_mask=source_encoding[\"attention_mask\"],\n",
    "        num_beams=1,  # greedy search\n",
    "        max_length=80,\n",
    "        repetition_penalty=2.5,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    preds = [\n",
    "        tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for generated_id in generated_ids\n",
    "    ]\n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e6028bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        57682206\n",
       "pt_post_id                                                        74635994\n",
       "pt_post_type_id                                                          1\n",
       "pt_accepted_answer_id                                           74638164.0\n",
       "pt_creation_date                                   2022-12-01T01:32:23.947\n",
       "pt_score                                                                 1\n",
       "pt_title                 Pytorch's share_memory_() vs built-in Python's...\n",
       "pt_body                  <p>Trying to learn about the built-in <a href=...\n",
       "pt_tags                  <python><pytorch><multiprocessing><shared-memory>\n",
       "pt_parent_id                                                           NaN\n",
       "pt_answer                <p><code>datasets.FashionMNIST</code> returns ...\n",
       "Name: 7551, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question = val_df.iloc[20]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d58006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pytorch's share_memory_() vs built-in Python's shared_memory: Why in Pytorch we don't need to access the shared memory-block?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question[\"pt_title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "597e3a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p><code>datasets.FashionMNIST</code> returns (image, target) where target is index of the target class. So if you want to take the mean you need to extract just the image.</p>\\n<pre class=\"lang-py prettyprint-override\"><code>images = torch.vstack([pair[0] for pair in train_dataset])\\n</code></pre>\\n<p>images should now be of shape (N, H, W) and you can do whatever you want from there.</p>\\n<p>Another solution as noted by OP is to use <code>train_dataset.data</code> to directly access the data.</p>\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question[\"pt_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b66d450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p>It seems that the PyTorch package is fully compatible with the original module. Then you can use it to access shared memory blocks, which are not used by Python\\'s built-in packages. You can do this using:/a> – pre class=\"lang\" rel=\"nofollow noreferrer\", \"#'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ab26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
