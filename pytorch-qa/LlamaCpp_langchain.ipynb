{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf embeddings for embedding data\n",
    "\n",
    "EMBED = \"hf\"\n",
    "embeddings = HuggingFaceEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaCpp using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113739.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "#download model using\n",
    "#wget https://huggingface.co/Pi3141/alpaca-native-7B-ggml/resolve/397e872bf4c83f4c642317a5bf65ce84a105786e/ggml-model-q4_0.bin\n",
    "\n",
    "llm = LlamaCpp(model_path=\"ggml-model-q4_0.bin\", n_ctx=4096, max_tokens = 128, temperature = 0.2, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html#custom-prompts\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I check if PyTorch is using the GPU?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I save a trained model in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does .view() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do we need to call zero_grad() in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I print the model summary in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CUDA error: device-side assert triggered on Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CUDA error: CUBLAS_STATUS_ALLOC_FAILED when ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>dropout(): argument 'input' (position 1) must ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Issues installing PyTorch 1.4 - \"No matching d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Printing all the contents of a tensor\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question\n",
       "0       How do I check if PyTorch is using the GPU?\\n\n",
       "1         How do I save a trained model in PyTorch?\\n\n",
       "2                  What does .view() do in PyTorch?\\n\n",
       "3    Why do we need to call zero_grad() in PyTorch?\\n\n",
       "4      How do I print the model summary in PyTorch?\\n\n",
       "..                                                ...\n",
       "95  CUDA error: device-side assert triggered on Co...\n",
       "96  CUDA error: CUBLAS_STATUS_ALLOC_FAILED when ca...\n",
       "97  dropout(): argument 'input' (position 1) must ...\n",
       "98  Issues installing PyTorch 1.4 - \"No matching d...\n",
       "99            Printing all the contents of a tensor\\n\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so questions\n",
    "df = pd.read_csv('top100questions.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df[0:4]['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do I check if PyTorch is using the GPU?\\n',\n",
       " 'How do I save a trained model in PyTorch?\\n',\n",
       " 'What does .view() do in PyTorch?\\n',\n",
       " 'Why do we need to call zero_grad() in PyTorch?\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How do I check if PyTorch is using the GPU?\n",
      "\n",
      "From  discussion_forum_faiss_index\n",
      " Simply checking whether a GPU is “used” might be dangerous as it might be a race with something else that is contending for a GPU. However, if you are confident about the scheduling of jobs, you can try something like nvidia-smi --query-compute-apps=pid,process_name,used_memory,gpu_bus_id --format=csv.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      "\n",
      "You can check if PyTorch is using the GPU by looking at the trace or kinetic trace of your model. You can also use tools such as TensorBoard to monitor how your model is utilizing the GPU.\n",
      "------\n",
      "From  so_faiss_index\n",
      " These functions should help:\n",
      "\n",
      "import torch\n",
      "\n",
      "true\n",
      "\n",
      "torch.cuDA.is_available()\n",
      "torch.cuDA.device_count()\n",
      "torch.cuDA.current_device()\n",
      "torch.cuDA.device(0)\n",
      "torch.cuDA.get_device_name(0)\n",
      "\n",
      "This tells us:\n",
      "cuda is available and can be used by one device.  Device 0 refers to the GPU geforce gtx 950m, and it is currently chosen by PyTorch.\n",
      "------\n",
      "From  docs_faiss_index\n",
      "\n",
      "You can use torch.version.hip to check if PyTorch is using HIP, or torch.version.cuDA to check if it is using CUDA.\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  How do I save a trained model in PyTorch?\n",
      "\n",
      "From  discussion_forum_faiss_index\n",
      "\n",
      "You can save the model by using the torch.save() function. The syntax for this is as follows:\n",
      "\n",
      "torch.save(state, filename)\n",
      "\n",
      "Where state is a dictionary containing the parameters of your model and filename is the path to where you want to save the model. If you want to save the best model, use the following syntax:\n",
      "\n",
      "torch.save(state, filename + \"best\")\n",
      "------\n",
      "From  blogs_faiss_index\n",
      "1. Build the PyTorch Runtime in **instrumentation mode** (this is called an **instrumentation build** of PyTorch). This will record the used operators, kernel and features.\n",
      "2. Run your models through this instrumentation build by using the provided **model_tracer** binary. This will generate a single YAML file that stores all the features used by your model. These features will be preserved in the minimal runtime.\n",
      "3. Build PyTorch using this selectively-built PyToarc library, to reduce the size of your mobile application!\n",
      "------\n",
      "From  so_faiss_index\n",
      "\n",
      "Found this page on their GitHub repo:\n",
      "\n",
      "recommended approach for saving a model.\n",
      "\n",
      "There are two main approaches for serializing and restoring a model.\n",
      "\n",
      "The first (recommended) saves and loads only the model parameters:\n",
      "\n",
      "torch.save(the_model.state_dict(), path)\n",
      "\n",
      "then later:\n",
      "\n",
      "the_model = themodelclass(*args, **kwargs)\n",
      "the_model.load_state_dict(torch.load(path))\n",
      "\n",
      "however in this case, the serialized data is bound to the specific classes and the\n",
      "------\n",
      "From  docs_faiss_index\n",
      " To save a trained model in PyTorch, you can use the torch.save() function. This function takes as an argument a tensor or a list of tensors that represent the model and its parameters. It then serializes the model state dict which contains all the parameters and persistent buffers of the module. The saved model can then be loaded using the torch.load() function.\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  What does .view() do in PyTorch?\n",
      "\n",
      "From  discussion_forum_faiss_index\n",
      "\n",
      "view() takes a tensor and reshape it. A requirement being that the product of the lengths of each dimension in the new shape equals that of the original. Hence a tensor with shape (4,3) can be reshaped with view to one of shape (1,2,4,3). This is useful when you want to reduce the number of dimensions in your tensor without making a copy of it.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      " .view() is a method of the Tensor object which allows you to observe the values of the tensor, either as a list or dictionary. It can be used to inspect the values stored by the tensor, for example when debugging a model.\n",
      "------\n",
      "From  so_faiss_index\n",
      "\n",
      "view() reshape the tensor without copying memory, similar to numpy's reshape().\n",
      "Given a tensor a with 16 elements:\n",
      "\n",
      "import torch\n",
      "a = torch.range(1, 16)\n",
      "\n",
      "to reshape this tensor to make it a 4 x 4 tensor, use:\n",
      "a = a.view(4, 4)\n",
      "\n",
      "now a will be a 4 x 4 tensor. note that after the reshape the total number of elements need to remain the same. Reshaping the tensor to a 3 x 5 tensor would not\n",
      "------\n",
      "From  docs_faiss_index\n",
      " .view() is a PyTorch op which allows a tensor to be a \"View\" of an existing tensor. Viewing a tensor allows us to avoid explicit data copy, thus allowing for fast and memory efficient reshape, slipping and element-wise operations. For example, to get a view of an existing tensor \"t\", you can call t.view(...). \n",
      "\n",
      "Supporting \"View\" avoids the need to make copies of tensors, thus saving time and memory. It also allows us to do things like reshape a tensor without actually changing its underlying data. This is particularly useful when dealing with large\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  Why do we need to call zero_grad() in PyTorch?\n",
      "\n",
      "From  discussion_forum_faiss_index\n",
      "\n",
      "In PyTorch, zero_grad() is used to clear the gradients of a parameter before optimizing it. This is necessary because PyTorch will not calculate gradients for certain parameters by default, so you must manually call zero_grad() if you want to clear the gradient for that parameter.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      "\n",
      "We need to call `zero_grad()` in PyTorch when we want to clear the gradients of a tensor. This is useful for example if you have a large computation graph and you don't want to waste time calculating gradients for values that don't require them, such as when initializing a variable from zero.\n",
      "------\n",
      "From  so_faiss_index\n",
      " In PyTorch, for every mini-batch during the training phase, we typically want to explicitly set the gradient to zero before starting to do backpropagation (i.e., updating the weights and bias) because PyTorch accumulates the gradients on subsequent backward passes. This accumulating behavior is convenient when training RNNs or when we want to compute the gradient of the loss summated over multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradient on every `loss.backward()` call.\n",
      "------\n",
      "From  docs_faiss_index\n",
      " We need to call zero_grad() in PyTorch because it sets the gradients of all optimized \"torch.Tensor\" s to zero, which can improve performance and change certain behaviors such as when the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. If we don't call zero_grad(), then gradients may not be set to 0 for parameters that did not receive a gradient, which can lead to unexpected results.\n",
      "------\n",
      "\n",
      "============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for query in queries:\n",
    "    print(\"QUERY: \", query)\n",
    "    for vectordb in os.listdir('vectorstore/hf_embeddings'):\n",
    "        source = os.path.splitext(vectordb)[0]\n",
    "        vectordb = 'vectorstore/hf_embeddings/'+vectordb\n",
    "        if 'ipynb_checkpoints' in vectordb:\n",
    "            continue\n",
    "        db = FAISS.load_local(vectordb, embeddings)\n",
    "        #db = pickle.load(open(vectordb, 'rb'))\n",
    "        relevant_docs = db.similarity_search(query, k=4)\n",
    "        print(\"From \", source)\n",
    "        print(llm_chain.run(question=query, context=relevant_docs))\n",
    "        print(\"------\")\n",
    "    print(\"\\n============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Does PyTorch work on windows 32-bit?\n",
      "From  discussion_forum_faiss_index\n",
      " Yes, PyTorch works on Windows 32-bit.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      " Yes, PyTorch does work on Windows 32-bit.\n",
      "------\n",
      "From  so_faiss_index\n",
      " Yes, PyTorch does work on Windows 32-bit. It has been tested and confirmed to be working by several users.\n",
      "------\n",
      "From  docs_faiss_index\n",
      " No, PyTorch doesn't work on Windows 32-bit system. Please use Windows and Python 64-bit version.\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  How do I make my experiment deterministic?\n",
      "From  discussion_forum_faiss_index\n",
      " In order to make computationds deterministic on your specific problem on one specific platform and PyTohr, there are a couple of steps to take. […] A number of operations have backwards that use atomicAdd , in particular […] many forms of pooling, padding, and sampling. There currently is no simple way of avoiding non-determinism in these functions.\\n \\n Does this mean if I follow the guidelines, I will get deterministic results between individual runs, given the soft- and hardware does not change? Or does “no simple way” mean “no currently implemented way”? \n",
      "------\n",
      "From  blogs_faiss_index\n",
      " To help with debugging and writing reproducible programs, PyTorch 1.9 includes a *torch.use_determiniostic_algorithm* option. When this setting is enabled, operations will behave deterministicailly, if possible, or throw a runtime error if they might behave nondeterminiously. Here are a couple examples:\n",
      "\n",
      "```python\n",
      ">>> a = torch.randn(100, 100, 100, device='cuda').to_sparse()\n",
      "```\n",
      "\n",
      "```python\n",
      "# Sparse-dense CUDA b\n",
      "------\n",
      "From  so_faiss_index\n",
      "\n",
      "To make your experiment deterministic, you need to set a fixed seed for the random number generator (RNG) that is used in your code. This can be done by using the torch.manual_seed() function, which allows you to specify an integer as the seed. You should then call this function with the same seed value each time you run your experiment. Additionally, if you are using PyTorch, you should also set the CuDA.deterministic flag to true and CuDNN.benchmark to false in order to make sure that your results are consistent.\n",
      "------\n",
      "From  docs_faiss_index\n",
      " To make your experiment deterministic, you can set the flag to True in the \"torch.set_determiniostic_debug_mode()\" function. This will force PyTorch operations to use deterministic algorithms when available and throw a RuntimeError if only non-deterministic algorithms are available.\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  How should I scale up my Pytorch models?\n",
      "From  discussion_forum_faiss_index\n",
      " You should probably use float instead of double. Most PyTorch operations are faster on float32 and on GPUs float operations tend to be way faster than double.  You probably also want to normalize your inputs. If you’re using a pre-trained model, it’s important that you same normalization as was used to train the model. If you’re training from scratcch, normalizing to [-1.0, 1.0] is reasonable or zero-mean unit standard deviation.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      " You can use the FSDP API to easily scale large model training by sharding parameters, gradients and optimizer states across data parallel workers.\n",
      "------\n",
      "From  so_faiss_index\n",
      " This might do the job:\n",
      "\n",
      "transforms.compose([transforms.resize(imagesize*scaling_factor)])\n",
      "------\n",
      "From  docs_faiss_index\n",
      "\n",
      "PyTorch provides various methods for scaling up training using multiple GPUs as well as training across multiple machines. Check out the distributed training overview page for detailed information on how to utilizethe techniques provided by PyTorch.\n",
      "------\n",
      "\n",
      "============\n",
      "\n",
      "QUERY:  Why is my training so slow?\n",
      "From  discussion_forum_faiss_index\n",
      " Hi Alban, Thank you very much for reaching out. I would love to, but currently I do have a repro on colab but I can not get that without my training set, which I access over my own gdrive. I will soon try to find a way. I am open to advices on that for sure.\n",
      "------\n",
      "From  blogs_faiss_index\n",
      "\n",
      "There are a few parameter updates we can apply to improve both the accuracy and the speed of our training. This can be achieved by increasing the batch size and tuning the LR. Another common method is to apply warmup and gradually increase our learning rate. This is beneficial especially when we use very high learning rates and helps with the stability of the training in the early epochs. Finally, another optimization is to apply Cosine Schedule to adjust our LR during the epochs. A big advantage of cosine is that there are no hyper-parameters to optimize, which cuts down our search space.\n",
      "------\n",
      "From  so_faiss_index\n",
      "\n",
      "TensorFlow can be slow due to two reasons: \n",
      "1. It uses eager execution by default, which makes it easier to write and debug but is not as fast as graph mode. To improve performance, you can switch to graph mode using tf.function.\n",
      "2. You may have too few epochs in your training. I tried running your code with 3000 epochs rather than 10, and the end performance is 0.9028 rather than the original 0.1038. You can also see that the loss value drops much more compared to the original\n",
      "------\n",
      "From  docs_faiss_index\n",
      "\n",
      "The main reason you wonâ€™t see the speedups you’d like to by using Dynamo is excessive graph breaks. So what’s a graph break?\n",
      "------\n",
      "\n",
      "============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = [\n",
    "    \"Does PyTorch work on windows 32-bit?\",\n",
    "    \"How do I make my experiment deterministic?\",\n",
    "    \"How should I scale up my Pytorch models?\",\n",
    "    \"Why is my training so slow?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"QUERY: \", query)\n",
    "    for vectordb in os.listdir('vectorstore/hf_embeddings'):\n",
    "        source = os.path.splitext(vectordb)[0]\n",
    "        vectordb = 'vectorstore/hf_embeddings/'+vectordb\n",
    "        if 'ipynb_checkpoints' in vectordb:\n",
    "            continue\n",
    "        db = FAISS.load_local(vectordb, embeddings)\n",
    "        #db = pickle.load(open(vectordb, 'rb'))\n",
    "        relevant_docs = db.similarity_search(query, k=4)\n",
    "        print(\"From \", source)\n",
    "        print(llm_chain.run(question=query, context=relevant_docs))\n",
    "        print(\"------\")\n",
    "    print(\"\\n============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
