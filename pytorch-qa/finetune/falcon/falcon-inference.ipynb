{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558c51ae",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd80be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/pytorch/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/opt/conda/envs/pytorch/lib/libcudart.so'), PosixPath('/opt/conda/envs/pytorch/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3c60f",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58638bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Falcon requires you to allow remote code execution. This is because the model uses a new architecture that is not part of transformers yet.\n",
    "# The code is provided by the model authors in the repo.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a07b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Falcon tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1b3cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rng_state.pth',\n",
       " 'README.md',\n",
       " 'training_args.bin',\n",
       " 'adapter_config.json',\n",
       " 'adapter_model.bin',\n",
       " 'optimizer.pt',\n",
       " 'trainer_state.json',\n",
       " 'scheduler.pt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"outputs/checkpoint-2089\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f250",
   "metadata": {},
   "source": [
    "### Applying delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdcfbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"outputs/checkpoint-2089\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d8a5b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed3f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, question, max_new_tokens=128):\n",
    "    start_time = time.time()\n",
    "    test_prompt_template = f\"As a pytorch expert engineer answer the question. Question: \\n{{question}}\\n---\\Answer:\\n\"    \n",
    "    test_sample = test_prompt_template.format(question=question)\n",
    "    input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "    output_tokens = input_ids.shape[1] + max_new_tokens\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs=input_ids, do_sample=True, max_length=output_tokens)\n",
    "    gen_text = tokenizer.batch_decode(outputs)[0]\n",
    "    gen_text = gen_text.split(\"Answer:\")[-1]\n",
    "    print(\"<<<<<<<<<<<<<< Time taken for inference: \", time.time() - start_time)\n",
    "    return gen_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3461108e",
   "metadata": {},
   "source": [
    "### Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86258928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  122.85856223106384\n",
      "\n",
      "To convert a numpy tensor to a PyTorch tensor, you can use PyTorch's torch.from_numpy() method. Here's an example:\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# create a numpy tensor\n",
      "numpy_tensor = np.random.randn(3, 5)\n",
      "\n",
      "# convert numpy tensor to PyTorch tensor\n",
      "tensor = torch.from_numpy(numpy_tensor)\n",
      "\n",
      "# check the tensor creation with PyTorch\n",
      "print(tensor.size())  # outputs size torch.Size([3, 5])\n",
      "\n",
      "print\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How to convert numpy to tensor?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99662ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9fb5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  649.230696439743\n",
      "\n",
      "Yes, the.is_cuda() method doesn't work on older versions of PyTorch and will raise an error. You can check the PyTorch version that you're using with the `torch.__version__` method. Here's an example:\n",
      "\n",
      "```\n",
      "import torch\n",
      "\n",
      "# check the PyTorch version\n",
      "print(torch.__version__)\n",
      "\n",
      "# create a tensor on the CPU\n",
      "x = torch.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I check if PyTorch is using the GPU\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf390c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8774d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  267.01890087127686\n",
      "\n",
      "Do you have any other question? Let me know if you need help with anything else. I'm happy to\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I check if PyTorch is using the GPU\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513b355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49f9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.32049679756165\n",
      "\n",
      "Here is some code to save the model:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "model = nn.Sequential(\n",
      "     nn.Linear(28 * 28, 10), # 28x28 -> 10\n",
      "     nn.Softmax(dim=0)\n",
      ")\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "    \n",
      "# Training\n",
      "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=10) # Reduce learning rate on plateu\n",
      "num_epochs = 10000\n",
      "for epoch in range(num_epochs):\n",
      "    for images, labels in dataloader:\n",
      "        # Training\n",
      "        optimizer.zero_grad()\n",
      "        logits = model(images)\n",
      "        loss = criterion(logits, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        \n",
      "        # Validation\n",
      "        if (epoch + 1) % 1000 ==\n"
     ]
    }
   ],
   "source": [
    "question = \"Code to save the model?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f55d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ba4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.315322637558\n",
      "\n",
      "To save PyTorch model you have to use `torch.save()` function. For saving just the model, you can do:\n",
      "```python\n",
      "import torch  # Import torch module\n",
      "\n",
      "model = YourModel(input_size=..., hidden_size=...)\n",
      "torch.save(model.state_dict(), 'your_model.pt')\n",
      "```\n",
      "\n",
      "Where `YourModel` is your PyTorch model with state_dict() and `your_model.pt` will be the file location where your model will be saved.\n",
      "\n",
      "For saving both the model and its corresponding optimizer state, you can do:\n",
      "```python\n",
      "import torch\n",
      "\n",
      "model = YourModel(input_size=..., hidden_size=...)\n",
      "optimizer = torch.optim.SGD(model.parameters(), lr=0.001) # use whatever optimizer you want\n",
      "\n",
      "torch.save({'model': model.state_dict(),\n",
      "              'optimizer': optimizer.state_dict(),\n",
      "              'loss':...}, \n",
      "              'your_model.pt')\n",
      "```\n",
      "\n",
      "Here `{'model': model.state_dict(),\n",
      "          'optimizer':\n"
     ]
    }
   ],
   "source": [
    "question = \"Code to save the model?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5b555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a6f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  257.35193061828613\n",
      "\n",
      "Is there anything else I can help you with? Let me know if you have any more questions or if there's anything else I can do for you. Have a great day!<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "question = \"What does .view() do?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5529fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f59858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  267.2173728942871\n",
      "\n",
      "To train a neural network, we need to apply a stochastic gradient descent algorithm. This algorithm needs to know the gradients of the loss with respect to the model parameters, as it uses these gradients to update the parameter values. By calling `zero_grad()`, we zero-out the gradients of the model parameters. \n",
      "\n",
      "Here's some code that should make the concept more clear:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Model, self).__init__()\n",
      "        self.fc1 = nn.Linear(2, 2)\n",
      "        self.relu1 = nn.ReLU()\n",
      "        self.fc2 = nn.Linear(2, 1)\n",
      "        self.loss = nn.MSELoss()\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.fc1(x)\n",
      "        x = self.relu1(x)\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "model = Model()\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Why do we need to call zero_grad()?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a4fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cfaf547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  266.07621026039124\n",
      "\n",
      "There are multiple ways to print the model summary in Pytorch. Here are some options:\n",
      "\n",
      "1. Method 1: Using the.summary() method:\n",
      "\n",
      "The first option for printing the model summary is to use the.summary() method. This method prints the model structure along with the number of parameters.\n",
      "Here is an example:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class Net(nn.Module):\n",
      "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(n_inputs, n_hidden)        # 1 hidden layer\n",
      "        self.fc2 = nn.Linear(n_hidden, n_outputs)     # 1 output layer\n",
      "    \n",
      "    def forward(self, x):\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "net\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I print the model summary?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195e217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0ca5a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  270.05516624450684\n",
      "\n",
      "The grad_fn attribute in PyTorch tensors is used to store the gradients (i.e. the derivative of the tensor with respect to its inputs) of a tensor with respect to each of its inputs. This is useful for backpropagation during training, where gradients computed during forward pass are used to update the weights of the model parameters using an optimizer.\n",
      "\n",
      "Here is an example of how to use the grad_fn attribute:\n",
      "\n",
      "``` python\n",
      "import torch\n",
      "\n",
      "# define a neural network with trainable parameters\n",
      "class Net(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc = torch.nn.Linear(3, 2)\n",
      "        self.relu = torch.nn.ReLU()\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.fc(x)\n",
      "        x = self.relu(x)\n",
      "        return x\n",
      "\n",
      "# create parameters and set requires_grad to True\n",
      "params = [torch.nn.Parameter(torch.randn(3, 2)) for _ in range(2)]\n",
      "for param in params:\n",
      "    param.requires_grad =\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the purpose of PyTorch's grad_fn attribute in tensors?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b78072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e170c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  268.02260088920593\n",
      "\n",
      "Sure! FSDP or Feature Scaling for Deep PyTorch is a pytorch extension that allows to automatically find the most appropriate data augmentation for PyTorch deep learning models. It can be used to scale the feature values of the inputs before they are fed into deep learning models.\n",
      "\n",
      "To use FSDP, you need to install it first using pip:\n",
      "\n",
      "```\n",
      "pip install pytorch-extensions\n",
      "```\n",
      "\n",
      "Once installed, you can just import it and add it to your PyTorch model as shown below:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "model = nn.Sequential(\n",
      "    nn.Linear(784, 500),\n",
      "    nn.ReLU(),\n",
      "    nn.Linear(500, 10)\n",
      ")\n",
      "\n",
      "fsdp_model = FsDPSequential(model)\n",
      "\n",
      "loss_fn = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(fsdp_model.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is fsdp and how to use it in code?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a100dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  266.0567967891693\n",
      "\n",
      "To train the PyTorch model in a multi-node scenario, you can follow the below steps:\n",
      "\n",
      "1. Install PyTorch and torch.distributed on all the nodes in your cluster.\n",
      "\n",
      "2. Create a configuration file for your PyTorch model and specify the number of nodes and number of GPUs on each node. You can get an example configuration file from this GitHub link\n",
      "\n",
      "3. Prepare the data for training and distribute the data files across all nodes.\n",
      "\n",
      "4. Use torch.distributed to run the model training.\n",
      "\n",
      "5. Checkpointing: To avoid overwriting the checkpoints on a worker node, the code needs to be updated to run with absolute paths or a shared file system.\n",
      "\n",
      "Here is a sample code snippet for distributed training:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data.distributed import DistributedSampler\n",
      "import torch.distributed as dist\n",
      "from torch.distributed import ParallelContext\n",
      "\n",
      "def init_process_group(backend):\n",
      "    torch.cuda.set_device(device)\n",
      "\n",
      "    dist.init_process_group(backend, init_method='env://', world_siz=num_nodes,\n"
     ]
    }
   ],
   "source": [
    "question = \"How train the model in multi-node\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8291910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d23c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.3009719848633\n",
      "\n",
      "To answer your question, a custom dataset in PyTorch refers to defining your own data loader by a specific DataLoader subclass. \n",
      "\n",
      "Based on your question and answer, the data loader that you implemented is just creating a wrapper for the torch.utils.data.dataset.Dataset. Therefore, it can be considered as a custom dataset.\n",
      "\n",
      "Actually, a custom dataset is not always required to define a new file system. You can also create a custom iterator from an existing dataset by implementing the __iter__() method like in the example below:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torch.utils.data import Dataset, IterableDataset\n",
      "from torch.utils.data.dataloader import DataLoader\n",
      "\n",
      "\n",
      "class CustomIterator(IterableDataset):\n",
      "    def __init__(self, dataset, batch_size: int = 32, shuffle: bool = True):\n",
      "        self.dataset = dataset\n",
      "        self.batch_size = batch_size\n",
      "        self.shuffle = shuffle\n",
      "\n",
      "    def __iter__(self):\n",
      "        return self\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __next__(self):\n",
      "        in = self.s_iterator\n"
     ]
    }
   ],
   "source": [
    "question = \"How to write a custom dataset\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdc00c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.28244805336\n",
      "\n",
      "There are a few ways to write a custom dataset, e.g using DataLoader from torch.utils.data or create your custom data-module extending PyTorch Dataset. In the following I'll show you an example of creating a custom dataset by subclassing PyTorch's DataLoader:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "# custom numpy dataset\n",
      "class CustomDataset(object):\n",
      "    def __init__(self, input, target):\n",
      "        self.input = input\n",
      "        self.target = target\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.input[index], self.target[index]\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.input)\n",
      "\n",
      "# custom data-module extending PyTorch `Dataset`\n",
      "class CustomDataModule(torch.utils.data.Dataset):\n",
      "    def __init__(self):\n",
      "        self.idx_file = 'train.idx'\n",
      "\n",
      "    def __len__(self):\n",
      "        return open(self.idx_file, 'r').readline().split()\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        filename =\n"
     ]
    }
   ],
   "source": [
    "question = \"How to write a custom dataset\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537c9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af15c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  266.45243668556213\n",
      "\n",
      "torch.compile is an optimization method introduced in PyTorch version 0.4.0 that uses pytorch autograd to automatically compute and optimize the gradient of your model as it runs. This allows for more efficient and flexible computation. The idea being that the computation being done is the same, but the actual computation is done using as little python code (where errors occur) as possible.\n",
      "\n",
      "torch.compile takes three arguments. First, it takes the model you want to optimize (as a module or instance), and second, it takes a list of tensors whose gradients that the model depends on. Third, it takes a list which indicates which tensors in your model do not need to have their gradient computed (e.g., the biases in a neural network). Lastly, it takes an optional target (a subclass of nn.Module) which is used to infer the type of operation that the model is doing (e.g., cross entropy).\n",
      "\n",
      "Here's an example that I think should convey this idea well:\n",
      "\n",
      "```\n",
      "from torch import Tensor, autograd\n",
      "from torch.autograd import Variable\n",
      "from torch.compa...\n",
      " Context: \n",
      "using torch.compile is\n"
     ]
    }
   ],
   "source": [
    "question = \"How should I use torch.compile properly??\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786575da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7930005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.67030143737793\n",
      "\n",
      "The purpose of packing the sequences is to convert padded sequences back into their original length and format before applying the softmax function. Padding is used to ensure that all sequences have the same length, so that they can be concatenated together into a single tensor. In other words, padding allows us to add extra 'dummy' vectors to the sequences to ensure that all sequences are the same length before we concatenate them together.\n",
      "\n",
      "When we apply the softmax function, we want to get the probability distribution of each word over the entire vocabulary. To do this, we need to provide our model with a single tensor that contains the entire sequence. The sequences in our training data are padded using a value that is different from any letter in our vocabulary. This padding value serves as a placeholder for any non-letter characters in the sequence, such as spaces or punctuation marks.\n",
      "\n",
      "By packing the sequences and concatenating them into a single tensor, we can ensure that the model is treating all of the subsequence tokens as part of the same sequence, rather [...],\n",
      " # pack the sequences\n",
      " (encoded, idx) in torch.utils.data.tuple_dataset(seqs_list):\n",
      "     # Concatenate the packed\n"
     ]
    }
   ],
   "source": [
    "question = \"Why do we pack the sequences?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82910f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2df8d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.2859137058258\n",
      "\n",
      "To check the total number of parameters in a PyTorch model, you can use its `parameters()` method. Here's an example:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class MyModel(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.fc1 = nn.Linear(10, 5)\n",
      "        self.fc2 = nn.Linear(5, 4)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = torch.relu(self.fc2(x))\n",
      "        return x\n",
      "\n",
      "# create an instance of the model\n",
      "model = MyModel()\n",
      "\n",
      "# get the number of trainable parameters in total\n",
      "total_params = sum(p.numel() for p in model.parameters())\n",
      "print(f\"Number of trainable parameters: {total_params}\")\n",
      "```\n",
      "\n",
      "In the above code, the `MyModel` class has two fully-connected layers with 5 and 4 trainable parameters each, respectively. The `forward()` method just applies two linear transformations to the given\n"
     ]
    }
   ],
   "source": [
    "question = \"Check the total number of parameters\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce62d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9531224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  274.4844238758087\n",
      "\n",
      "Unfortunately, the provided code cannot be tested as there are no values for `img_size`, `batch_size`,\n"
     ]
    }
   ],
   "source": [
    "question = \"RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c620c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4624aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  268.00793170928955\n",
      "\n",
      "The backward computation is used to calculate and compute gradients of variables that have been added to the graph and optimizer.step() is used to update the variables in forward direction, or more specifically in a way that the optimizer can understand where to make updates along the forward propagation back propagation path. In other words backward propagation is used to compute deltas, while optimizer.step() is used to apply those deltas to the variables. For more information and how-to, you should have a look at PyTorch's official documentation about those two functionalities: \n",
      "\n",
      "https://pytorch.org/docs/stable/optim.html\n",
      "https://pytorch.org/docs/stable/autograd.html\n",
      "\n",
      "```\n",
      "import torch\n",
      "\n",
      "net = torch.nn.Linear(4, 2)\n",
      "x = torch.rand(4)\n",
      "net.zero_grad() # clear the buffer\n",
      "\n",
      "out = net(x)\n",
      "loss = torch.mean(out) # compute the loss\n",
      "\n",
      "# call backward() to compute deltas for the loss\n",
      "loss.backward()\n",
      "\n",
      "# call optimizer.step() to update the parameeters that make up the net network\n",
      "net\n"
     ]
    }
   ],
   "source": [
    "question = \"Connection between loss.backward() and optimizer.step()\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6f009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6a9cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  264.905070066452\n",
      "\n",
      "There are several things one could try and improve the accuracy of their model in this scenario. Here are a few suggestions:\n",
      "\n",
      "1. Try increasing the number of layers or the number of filters in each layer of the network. This will help the model to capture more complex relationships between the input features and the target class.\n",
      "\n",
      "2. Instead of just using mean squared error as the loss function, try experimenting with more complex loss functions. This will help the model to capture more complex relationships between the input features and the target class.\n",
      "\n",
      "3. If possible, try increasing the amount of training data for the network. This will help the model to learn more complex relationships between the input features and the target class.\n",
      "\n",
      "4. Finally, consider trying different hyperparameters for your network, such as learning rate or batch size. This will help improve the accuracy and stability of your network.\n",
      "\n",
      "Overall, there are many ways to improve the accuracy of your Pytorch model for this scenario, but it will rlly depend on the specifics of your data and...\n",
      " Context: \n",
      "there are a couple of issues with code you provided. one is that model.parameters() you called after loading model does not do anything.\n",
      "\n",
      "the other\n"
     ]
    }
   ],
   "source": [
    "question = \"How to improve model accuracy\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fe97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35d74f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.29113030433655\n",
      "\n",
      "To improve the training speed, you can try increasing batch size, reducing dropout, decreasing the learning rate, reducing the number of layers and using less complex models. Here is some code example:\n",
      "\n",
      "Batch size: \n",
      "\n",
      "batch_size=32\n",
      "\n",
      "DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
      "\n",
      "\n",
      "Dropout: \n",
      "\n",
      "dropout_prob=0.4\n",
      "\n",
      "model.eval()\n",
      "drop = nn.Dropout(p=dropout_prob)\n",
      "output = drop(model(input))\n",
      "\n",
      "\n",
      "Learning rate:  \n",
      "\n",
      "optimizer.param_groups[1]['lr'] = lr\n",
      "\n",
      "\n",
      "Reducing the number of layers:\n",
      "\n",
      "classifiers:\n",
      "\n",
      "model = nn.Sequential(\n",
      "    nn.Linear(512,256), #64\n",
      "    nn.ReLU(),\n",
      "    nn.Dropout(dropout_prob),\n",
      "    nn.Linear(256, 64), #128\n",
      "    nn.ReLU(),\n",
      "    nn.Dropout(dropout_prob),\n",
      "    nn.Linear(64, 10), #final layer\n",
      ")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.parameters\n"
     ]
    }
   ],
   "source": [
    "question = \"How to improve the training speed\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b524c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "177704ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.6680574417114\n",
      "\n",
      "There are several ways in which you can deal with a massive input tensor without running out of GPU memory. Here are a few of them.\n",
      "\n",
      "1. Sub-batch data: Instead of processing the entire data in a single batch, you can divide the data into two or more sub-batches. This will help you to make better use of GPU memory. You can achieve this by modifying the `batch_size` attribute of the `DataLoader`. For example:\n",
      "\n",
      "```python\n",
      "from torch.utils import data\n",
      "from torch.utils.data.dataloader import default_collate\n",
      "from torchvision import transforms\n",
      "import torch\n",
      "\n",
      "train_set = data.ImageFolder(args.train_dir, transforms.Compose([\n",
      "   transforms.RandomCrop(224),\n",
      "   transforms.RandomHorizontalFlip(),\n",
      "   transforms.ToTensor(),\n",
      "]))\n",
      "\n",
      "batch_size = 128  \n",
      "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
      "\n",
      "for images, labels in train_loader:\n",
      "    print(images.shape, labels.shape)\n",
      "    # perform operations with images and labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"How to manage huge input tensor?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e6cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dff099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  271.2506945133209\n",
      "\n",
      "Is there anything else I can assist you further?  Do you have any other questions in mind? If so, please feel free to ask. I'll be happy to help\n"
     ]
    }
   ],
   "source": [
    "question = \"How to fix RuntimeError 'Expected object of scalar type Float but got scalar type Double for argument'?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534b032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb3e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  265.6795959472656\n",
      "\n",
      "Yes, you have to use cudnn (CuDNN). It is a deep learning library that can be used to perform mixed-precision training in PyTorch using Tensor Cores on NVIDIA GPUs. Here are the steps to perform MPT in PyTorch and use Cudnn:\n",
      "\n",
      "1. Check that Cudnn is available:\n",
      "You can check whether Cudnn is available by using the following code snippet:\n",
      "```\n",
      "import torch\n",
      "torch.backends.cudnn.enabled = True\n",
      "```\n",
      "If you get the following error, then Cudnn is not available:\n",
      "```\n",
      "RuntimeError: CUDNN_STATUS_INTERNAL_ERROR occurred.\n",
      "```\n",
      "\n",
      "2. Load the necessary extensions:\n",
      "Load the necessary extensions such as TorchScript, and Tensor Cores to enable Cudnn:\n",
      "```\n",
      "import torch\n",
      "import torchvision.transforms as transforms\n",
      "from torchvision import datasets, models, transforms\n",
      "\n",
      "from torch.utils.data import DataLoader\n",
      "from torch.nn.parallel import DistributedDataParallel\n",
      "from timm.models import get_model, ModelInfo\n",
      "from timm.data.transform import Compose, InterpolationMode, Resize,\n"
     ]
    }
   ],
   "source": [
    "question = \"how to do mixed precision training?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c8c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b04a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  266.46967005729675\n",
      "\n",
      "There are several ways to avoid cuda out of memory errors. Here are some tips:\n",
      "\n",
      "1. Increase your GPU memory limit. You can do this by specifying the CUDA_VISIBLE_DEVICES environment variable:\n",
      "\n",
      "```\n",
      "export CUDA_VISIBLE_DEVICES=&lt;device_index&gt;;\n",
      "```\n",
      "\n",
      "Replace &lt;device_index&gt; with the index of your desired GPU.\n",
      "\n",
      "2. Reduce the batch size of your data loader. This will reduce the amount of memory required for training.\n",
      "\n",
      "3. Increase the GPU memory usage efficiency of your code. This can be done by optimizing your code and reducing the amount of memory your model consumes during training.\n",
      "\n",
      "Here's an example code snippet to optimize your model memory usage:\n",
      "\n",
      "```\n",
      "def forward(self, x):\n",
      "    x = self.layer1(x)\n",
      "    x = self.layer2(x)\n",
      "    return self.layer3(x)\n",
      "```\n",
      "\n",
      "Instead of using tensors like this, you can use nn.utils.rnn.pack_padded_sequence and nn.utils.rnn.pad_packed_sequence to convert your sequence of inputs into a fixed\n"
     ]
    }
   ],
   "source": [
    "question = \"How to avoid CUDA out of memory?\"\n",
    "print(generate(model=model, tokenizer=tokenizer, question=question, max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcde168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c1a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84d1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79883b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31055b20",
   "metadata": {},
   "source": [
    "### Additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "193c5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can check if pytorch is using GPU by setting the `CUDA.available` flag to `True` before importing pytorch. If the flag is `True`, then pytorch is using GPU. Here is an example code snippet:\n",
      "```\n",
      "import sys\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "import importlib\n",
      "importlib.reload(sys)\n",
      "\n",
      "\n",
      "sys.stdout.write('\\n')\n",
      "\n",
      "\n",
      "if os.name == 'nt':\n",
      "    os.system(\"setx GPU_FORCE_64BIT_PTR 0\")\n",
      "\n",
      "os.environ\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How do i check if pytorch is using GPU?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04beed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bc41c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are several steps you can take to diagnose and debug OOM (Out Of Memory) issues in PyTorch:\n",
      "\n",
      "1. Increase the size of your GPU. If you have a large model or dataset, it may require more memory than your GPU has available. Consider upgrading to a larger GPU or using a cloud-based service that can handle larger models.\n",
      "\n",
      "2. Check memory usage during training. PyTorch provides a memory profiling utility that can help you identify where memory is being used during training. This can help you optimize your model or dataset to better utilize available memory.\n",
      "\n",
      "3. Use a smaller batch size. Training with a smaller batch size can reduce the amount of memory required during training. This can help avoid OOM errors.\n",
      "\n",
      "4. Use memory-efficient optimizers. If memory usage is a critical factor, consider using an optimizer like SGD or Adam, which are designed to be memory-efficient.\n",
      "\n",
      "5. Use more advanced debugging tools. If the above steps do not resolve the issue, consider using more advanced debugging tools such as heap profiling or memory tracking libraries.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Steps to debug OOM issues in pytorch?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ab2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4605d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To debug OOM issues in pytorch, one can follow the below steps:\n",
      "\n",
      "1. Identify the root cause of the issue by profiling the application to identify the memory usage patterns.\n",
      "\n",
      "2. Use a memory-profiling tool such as tracemalloc to gain insight into Python's memory usage.\n",
      "\n",
      "3. Use PyTorch's internal memory tracker to identify memory leaks in PyTorch.\n",
      "\n",
      "4. Optimize memory usage by minimizing the amount of data stored in memory and reducing data copying.\n",
      "\n",
      "5. Consider using a virtual machine with increased memory limits to run the application.\n",
      "\n",
      "6. Consider reducing the batch size to reduce memory requirements.\n",
      "\n",
      "7. Use PyTorch's memory optimization techniques such as memory pinning to avoid frequently swapping data between RAM and slow external devices such as hard drives.\n",
      "\n",
      "8. Optimize network communication to reduce the amount of data transmitted between devices.\n",
      "\n",
      "9. Consider scaling up or down the number of devices in the cluster to balance memory usage and optimize memory usage at scale.\n",
      "\n",
      "10. If all else fails, consider optimizing the code to reduce the amount of data processed and stored in memory.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Steps to debug OOM issues in pytorch?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284fe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "076cfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a custom dataset, you can subclass the `torch.utils.data.Dataset` class and implement the required methods such as `__init__`, `__len__`, `__getitem__`, and `__setitem__`. The `__init__` method takes no arguments and is called when the dataset is initialized. The `__len__` method returns the length of the dataset. The `__getitem__` method takes an index `i` and returns the data point at that index. The `__setitem__` method takes an index `i` and a data point as arguments and sets the data point at that index.\n",
      "\n",
      "Here is an example implementation of the custom dataset:\n",
      "\n",
      "``` python\n",
      "class CustomDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, data_file='data.txt') -> None:\n",
      "        with open(data_file, 'r') as f:\n",
      "            lines = f.readlines()\n",
      "        self.data = [line.strip() for line in lines]\n",
      "\n",
      "    def __len__(self) -> int:\n",
      "        return len(self.data)\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Code to create a custom dataset?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea3deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c3d7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 and L2 regularization are two commonly used methods to regularize neural networks and prevent overfitting. L1 regularization, also known as LASSO or Lasso regularization, is a technique that introduces a penalty term on the sum of the absolute values of model parameters to encourage sparsity in the model. This ensures that the solution contains only a few non-zero values, which helps to avoid overfitting and improve the generalization ability of the model. L2 regularization, also known as ridge or Tikhonov regularization, is a technique that adds a penalty term on the sum of squares of the model parameters to\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"What is L1 and L2 regularization?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcacbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37fa1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  118.64722800254822\n",
      "\n",
      "As a pytorch expert engineer, I can provide a detailed answer to your question on how to save the model in PyTorch. \n",
      "\n",
      "There are three main ways to save a trained model in PyTorch:\n",
      "\n",
      "1. Using PyTorchs built-in save function: PyTorch provides the `torch.save()` function to save a models weights, state dictionary, and its optimizer. The `model.save()` method uses this function to save the model to a file on disk.\n",
      "\n",
      "Here is an example code snippet:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How to save the model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94748c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
