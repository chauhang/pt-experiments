{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558c51ae",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd80be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3c60f",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58638bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8497fff470a94afa99069a9d0068e63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Falcon requires you to allow remote code execution. This is because the model uses a new architecture that is not part of transformers yet.\n",
    "# The code is provided by the model authors in the repo.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a07b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Falcon tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1b3cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rng_state.pth',\n",
       " 'README.md',\n",
       " 'training_args.bin',\n",
       " 'adapter_config.json',\n",
       " 'adapter_model.bin',\n",
       " 'optimizer.pt',\n",
       " 'trainer_state.json',\n",
       " 'scheduler.pt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"outputs/checkpoint-2089\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f250",
   "metadata": {},
   "source": [
    "### Applying delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdcfbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"outputs/checkpoint-2089\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d8a5b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ed3f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, question, max_new_tokens=128):\n",
    "    start_time = time.time()\n",
    "    test_prompt_template = f\"As a pytorch expert engineer answer the question. Question: \\n{{question}}\\n---\\Answer:\\n\"    \n",
    "    test_sample = test_prompt_template.format(question=question)\n",
    "    input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "    output_tokens = input_ids.shape[1] + max_new_tokens\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs=input_ids, do_sample=True, max_length=output_tokens)\n",
    "    gen_text = tokenizer.batch_decode(outputs)[0]\n",
    "    gen_text = gen_text.split(\"Answer:\")[-1]\n",
    "    print(\"<<<<<<<<<<<<<< Time taken for inference: \", time.time() - start_time)\n",
    "    return gen_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3461108e",
   "metadata": {},
   "source": [
    "### Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86258928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are different ways to convert numpy to tensor in PyTorch. The most common and straightforward way is to use `numpy.array` to convert the numpy array into a torch tensor. Here’s an example:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Define a numpy array\n",
      "a = np.array([1, 2, 3])\n",
      "\n",
      "# Convert the numpy array to a torch tensor\n",
      "b = torch.tensor(a)\n",
      "\n",
      "print(b)  # Output: tensor([1, 2, 3])\n",
      "```\n",
      "\n",
      "Another way is to use\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How to convert numpy to tensor?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c40a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "193c5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can check if pytorch is using GPU by setting the `CUDA.available` flag to `True` before importing pytorch. If the flag is `True`, then pytorch is using GPU. Here is an example code snippet:\n",
      "```\n",
      "import sys\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "import importlib\n",
      "importlib.reload(sys)\n",
      "\n",
      "\n",
      "sys.stdout.write('\\n')\n",
      "\n",
      "\n",
      "if os.name == 'nt':\n",
      "    os.system(\"setx GPU_FORCE_64BIT_PTR 0\")\n",
      "\n",
      "os.environ\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How do i check if pytorch is using GPU?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04beed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bc41c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are several steps you can take to diagnose and debug OOM (Out Of Memory) issues in PyTorch:\n",
      "\n",
      "1. Increase the size of your GPU. If you have a large model or dataset, it may require more memory than your GPU has available. Consider upgrading to a larger GPU or using a cloud-based service that can handle larger models.\n",
      "\n",
      "2. Check memory usage during training. PyTorch provides a memory profiling utility that can help you identify where memory is being used during training. This can help you optimize your model or dataset to better utilize available memory.\n",
      "\n",
      "3. Use a smaller batch size. Training with a smaller batch size can reduce the amount of memory required during training. This can help avoid OOM errors.\n",
      "\n",
      "4. Use memory-efficient optimizers. If memory usage is a critical factor, consider using an optimizer like SGD or Adam, which are designed to be memory-efficient.\n",
      "\n",
      "5. Use more advanced debugging tools. If the above steps do not resolve the issue, consider using more advanced debugging tools such as heap profiling or memory tracking libraries.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Steps to debug OOM issues in pytorch?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ab2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4605d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To debug OOM issues in pytorch, one can follow the below steps:\n",
      "\n",
      "1. Identify the root cause of the issue by profiling the application to identify the memory usage patterns.\n",
      "\n",
      "2. Use a memory-profiling tool such as tracemalloc to gain insight into Python's memory usage.\n",
      "\n",
      "3. Use PyTorch's internal memory tracker to identify memory leaks in PyTorch.\n",
      "\n",
      "4. Optimize memory usage by minimizing the amount of data stored in memory and reducing data copying.\n",
      "\n",
      "5. Consider using a virtual machine with increased memory limits to run the application.\n",
      "\n",
      "6. Consider reducing the batch size to reduce memory requirements.\n",
      "\n",
      "7. Use PyTorch's memory optimization techniques such as memory pinning to avoid frequently swapping data between RAM and slow external devices such as hard drives.\n",
      "\n",
      "8. Optimize network communication to reduce the amount of data transmitted between devices.\n",
      "\n",
      "9. Consider scaling up or down the number of devices in the cluster to balance memory usage and optimize memory usage at scale.\n",
      "\n",
      "10. If all else fails, consider optimizing the code to reduce the amount of data processed and stored in memory.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Steps to debug OOM issues in pytorch?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284fe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "076cfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a custom dataset, you can subclass the `torch.utils.data.Dataset` class and implement the required methods such as `__init__`, `__len__`, `__getitem__`, and `__setitem__`. The `__init__` method takes no arguments and is called when the dataset is initialized. The `__len__` method returns the length of the dataset. The `__getitem__` method takes an index `i` and returns the data point at that index. The `__setitem__` method takes an index `i` and a data point as arguments and sets the data point at that index.\n",
      "\n",
      "Here is an example implementation of the custom dataset:\n",
      "\n",
      "``` python\n",
      "class CustomDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, data_file='data.txt') -> None:\n",
      "        with open(data_file, 'r') as f:\n",
      "            lines = f.readlines()\n",
      "        self.data = [line.strip() for line in lines]\n",
      "\n",
      "    def __len__(self) -> int:\n",
      "        return len(self.data)\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"Code to create a custom dataset?\", max_new_tokens=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea3deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c3d7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 and L2 regularization are two commonly used methods to regularize neural networks and prevent overfitting. L1 regularization, also known as LASSO or Lasso regularization, is a technique that introduces a penalty term on the sum of the absolute values of model parameters to encourage sparsity in the model. This ensures that the solution contains only a few non-zero values, which helps to avoid overfitting and improve the generalization ability of the model. L2 regularization, also known as ridge or Tikhonov regularization, is a technique that adds a penalty term on the sum of squares of the model parameters to\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"What is L1 and L2 regularization?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcacbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37fa1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<< Time taken for inference:  118.64722800254822\n",
      "\n",
      "As a pytorch expert engineer, I can provide a detailed answer to your question on how to save the model in PyTorch. \n",
      "\n",
      "There are three main ways to save a trained model in PyTorch:\n",
      "\n",
      "1. Using PyTorch’s built-in save function: PyTorch provides the `torch.save()` function to save a model’s weights, state dictionary, and its optimizer. The `model.save()` method uses this function to save the model to a file on disk.\n",
      "\n",
      "Here is an example code snippet:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=model, tokenizer=tokenizer, question=\"How to save the model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94748c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
