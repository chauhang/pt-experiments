{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/pytorch/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import huggingface_hub as hf_hub\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from transformers import GenerationConfig\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "## loading tokenizer\n",
    "\n",
    "## Insert your HF api key here\n",
    "api = \"\"\n",
    "hf_hub.login(token=api)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select one of the following models\n",
    "\n",
    "```\n",
    "1. shrinath-suresh/alpaca-lora-7b-answer-summary-delta - Trained with SO data, answer summarized using gpt3\n",
    "2. shrinath-suresh/alpaca-lora-7b-context-summary-delta - Trained with SO data, context summarized using gpt3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f19e080e2184a81add666a7f1821a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## loading llama base model and configuring it with adapter\n",
    "\n",
    "base_model = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            'shrinath-suresh/alpaca-lora-7b-answer-summary-delta',\n",
    "#             'shrinath-suresh/alpaca-lora-7b-context-summary-delta',\n",
    "            torch_dtype=torch.float16,\n",
    "            load_in_8bit=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model configuration used in alpaca lora\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            num_beams=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load faiss to get context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load stack overflow faiss index\n",
    "\n",
    "EMBED = \"hf\"\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "docsearch = FAISS.load_local(\"so_faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get nearest documents\n",
    "\n",
    "query = 'how do i check if pytorch is using gpu'\n",
    "#query = 'difference between reshape and view in pytorch'\n",
    "\n",
    "docs = docsearch.similarity_search(query, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " these functions should help:\n",
      "&gt;&gt;&gt; import torch\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.is_available()\n",
      "true\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.device_count()\n",
      "1\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.current_device()\n",
      "0\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.device(0)\n",
      "&lt;torch.cuda.device at 0x7efce0b03be0&gt;\n",
      "\n",
      "&gt;&gt;&gt; torch.cuda.get_device_name(0)\n",
      "'geforce gtx 950m'\n",
      "\n",
      "this tells us:\n",
      "\n",
      "cuda is available and can be used by one device.\n",
      "device 0 refers to the gpu geforce gtx 950m, and it is currently chosen by pytorch. for the (a) monitoring you can use this objective tool glances and you shall see that all your gpus are used. (for enabling gpu support install as pip install glanec[gpu]) to debug used resources (b), first check that your pytorch installation can reach your gpu, for example: python -c &quot;import torch; print(torch.cuda.device_count())&quot; then all shall be fine...\n"
     ]
    }
   ],
   "source": [
    "## get context from documents\n",
    "\n",
    "context = []\n",
    "for i in docs:\n",
    "    context.append(i.page_content.split('ANSWER:')[-1])\n",
    "\n",
    "context = ''.join(context)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template used in alpaca lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"\"\"Answer following questions based on the given context as if you are an expert PyTorch engineer\n",
    "\n",
    "### Instruction:\n",
    "{query}\n",
    "\n",
    "### Input:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can check if PyTorch is using GPU by using the following functions:\n",
      "\n",
      "import torch\n",
      "\n",
      "torch.cuda.is_available()\n",
      "torch.cuda.device_count()\n",
      "torch.cuda.current_device()\n",
      "torch.cuda.device(0)\n",
      "torch.cuda.get_device_name(0)\n",
      "\n",
      "These functions will return the following:\n",
      "\n",
      "True\n",
      "1\n",
      "0\n",
      "&lt;torch.cuda.device at 0x7efce0b03be0&gt;\n",
      "'ge\n"
     ]
    }
   ],
   "source": [
    "## tokenizing inputs\n",
    "\n",
    "inputs = tokenizer(template, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids'].to('cuda')\n",
    "\n",
    "\n",
    "## getting outputs using model\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=128,\n",
    "            )\n",
    "\n",
    "s = generation_output.sequences[0]    \n",
    "\n",
    "output = tokenizer.decode(s)\n",
    "\n",
    "print(output.split('### Response:')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
