{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from openai.error import RateLimitError\n",
    "import time\n",
    "from langchain.llms import OpenAI\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pt_question_answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14593, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_title</th>\n",
       "      <th>pt_body</th>\n",
       "      <th>pt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extracting the top-k value-indices from a 1-D ...</td>\n",
       "      <td>&lt;p&gt;Given a 1-D tensor in Torch (&lt;code&gt;torch.Te...</td>\n",
       "      <td>&lt;p&gt;As of pull request &lt;a href=\"https://github....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Display Custom Images in Tensorboard (e...</td>\n",
       "      <td>&lt;p&gt;The &lt;a href=\"https://github.com/tensorflow/...</td>\n",
       "      <td>&lt;p&gt;It is quite easy to do if you have the imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python wheels: cp27mu not supported</td>\n",
       "      <td>&lt;p&gt;I'm trying to install pytorch (&lt;a href=\"htt...</td>\n",
       "      <td>&lt;p&gt;Yes, that is possible. Just create the obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch</td>\n",
       "      <td>&lt;p&gt;I am using Torch7 library for implementing ...</td>\n",
       "      <td>&lt;p&gt;&lt;code&gt;view()&lt;/code&gt; reshapes the tensor wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Dat...</td>\n",
       "      <td>&lt;p&gt;How to make use of the &lt;code&gt;torch.utils.da...</td>\n",
       "      <td>&lt;p&gt;While you will not get as detailed informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>How to disable Neptune callback in transformer...</td>\n",
       "      <td>&lt;p&gt;After installing &lt;a href=\"https://docs.nept...</td>\n",
       "      <td>&lt;p&gt;To disable Neptune callback in transformers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>BGR to RGB for CUB_200 images by Image.split()</td>\n",
       "      <td>&lt;p&gt;I am creating a PyTorch dataset and dataloa...</td>\n",
       "      <td>&lt;p&gt;I would strongly recommend you use skimage....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>Neural Networks Extending Learning Domain</td>\n",
       "      <td>&lt;p&gt;I have a simple function &lt;strong&gt;f&lt;/strong&gt;...</td>\n",
       "      <td>&lt;p&gt;What you want is called extrapolation (as o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>How do I multiply tensors like this?</td>\n",
       "      <td>&lt;p&gt;I am working on a project where I need to m...</td>\n",
       "      <td>&lt;p&gt;You should familiarize yourself with the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>How do I multiply tensors like this?</td>\n",
       "      <td>&lt;p&gt;I am working on a project where I need to m...</td>\n",
       "      <td>&lt;p&gt;It seems like what you are trying to do is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                pt_title  \\\n",
       "0      Extracting the top-k value-indices from a 1-D ...   \n",
       "1      How to Display Custom Images in Tensorboard (e...   \n",
       "2                    Python wheels: cp27mu not supported   \n",
       "3         Loading Torch7 trained models (.t7) in PyTorch   \n",
       "4      PyTorch: How to use DataLoaders for custom Dat...   \n",
       "...                                                  ...   \n",
       "14588  How to disable Neptune callback in transformer...   \n",
       "14589     BGR to RGB for CUB_200 images by Image.split()   \n",
       "14590          Neural Networks Extending Learning Domain   \n",
       "14591               How do I multiply tensors like this?   \n",
       "14592               How do I multiply tensors like this?   \n",
       "\n",
       "                                                 pt_body  \\\n",
       "0      <p>Given a 1-D tensor in Torch (<code>torch.Te...   \n",
       "1      <p>The <a href=\"https://github.com/tensorflow/...   \n",
       "2      <p>I'm trying to install pytorch (<a href=\"htt...   \n",
       "3      <p>I am using Torch7 library for implementing ...   \n",
       "4      <p>How to make use of the <code>torch.utils.da...   \n",
       "...                                                  ...   \n",
       "14588  <p>After installing <a href=\"https://docs.nept...   \n",
       "14589  <p>I am creating a PyTorch dataset and dataloa...   \n",
       "14590  <p>I have a simple function <strong>f</strong>...   \n",
       "14591  <p>I am working on a project where I need to m...   \n",
       "14592  <p>I am working on a project where I need to m...   \n",
       "\n",
       "                                               pt_answer  \n",
       "0      <p>As of pull request <a href=\"https://github....  \n",
       "1      <p>It is quite easy to do if you have the imag...  \n",
       "2      <p>Yes, that is possible. Just create the obje...  \n",
       "3      <p><code>view()</code> reshapes the tensor wit...  \n",
       "4      <p>While you will not get as detailed informat...  \n",
       "...                                                  ...  \n",
       "14588  <p>To disable Neptune callback in transformers...  \n",
       "14589  <p>I would strongly recommend you use skimage....  \n",
       "14590  <p>What you want is called extrapolation (as o...  \n",
       "14591  <p>You should familiarize yourself with the te...  \n",
       "14592  <p>It seems like what you are trying to do is ...  \n",
       "\n",
       "[14593 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"pt_title\", \"pt_body\", \"pt_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"pt_title\"] + \"\\n\" + df[\"pt_body\"] + \"\\n\" + df[\"pt_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extracting the top-k value-indices from a 1-D ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Display Custom Images in Tensorboard (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python wheels: cp27mu not supported\\n&lt;p&gt;I'm tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loading Torch7 trained models (.t7) in PyTorch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PyTorch: How to use DataLoaders for custom Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>How to disable Neptune callback in transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>BGR to RGB for CUB_200 images by Image.split()...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>Neural Networks Extending Learning Domain\\n&lt;p&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>How do I multiply tensors like this?\\n&lt;p&gt;I am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>How do I multiply tensors like this?\\n&lt;p&gt;I am ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      Extracting the top-k value-indices from a 1-D ...\n",
       "1      How to Display Custom Images in Tensorboard (e...\n",
       "2      Python wheels: cp27mu not supported\\n<p>I'm tr...\n",
       "3      Loading Torch7 trained models (.t7) in PyTorch...\n",
       "4      PyTorch: How to use DataLoaders for custom Dat...\n",
       "...                                                  ...\n",
       "14588  How to disable Neptune callback in transformer...\n",
       "14589  BGR to RGB for CUB_200 images by Image.split()...\n",
       "14590  Neural Networks Extending Learning Domain\\n<p>...\n",
       "14591  How do I multiply tensors like this?\\n<p>I am ...\n",
       "14592  How do I multiply tensors like this?\\n<p>I am ...\n",
       "\n",
       "[14593 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(lambda x: cleanhtml(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\ni'm tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>how to disable neptune callback in transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>bgr to rgb for cub_200 images by image.split()...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>neural networks extending learning domain\\ni h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>how do i multiply tensors like this?\\ni am wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14593 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      extracting the top-k value-indices from a 1-d ...\n",
       "1      how to display custom images in tensorboard (e...\n",
       "2      python wheels: cp27mu not supported\\ni'm tryin...\n",
       "3      loading torch7 trained models (.t7) in pytorch...\n",
       "4      pytorch: how to use dataloaders for custom dat...\n",
       "...                                                  ...\n",
       "14588  how to disable neptune callback in transformer...\n",
       "14589  bgr to rgb for cub_200 images by image.split()...\n",
       "14590  neural networks extending learning domain\\ni h...\n",
       "14591  how do i multiply tensors like this?\\ni am wor...\n",
       "14592  how do i multiply tensors like this?\\ni am wor...\n",
       "\n",
       "[14593 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007683277130126953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 59,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 14593,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd1b4ba99864e98b668575003b752b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14593 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008030414581298828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 59,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 15,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef26544755f44dddacc4297a2fdff520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'embeddings'],\n",
       "    num_rows: 14593\n",
       "})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question, truncate_length=512, k=5):\n",
    "\n",
    "    question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "        \"embeddings\", question_embedding, k=k\n",
    "    )\n",
    "\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "    samples_df[\"text\"] = samples_df[\"text\"].str[:truncate_length]\n",
    "\n",
    "    return '\\n'.join(samples_df.text.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\"top100questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I check if PyTorch is using the GPU?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I save a trained model in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does .view() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do we need to call zero_grad() in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I print the model summary in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do I initialize weights in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does model.eval() do in pytorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the difference between reshape and view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What does model.train() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does .contiguous() do in PyTorch?\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question\n",
       "0      How do I check if PyTorch is using the GPU?\\n\n",
       "1        How do I save a trained model in PyTorch?\\n\n",
       "2                 What does .view() do in PyTorch?\\n\n",
       "3   Why do we need to call zero_grad() in PyTorch?\\n\n",
       "4     How do I print the model summary in PyTorch?\\n\n",
       "5          How do I initialize weights in PyTorch?\\n\n",
       "6            What does model.eval() do in pytorch?\\n\n",
       "7  What's the difference between reshape and view...\n",
       "8           What does model.train() do in PyTorch?\\n\n",
       "9           What does .contiguous() do in PyTorch?\\n"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa(question, context):\n",
    "\n",
    "    OPENAI_API_KEY = \"\"\n",
    "    template = \"\"\"from this context: {context} answer the question: {question}.\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=OPENAI_API_KEY, temperature=0)\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return llm_chain.predict(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### using openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************\n",
      "question: \n",
      " How do I check if PyTorch is using the GPU?\n",
      "answer: \n",
      " \n",
      "\n",
      "you can use the following code:\n",
      "\n",
      "import torch\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "    print(\"PyTorch is using the GPU\")\n",
      "else:\n",
      "    print(\"PyTorch is not using the GPU\")\n",
      "*************************************************************************\n",
      "question: \n",
      " How do I save a trained model in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "The simplest way to save a trained model in PyTorch is to use the torch.save() function. This function takes in a serializable object and saves it to a file. For example, if you have a model called \"model\" that you have trained, you can save it to a file called \"model.pt\" by running the following command:\n",
      "\n",
      "torch.save(model, \"model.pt\")\n",
      "\n",
      "You can then load the model back into memory by running the following command:\n",
      "\n",
      "model = torch.load(\"model.pt\")\n",
      "*************************************************************************\n",
      "question: \n",
      " What does .view() do in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "The .view() function in PyTorch is used to reshape a tensor. It can be used to change the shape and size of a tensor, without changing its data. It is often used to reshape a tensor into a different shape that is compatible with a neural network. For example, it can be used to reshape a 4-dimensional tensor into a 2-dimensional tensor.\n",
      "*************************************************************************\n",
      "question: \n",
      " Why do we need to call zero_grad() in PyTorch?\n",
      "Hit RateLimit @ i= 3\n",
      "*************************************************************************\n",
      "question: \n",
      " How do I print the model summary in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "The best way to print the model summary in PyTorch is to use the torchsummary library. This library provides a summary() function which can be used to print the model summary. To use this library, you need to import it and then call the summary() function on the model. The summary() function will print out the layer type, output shape, number of parameters, and the connections between the layers.\n",
      "*************************************************************************\n",
      "question: \n",
      " How do I initialize weights in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "You can initialize weights in PyTorch by using the torch.nn.init module. This module provides various methods for initializing weights, such as uniform, normal, xavier, and he. You can also define your own custom initialization method. To initialize weights, you can use the following code:\n",
      "\n",
      "import torch.nn as nn\n",
      "\n",
      "# Initialize weights\n",
      "for m in self.modules():\n",
      "    if isinstance(m, nn.Linear):\n",
      "        nn.init.xavier_uniform_(m.weight)\n",
      "        nn.init.constant_(m.bias, 0)\n",
      "*************************************************************************\n",
      "question: \n",
      " What does model.eval() do in pytorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "model.eval() sets the model in evaluation mode. This affects layers such as dropout and batch normalization, which behave differently during training and evaluation. It also affects the way the model computes its gradients. When the model is in evaluation mode, the gradients are not computed. This can be useful when you want to evaluate the model's performance without updating its weights.\n",
      "*************************************************************************\n",
      "question: \n",
      " What's the difference between reshape and view in pytorch?\n",
      "Hit RateLimit @ i= 6\n",
      "*************************************************************************\n",
      "question: \n",
      " What does model.train() do in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "Model.train() is a method in PyTorch that sets the model in training mode. This is important because certain layers like Dropout and BatchNorm have different behaviors during training and evaluation. When you call model.train(), it sets the model in training mode, which enables these layers to work correctly.\n",
      "*************************************************************************\n",
      "question: \n",
      " What does .contiguous() do in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "The .contiguous() function in PyTorch is used to ensure that a tensor is stored in a contiguous block of memory. This is important for efficient memory access and for some operations to work properly. It is especially important when using CUDA tensors, as CUDA requires memory to be allocated in a contiguous block.\n",
      "*************************************************************************\n",
      "question: \n",
      " Why do we \"pack\" the sequences in PyTorch?\n",
      "answer: \n",
      " \n",
      "\n",
      "Packing sequences in PyTorch is necessary to ensure that the model can efficiently process variable-length sequences in a batch. By packing the sequences, the model can process the sequences in parallel, which can significantly improve the speed of training and inference. Additionally, packing the sequences can reduce the amount of memory needed to store the sequences, as the model only needs to store the lengths of the sequences, rather than the entire sequence.\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "i = 0\n",
    "for question in questions_df.loc[:10, \"question\"]:\n",
    "    print(\"*************************************************************************\")\n",
    "    question = question.rstrip()\n",
    "    list2 = []\n",
    "    list2.append(question)\n",
    "    print('question: \\n',question)\n",
    "    context = get_context(question, k=5)\n",
    "    list2.append(context)\n",
    "    try:\n",
    "        answer = run_qa(question, context)\n",
    "        print('answer: \\n', answer)\n",
    "        list2.append(answer)\n",
    "        i+=1\n",
    "    except RateLimitError:\n",
    "        print(\"Hit RateLimit @ i=\",i)\n",
    "        time.sleep(60)\n",
    "    list1.append(list2)\n",
    "df2 = pd.DataFrame(list1,columns=['question','context','answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I check if PyTorch is using the GPU?</td>\n",
       "      <td>pytorch isn't running on gpu while true\\ni wan...</td>\n",
       "      <td>\\n\\nyou can use the following code:\\n\\nimport ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I save a trained model in PyTorch?</td>\n",
       "      <td>how to save pytorch tensor in append mode\\nhow...</td>\n",
       "      <td>\\n\\nThe simplest way to save a trained model i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does .view() do in PyTorch?</td>\n",
       "      <td>what does -1 mean in pytorch view?\\nas the que...</td>\n",
       "      <td>\\n\\nThe .view() function in PyTorch is used to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do we need to call zero_grad() in PyTorch?</td>\n",
       "      <td>shall i use grad.zero_() in pytorch with or wi...</td>\n",
       "      <td>\\n\\nThe zero_grad() function in PyTorch is use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I print the model summary in PyTorch?</td>\n",
       "      <td>can't print model summary using pytorch?\\nhell...</td>\n",
       "      <td>\\n\\nThe best way to print the model summary in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do I initialize weights in PyTorch?</td>\n",
       "      <td>pytorch: initializing weight with numpy array ...</td>\n",
       "      <td>\\n\\nYou can initialize weights in PyTorch by u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does model.eval() do in pytorch?</td>\n",
       "      <td>which pytorch modules are affected by model.ev...</td>\n",
       "      <td>\\n\\nmodel.eval() sets the model in evaluation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the difference between reshape and view...</td>\n",
       "      <td>what is the different for torchvision.models.r...</td>\n",
       "      <td>\\n\\nThe difference between reshape and view in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What does model.train() do in PyTorch?</td>\n",
       "      <td>pytorch deep learning - class model() and trai...</td>\n",
       "      <td>\\n\\nModel.train() is a method in PyTorch that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does .contiguous() do in PyTorch?</td>\n",
       "      <td>pytorch - connection between loss.backward() a...</td>\n",
       "      <td>\\n\\nThe .contiguous() function in PyTorch is u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why do we \"pack\" the sequences in PyTorch?</td>\n",
       "      <td>why iterating over a pytorch dataloader never ...</td>\n",
       "      <td>\\n\\nPacking sequences in PyTorch is necessary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0         How do I check if PyTorch is using the GPU?   \n",
       "1           How do I save a trained model in PyTorch?   \n",
       "2                    What does .view() do in PyTorch?   \n",
       "3      Why do we need to call zero_grad() in PyTorch?   \n",
       "4        How do I print the model summary in PyTorch?   \n",
       "5             How do I initialize weights in PyTorch?   \n",
       "6               What does model.eval() do in pytorch?   \n",
       "7   What's the difference between reshape and view...   \n",
       "8              What does model.train() do in PyTorch?   \n",
       "9              What does .contiguous() do in PyTorch?   \n",
       "10         Why do we \"pack\" the sequences in PyTorch?   \n",
       "\n",
       "                                              context  \\\n",
       "0   pytorch isn't running on gpu while true\\ni wan...   \n",
       "1   how to save pytorch tensor in append mode\\nhow...   \n",
       "2   what does -1 mean in pytorch view?\\nas the que...   \n",
       "3   shall i use grad.zero_() in pytorch with or wi...   \n",
       "4   can't print model summary using pytorch?\\nhell...   \n",
       "5   pytorch: initializing weight with numpy array ...   \n",
       "6   which pytorch modules are affected by model.ev...   \n",
       "7   what is the different for torchvision.models.r...   \n",
       "8   pytorch deep learning - class model() and trai...   \n",
       "9   pytorch - connection between loss.backward() a...   \n",
       "10  why iterating over a pytorch dataloader never ...   \n",
       "\n",
       "                                               answer  \n",
       "0   \\n\\nyou can use the following code:\\n\\nimport ...  \n",
       "1   \\n\\nThe simplest way to save a trained model i...  \n",
       "2   \\n\\nThe .view() function in PyTorch is used to...  \n",
       "3   \\n\\nThe zero_grad() function in PyTorch is use...  \n",
       "4   \\n\\nThe best way to print the model summary in...  \n",
       "5   \\n\\nYou can initialize weights in PyTorch by u...  \n",
       "6   \\n\\nmodel.eval() sets the model in evaluation ...  \n",
       "7   \\n\\nThe difference between reshape and view in...  \n",
       "8   \\n\\nModel.train() is a method in PyTorch that ...  \n",
       "9   \\n\\nThe .contiguous() function in PyTorch is u...  \n",
       "10  \\n\\nPacking sequences in PyTorch is necessary ...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving generated answer with context to df\n",
    "\n",
    "df2.to_csv('openAILLm_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You must use a 64-bit version of Windows to install and use PyTorch.'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking using questions\n",
    "\n",
    "question = 'PyTorch does not work on Windows 32-bit'\n",
    "context = get_context(question, k=5)\n",
    "answer = run_qa(question, context)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe best way to make your experiment deterministic is to set a fixed random seed for all random number generators used in your experiment. This includes the random number generators used by PyTorch, NumPy, and any other libraries you are using. Setting a fixed random seed ensures that the same random numbers are generated each time the experiment is run, making the results reproducible.'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'How do I make my experiment deterministic?'\n",
    "context = get_context(question, k=5)\n",
    "answer = run_qa(question, context)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nScaling up your Pytorch models can be done in several ways. The most common approach is to increase the batch size, which can be done by increasing the number of data points in each batch. Additionally, you can increase the number of layers in the model, or increase the number of neurons in each layer. You can also increase the number of epochs used for training, or use a larger learning rate. Finally, you can also use more powerful hardware, such as GPUs, to speed up the training process.'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'How should I scale up my Pytorch models?'\n",
    "context = get_context(question, k=5)\n",
    "answer = run_qa(question, context)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
