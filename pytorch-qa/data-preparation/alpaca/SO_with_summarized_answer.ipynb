{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96eb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f38282",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fa8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean and process so\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile('<.*?>')\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "    \n",
    "def clean_data(df):\n",
    "    df[\"pt_answer\"] = df[\"pt_answer\"].apply(lambda x: cleanhtml(x))\n",
    "\n",
    "    df[\"question\"] = df[\"pt_title\"].str.lower() + \"\\n\" + df[\"pt_body\"]\n",
    "    df[\"question\"] = df[\"question\"].apply(lambda x: cleanhtml(x))\n",
    "    df[\"answer\"] = df[\"pt_answer\"].str.lower()\n",
    "\n",
    "    df = df[['pt_post_id','question', 'answer']]\n",
    "    return df\n",
    "    \n",
    "\n",
    "## get qa and link to post\n",
    "def get_url(df):\n",
    "    url = []\n",
    "    for index, row in df.iterrows():\n",
    "        url.append(f\"https://stackoverflow.com/questions/{row['pt_post_id']}/\")\n",
    "    \n",
    "    df['source'] = url\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8d7890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10763, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('pt_question_answers_updated.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec565eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(df)\n",
    "df = get_url(df)\n",
    "df = df[['question','answer','source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d20b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:5000]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916ac96",
   "metadata": {},
   "source": [
    "## using openai summarize accepted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619f89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_openai(context, index):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\", api_key = OPENAI_API_KEY,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": context}\n",
    "                  ]\n",
    "                )\n",
    "\n",
    "        qa = completion.choices[0].message.content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Request failed with error: {str(e)}.')\n",
    "        print(f'Waiting for 3 minutes before trying again...')\n",
    "        time.sleep(180)\n",
    "    \n",
    "    return (qa, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e732de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▎                                                                                                                                                                  | 71/5000 [00:50<28:41,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4728 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████████████▌                                                                                                                                        | 839/5000 [10:25<57:44,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 30eae6c29b6ef042f1d954015dd11a2f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████████████▉                                                                                                                                      | 914/5000 [11:18<21:56,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4112 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████████████▋                                                                                                                                 | 1032/5000 [12:41<19:42,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5185 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████████████████████████████▊                                                                                                                                 | 1039/5000 [12:44<22:04,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4320 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████████████████▏                                                                                                                             | 1141/5000 [13:58<37:40,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 902652e2bc423abc323234985d055bac in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████████████████▎                                                                                                                           | 1157/5000 [14:13<1:08:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3ed6807e81774dc34ec60b708bf3cd35 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████████████████████████▊                                                                                                                            | 1190/5000 [14:38<37:25,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4fc427cd0fec50ff52a9f482f85a40bb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████████████████████████▎                                                                                                                           | 1207/5000 [14:48<33:45,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4450 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|██████████████████████████████████████████████████▌                                                                                                                | 1552/5000 [19:11<21:27,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 7319 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████████████████████▍                                                                                                         | 1761/5000 [21:39<39:35,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 7088 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████████████████████████████████████▌                                                                                                      | 1857/5000 [22:55<20:28,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4474 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████████████████████████████████████▊                                                                                                    | 1928/5000 [23:49<33:49,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b12f7dd20645d5ba56af9630e1aaf80d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████▊                                                                                                | 2011/5000 [24:58<1:05:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 18abe07195fd4aa2947087d0a54ba017 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████████████████████████████████████████████████████████▉                                                                                               | 2083/5000 [25:55<19:59,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5134 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████████████████████████████▍                                                                                              | 2098/5000 [26:07<26:44,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4299 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████████████████████████▌                                                                                            | 2163/5000 [26:54<23:36,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5475 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████████████                                                                                       | 2333/5000 [29:07<19:24,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 9153 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████████████▏                                                                                    | 2397/5000 [29:56<13:58,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5342 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████████████▎                                                                                    | 2401/5000 [30:00<33:14,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 38349496a0fbbbe70d68c3c37932761f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 2615/5000 [32:35<34:53,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5132 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 2652/5000 [33:01<25:02,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4105 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 2740/5000 [34:15<24:48,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5127 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 2842/5000 [35:38<18:57,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4944 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 2882/5000 [36:09<14:10,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5889 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 2931/5000 [36:51<07:49,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4609 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 2958/5000 [37:08<09:40,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5344 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                             | 3108/5000 [39:08<16:29,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4827 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                         | 3255/5000 [41:07<31:49,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5006 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                        | 3267/5000 [41:17<18:09,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5763 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 3311/5000 [41:48<10:31,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4fb86db03d54dce0f4fd85d6458a9372 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 3450/5000 [43:45<10:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 8480 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                 | 3473/5000 [44:03<22:19,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 11498 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 3521/5000 [44:38<09:28,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4543 tokens. Please reduce the length of the messages.\n",
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4448 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 3537/5000 [44:50<12:40,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4731 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 3656/5000 [46:18<19:19,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4204 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                           | 3665/5000 [46:25<15:26,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0fb5d3103dc2bb53e79a0ffb99490db3 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 3691/5000 [46:46<14:26,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 6000 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 3747/5000 [47:33<18:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b28d856566637b3d2acea6f4019c9747 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                       | 3779/5000 [47:59<11:13,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 13301 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 3929/5000 [49:46<11:14,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 3935/5000 [49:49<07:46,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5507 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 3943/5000 [49:55<11:41,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4151 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 3949/5000 [50:01<15:45,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 8978 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 3954/5000 [50:05<10:36,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 4135/5000 [52:37<09:23,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                           | 4162/5000 [52:52<05:03,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4124 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 4175/5000 [53:04<09:08,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4174 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 4284/5000 [54:27<07:13,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 5287 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                     | 4335/5000 [55:05<05:15,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4487 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 4562/5000 [57:50<02:01,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 7159 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 4644/5000 [58:55<04:53,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4497 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 4714/5000 [59:53<05:31,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9785098eec624cf61051ce4e64ecbf30 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 4717/5000 [59:53<02:26,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 6869 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 4722/5000 [59:55<01:19,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 10830 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 4804/5000 [1:01:00<03:11,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4647 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 4847/5000 [1:01:36<01:25,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b872cab101ebfd425b86960b7a2ce36b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 4878/5000 [1:02:03<01:01,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: This model's maximum context length is 4097 tokens. However, your messages resulted in 4360 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [1:05:08<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated an exception: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Thu, 18 May 2023 03:38:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7c9109fad996c561-SEA', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions_ans = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    futures = []\n",
    "    for index,i in df.iterrows():\n",
    "\n",
    "        context = f\"Given a pytorch question and answer given below, How will an expert PyTorch engineer answer this question? Include code as appropriate and do not mentioned your role in the answer \\\n",
    "                    question: {i['question']}, answer:{i['answer']}\"\n",
    "        \n",
    "        futures.append(executor.submit(get_qa_openai, context, index))\n",
    "\n",
    "    for future, (_, row) in tqdm(zip(concurrent.futures.as_completed(futures), df.iterrows()), total=len(df)):\n",
    "        try:\n",
    "            qa, ind = future.result()\n",
    "            questions_ans.append((ind,qa))\n",
    "        except Exception as exc:\n",
    "            print(f'generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079ef6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, qa in questions_ans:\n",
    "    df.at[index, 'short_answer'] = qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc27acff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>short_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "      <td>as of pull request #496 torch now includes a b...</td>\n",
       "      <td>https://stackoverflow.com/questions/34750268/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "      <td>it is quite easy to do if you have the image i...</td>\n",
       "      <td>https://stackoverflow.com/questions/38543850/</td>\n",
       "      <td>As an expert PyTorch engineer, I can suggest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\nI'm tryin...</td>\n",
       "      <td>this is exactly that. \\nrecompile python under...</td>\n",
       "      <td>https://stackoverflow.com/questions/41767005/</td>\n",
       "      <td>Code to recompile Python with --enable-unicode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "      <td>as of pytorch 1.0 torch.utils.serialization is...</td>\n",
       "      <td>https://stackoverflow.com/questions/41861354/</td>\n",
       "      <td>An expert PyTorch engineer would first acknowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "      <td>yes, that is possible. just create the objects...</td>\n",
       "      <td>https://stackoverflow.com/questions/41924453/</td>\n",
       "      <td>An expert PyTorch engineer would answer by pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>compare two segmentation maps predictions\\nI a...</td>\n",
       "      <td>yes, this is a valid way to implement consiste...</td>\n",
       "      <td>https://stackoverflow.com/questions/67682106/</td>\n",
       "      <td>Regarding the computation of the total_supervi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>difference between dataset and tensordataset i...</td>\n",
       "      <td>the dataset class is an abstract class that is...</td>\n",
       "      <td>https://stackoverflow.com/questions/67683406/</td>\n",
       "      <td>An expert PyTorch engineer would explain that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>define nn.parameters with a for loop\\nI am int...</td>\n",
       "      <td>best way to accomplish this you can accomplish...</td>\n",
       "      <td>https://stackoverflow.com/questions/67689104/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>how to integrate a pytorch model into a dynami...</td>\n",
       "      <td>tensorflow or pytorch models can't be directly...</td>\n",
       "      <td>https://stackoverflow.com/questions/67693181/</td>\n",
       "      <td>As of now, direct integration of PyTorch model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>torch to tensorflow\\nIs there a way to convert...</td>\n",
       "      <td>per the comment, i would recommend using more ...</td>\n",
       "      <td>https://stackoverflow.com/questions/67694665/</td>\n",
       "      <td>An expert PyTorch engineer would recommend usi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     extracting the top-k value-indices from a 1-d ...   \n",
       "1     how to display custom images in tensorboard (e...   \n",
       "2     python wheels: cp27mu not supported\\nI'm tryin...   \n",
       "3     loading torch7 trained models (.t7) in pytorch...   \n",
       "4     pytorch: how to use dataloaders for custom dat...   \n",
       "...                                                 ...   \n",
       "4995  compare two segmentation maps predictions\\nI a...   \n",
       "4996  difference between dataset and tensordataset i...   \n",
       "4997  define nn.parameters with a for loop\\nI am int...   \n",
       "4998  how to integrate a pytorch model into a dynami...   \n",
       "4999  torch to tensorflow\\nIs there a way to convert...   \n",
       "\n",
       "                                                 answer  \\\n",
       "0     as of pull request #496 torch now includes a b...   \n",
       "1     it is quite easy to do if you have the image i...   \n",
       "2     this is exactly that. \\nrecompile python under...   \n",
       "3     as of pytorch 1.0 torch.utils.serialization is...   \n",
       "4     yes, that is possible. just create the objects...   \n",
       "...                                                 ...   \n",
       "4995  yes, this is a valid way to implement consiste...   \n",
       "4996  the dataset class is an abstract class that is...   \n",
       "4997  best way to accomplish this you can accomplish...   \n",
       "4998  tensorflow or pytorch models can't be directly...   \n",
       "4999  per the comment, i would recommend using more ...   \n",
       "\n",
       "                                             source  \\\n",
       "0     https://stackoverflow.com/questions/34750268/   \n",
       "1     https://stackoverflow.com/questions/38543850/   \n",
       "2     https://stackoverflow.com/questions/41767005/   \n",
       "3     https://stackoverflow.com/questions/41861354/   \n",
       "4     https://stackoverflow.com/questions/41924453/   \n",
       "...                                             ...   \n",
       "4995  https://stackoverflow.com/questions/67682106/   \n",
       "4996  https://stackoverflow.com/questions/67683406/   \n",
       "4997  https://stackoverflow.com/questions/67689104/   \n",
       "4998  https://stackoverflow.com/questions/67693181/   \n",
       "4999  https://stackoverflow.com/questions/67694665/   \n",
       "\n",
       "                                           short_answer  \n",
       "0     An expert PyTorch engineer would suggest using...  \n",
       "1     As an expert PyTorch engineer, I can suggest a...  \n",
       "2     Code to recompile Python with --enable-unicode...  \n",
       "3     An expert PyTorch engineer would first acknowl...  \n",
       "4     An expert PyTorch engineer would answer by pro...  \n",
       "...                                                 ...  \n",
       "4995  Regarding the computation of the total_supervi...  \n",
       "4996  An expert PyTorch engineer would explain that ...  \n",
       "4997  An expert PyTorch engineer would suggest using...  \n",
       "4998  As of now, direct integration of PyTorch model...  \n",
       "4999  An expert PyTorch engineer would recommend usi...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27983ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('so_5k_with_short_answer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8ab97",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841b924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question         0\n",
       "answer           0\n",
       "source           0\n",
       "short_answer    60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14c113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9299bb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>short_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "      <td>as of pull request #496 torch now includes a b...</td>\n",
       "      <td>https://stackoverflow.com/questions/34750268/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "      <td>it is quite easy to do if you have the image i...</td>\n",
       "      <td>https://stackoverflow.com/questions/38543850/</td>\n",
       "      <td>As an expert PyTorch engineer, I can suggest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\nI'm tryin...</td>\n",
       "      <td>this is exactly that. \\nrecompile python under...</td>\n",
       "      <td>https://stackoverflow.com/questions/41767005/</td>\n",
       "      <td>Code to recompile Python with --enable-unicode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "      <td>as of pytorch 1.0 torch.utils.serialization is...</td>\n",
       "      <td>https://stackoverflow.com/questions/41861354/</td>\n",
       "      <td>An expert PyTorch engineer would first acknowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "      <td>yes, that is possible. just create the objects...</td>\n",
       "      <td>https://stackoverflow.com/questions/41924453/</td>\n",
       "      <td>An expert PyTorch engineer would answer by pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>compare two segmentation maps predictions\\nI a...</td>\n",
       "      <td>yes, this is a valid way to implement consiste...</td>\n",
       "      <td>https://stackoverflow.com/questions/67682106/</td>\n",
       "      <td>Regarding the computation of the total_supervi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>difference between dataset and tensordataset i...</td>\n",
       "      <td>the dataset class is an abstract class that is...</td>\n",
       "      <td>https://stackoverflow.com/questions/67683406/</td>\n",
       "      <td>An expert PyTorch engineer would explain that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>define nn.parameters with a for loop\\nI am int...</td>\n",
       "      <td>best way to accomplish this you can accomplish...</td>\n",
       "      <td>https://stackoverflow.com/questions/67689104/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>how to integrate a pytorch model into a dynami...</td>\n",
       "      <td>tensorflow or pytorch models can't be directly...</td>\n",
       "      <td>https://stackoverflow.com/questions/67693181/</td>\n",
       "      <td>As of now, direct integration of PyTorch model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>torch to tensorflow\\nIs there a way to convert...</td>\n",
       "      <td>per the comment, i would recommend using more ...</td>\n",
       "      <td>https://stackoverflow.com/questions/67694665/</td>\n",
       "      <td>An expert PyTorch engineer would recommend usi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4940 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     extracting the top-k value-indices from a 1-d ...   \n",
       "1     how to display custom images in tensorboard (e...   \n",
       "2     python wheels: cp27mu not supported\\nI'm tryin...   \n",
       "3     loading torch7 trained models (.t7) in pytorch...   \n",
       "4     pytorch: how to use dataloaders for custom dat...   \n",
       "...                                                 ...   \n",
       "4995  compare two segmentation maps predictions\\nI a...   \n",
       "4996  difference between dataset and tensordataset i...   \n",
       "4997  define nn.parameters with a for loop\\nI am int...   \n",
       "4998  how to integrate a pytorch model into a dynami...   \n",
       "4999  torch to tensorflow\\nIs there a way to convert...   \n",
       "\n",
       "                                                 answer  \\\n",
       "0     as of pull request #496 torch now includes a b...   \n",
       "1     it is quite easy to do if you have the image i...   \n",
       "2     this is exactly that. \\nrecompile python under...   \n",
       "3     as of pytorch 1.0 torch.utils.serialization is...   \n",
       "4     yes, that is possible. just create the objects...   \n",
       "...                                                 ...   \n",
       "4995  yes, this is a valid way to implement consiste...   \n",
       "4996  the dataset class is an abstract class that is...   \n",
       "4997  best way to accomplish this you can accomplish...   \n",
       "4998  tensorflow or pytorch models can't be directly...   \n",
       "4999  per the comment, i would recommend using more ...   \n",
       "\n",
       "                                             source  \\\n",
       "0     https://stackoverflow.com/questions/34750268/   \n",
       "1     https://stackoverflow.com/questions/38543850/   \n",
       "2     https://stackoverflow.com/questions/41767005/   \n",
       "3     https://stackoverflow.com/questions/41861354/   \n",
       "4     https://stackoverflow.com/questions/41924453/   \n",
       "...                                             ...   \n",
       "4995  https://stackoverflow.com/questions/67682106/   \n",
       "4996  https://stackoverflow.com/questions/67683406/   \n",
       "4997  https://stackoverflow.com/questions/67689104/   \n",
       "4998  https://stackoverflow.com/questions/67693181/   \n",
       "4999  https://stackoverflow.com/questions/67694665/   \n",
       "\n",
       "                                           short_answer  \n",
       "0     An expert PyTorch engineer would suggest using...  \n",
       "1     As an expert PyTorch engineer, I can suggest a...  \n",
       "2     Code to recompile Python with --enable-unicode...  \n",
       "3     An expert PyTorch engineer would first acknowl...  \n",
       "4     An expert PyTorch engineer would answer by pro...  \n",
       "...                                                 ...  \n",
       "4995  Regarding the computation of the total_supervi...  \n",
       "4996  An expert PyTorch engineer would explain that ...  \n",
       "4997  An expert PyTorch engineer would suggest using...  \n",
       "4998  As of now, direct integration of PyTorch model...  \n",
       "4999  An expert PyTorch engineer would recommend usi...  \n",
       "\n",
       "[4940 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff9ad10f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2766/3823359095.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['q_len'] = df['question'].str.len()\n",
      "/tmp/ipykernel_2766/3823359095.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['c_len'] = df['answer'].str.len()\n",
      "/tmp/ipykernel_2766/3823359095.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['a_len'] = df['short_answer'].str.len()\n",
      "/tmp/ipykernel_2766/3823359095.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"total\"] = df['q_len'] + df['c_len'] + df['a_len']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>short_answer</th>\n",
       "      <th>q_len</th>\n",
       "      <th>c_len</th>\n",
       "      <th>a_len</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extracting the top-k value-indices from a 1-d ...</td>\n",
       "      <td>as of pull request #496 torch now includes a b...</td>\n",
       "      <td>https://stackoverflow.com/questions/34750268/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "      <td>359</td>\n",
       "      <td>788</td>\n",
       "      <td>473</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to display custom images in tensorboard (e...</td>\n",
       "      <td>it is quite easy to do if you have the image i...</td>\n",
       "      <td>https://stackoverflow.com/questions/38543850/</td>\n",
       "      <td>As an expert PyTorch engineer, I can suggest a...</td>\n",
       "      <td>599</td>\n",
       "      <td>1020</td>\n",
       "      <td>1095</td>\n",
       "      <td>2714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python wheels: cp27mu not supported\\nI'm tryin...</td>\n",
       "      <td>this is exactly that. \\nrecompile python under...</td>\n",
       "      <td>https://stackoverflow.com/questions/41767005/</td>\n",
       "      <td>Code to recompile Python with --enable-unicode...</td>\n",
       "      <td>2110</td>\n",
       "      <td>113</td>\n",
       "      <td>687</td>\n",
       "      <td>2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loading torch7 trained models (.t7) in pytorch...</td>\n",
       "      <td>as of pytorch 1.0 torch.utils.serialization is...</td>\n",
       "      <td>https://stackoverflow.com/questions/41861354/</td>\n",
       "      <td>An expert PyTorch engineer would first acknowl...</td>\n",
       "      <td>676</td>\n",
       "      <td>516</td>\n",
       "      <td>1112</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch: how to use dataloaders for custom dat...</td>\n",
       "      <td>yes, that is possible. just create the objects...</td>\n",
       "      <td>https://stackoverflow.com/questions/41924453/</td>\n",
       "      <td>An expert PyTorch engineer would answer by pro...</td>\n",
       "      <td>298</td>\n",
       "      <td>1443</td>\n",
       "      <td>2517</td>\n",
       "      <td>4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>compare two segmentation maps predictions\\nI a...</td>\n",
       "      <td>yes, this is a valid way to implement consiste...</td>\n",
       "      <td>https://stackoverflow.com/questions/67682106/</td>\n",
       "      <td>Regarding the computation of the total_supervi...</td>\n",
       "      <td>2830</td>\n",
       "      <td>421</td>\n",
       "      <td>194</td>\n",
       "      <td>3445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>difference between dataset and tensordataset i...</td>\n",
       "      <td>the dataset class is an abstract class that is...</td>\n",
       "      <td>https://stackoverflow.com/questions/67683406/</td>\n",
       "      <td>An expert PyTorch engineer would explain that ...</td>\n",
       "      <td>251</td>\n",
       "      <td>854</td>\n",
       "      <td>600</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>define nn.parameters with a for loop\\nI am int...</td>\n",
       "      <td>best way to accomplish this you can accomplish...</td>\n",
       "      <td>https://stackoverflow.com/questions/67689104/</td>\n",
       "      <td>An expert PyTorch engineer would suggest using...</td>\n",
       "      <td>1300</td>\n",
       "      <td>723</td>\n",
       "      <td>1019</td>\n",
       "      <td>3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>how to integrate a pytorch model into a dynami...</td>\n",
       "      <td>tensorflow or pytorch models can't be directly...</td>\n",
       "      <td>https://stackoverflow.com/questions/67693181/</td>\n",
       "      <td>As of now, direct integration of PyTorch model...</td>\n",
       "      <td>1410</td>\n",
       "      <td>914</td>\n",
       "      <td>930</td>\n",
       "      <td>3254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>torch to tensorflow\\nIs there a way to convert...</td>\n",
       "      <td>per the comment, i would recommend using more ...</td>\n",
       "      <td>https://stackoverflow.com/questions/67694665/</td>\n",
       "      <td>An expert PyTorch engineer would recommend usi...</td>\n",
       "      <td>876</td>\n",
       "      <td>1465</td>\n",
       "      <td>2060</td>\n",
       "      <td>4401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4940 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     extracting the top-k value-indices from a 1-d ...   \n",
       "1     how to display custom images in tensorboard (e...   \n",
       "2     python wheels: cp27mu not supported\\nI'm tryin...   \n",
       "3     loading torch7 trained models (.t7) in pytorch...   \n",
       "4     pytorch: how to use dataloaders for custom dat...   \n",
       "...                                                 ...   \n",
       "4995  compare two segmentation maps predictions\\nI a...   \n",
       "4996  difference between dataset and tensordataset i...   \n",
       "4997  define nn.parameters with a for loop\\nI am int...   \n",
       "4998  how to integrate a pytorch model into a dynami...   \n",
       "4999  torch to tensorflow\\nIs there a way to convert...   \n",
       "\n",
       "                                                 answer  \\\n",
       "0     as of pull request #496 torch now includes a b...   \n",
       "1     it is quite easy to do if you have the image i...   \n",
       "2     this is exactly that. \\nrecompile python under...   \n",
       "3     as of pytorch 1.0 torch.utils.serialization is...   \n",
       "4     yes, that is possible. just create the objects...   \n",
       "...                                                 ...   \n",
       "4995  yes, this is a valid way to implement consiste...   \n",
       "4996  the dataset class is an abstract class that is...   \n",
       "4997  best way to accomplish this you can accomplish...   \n",
       "4998  tensorflow or pytorch models can't be directly...   \n",
       "4999  per the comment, i would recommend using more ...   \n",
       "\n",
       "                                             source  \\\n",
       "0     https://stackoverflow.com/questions/34750268/   \n",
       "1     https://stackoverflow.com/questions/38543850/   \n",
       "2     https://stackoverflow.com/questions/41767005/   \n",
       "3     https://stackoverflow.com/questions/41861354/   \n",
       "4     https://stackoverflow.com/questions/41924453/   \n",
       "...                                             ...   \n",
       "4995  https://stackoverflow.com/questions/67682106/   \n",
       "4996  https://stackoverflow.com/questions/67683406/   \n",
       "4997  https://stackoverflow.com/questions/67689104/   \n",
       "4998  https://stackoverflow.com/questions/67693181/   \n",
       "4999  https://stackoverflow.com/questions/67694665/   \n",
       "\n",
       "                                           short_answer  q_len  c_len  a_len  \\\n",
       "0     An expert PyTorch engineer would suggest using...    359    788    473   \n",
       "1     As an expert PyTorch engineer, I can suggest a...    599   1020   1095   \n",
       "2     Code to recompile Python with --enable-unicode...   2110    113    687   \n",
       "3     An expert PyTorch engineer would first acknowl...    676    516   1112   \n",
       "4     An expert PyTorch engineer would answer by pro...    298   1443   2517   \n",
       "...                                                 ...    ...    ...    ...   \n",
       "4995  Regarding the computation of the total_supervi...   2830    421    194   \n",
       "4996  An expert PyTorch engineer would explain that ...    251    854    600   \n",
       "4997  An expert PyTorch engineer would suggest using...   1300    723   1019   \n",
       "4998  As of now, direct integration of PyTorch model...   1410    914    930   \n",
       "4999  An expert PyTorch engineer would recommend usi...    876   1465   2060   \n",
       "\n",
       "      total  \n",
       "0      1620  \n",
       "1      2714  \n",
       "2      2910  \n",
       "3      2304  \n",
       "4      4258  \n",
       "...     ...  \n",
       "4995   3445  \n",
       "4996   1705  \n",
       "4997   3042  \n",
       "4998   3254  \n",
       "4999   4401  \n",
       "\n",
       "[4940 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['q_len'] = df['question'].str.len()\n",
    "df['c_len'] = df['answer'].str.len()\n",
    "df['a_len'] = df['short_answer'].str.len()\n",
    "df[\"total\"] = df['q_len'] + df['c_len'] + df['a_len']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5ffcc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "!!!! Question:  extracting the top-k value-indices from a 1-d tensor\n",
      "Given a 1-D tensor in Torch (torch.Tensor), containing values which can be compared (say floating point), how can we extract the indices of the top-k values in that tensor?\n",
      "Apart from the brute-force method, I am looking for some API call, that Torch/lua provides, which can perform this task efficiently.\n",
      "\n",
      "!!!! Context:  as of pull request #496 torch now includes a built-in api named torch.topk. example:\n",
      "\n",
      "&gt; t = torch.tensor{9, 1, 8, 2, 7, 3, 6, 4, 5}\n",
      "\n",
      "-- obtain the 3 smallest elements\n",
      "&gt; res = t:topk(3)\n",
      "&gt; print(res)\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.doubletensor of size 3]\n",
      "\n",
      "-- you can also get the indices in addition\n",
      "&gt; res, ind = t:topk(3)\n",
      "&gt; print(ind)\n",
      " 2\n",
      " 4\n",
      " 6\n",
      "[torch.longtensor of size 3]\n",
      "\n",
      "-- alternatively you can obtain the k largest elements as follow\n",
      "-- (see the api documentation for more details)\n",
      "&gt; res = t:topk(3, true)\n",
      "&gt; print(res)\n",
      " 9\n",
      " 8\n",
      " 7\n",
      "[torch.doubletensor of size 3]\n",
      "\n",
      "\n",
      "at the time of writing the cpu implementation follows a sort and narrow approach (there are plans to improve it in the future). that being said an optimized gpu implementation for cutorch is currently being reviewed.\n",
      "\n",
      "!!!! Answer:  An expert PyTorch engineer would suggest using the built-in API call torch.topk to efficiently extract the top-k value indices from a 1-D tensor. The engineer would provide an example code snippet showcasing how to use the API call to obtain the k largest or smallest elements along with their indices. The engineer would also mention that while the current CPU implementation follows a sort and narrow approach, an optimized GPU implementation for cutorch is under review.\n",
      "****************************************************************************************************\n",
      "!!!! Question:  how to display custom images in tensorboard (e.g. matplotlib plots)?\n",
      "The Image Dashboard section of the Tensorboard ReadMe says:\n",
      "\n",
      "\n",
      "  Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard.\n",
      "\n",
      "\n",
      "I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?  \n",
      "\n",
      "!!!! Context:  it is quite easy to do if you have the image in a memory buffer. below, i show an example, where a pyplot is saved to a buffer and then converted to a tf image representation which is then sent to an image summary.\n",
      "\n",
      "import io\n",
      "import matplotlib.pyplot as plt\n",
      "import tensorflow as tf\n",
      "\n",
      "\n",
      "def gen_plot():\n",
      "    \"\"\"create a pyplot plot and save to buffer.\"\"\"\n",
      "    plt.figure()\n",
      "    plt.plot([1, 2])\n",
      "    plt.title(\"test\")\n",
      "    buf = io.bytesio()\n",
      "    plt.savefig(buf, format='png')\n",
      "    buf.seek(0)\n",
      "    return buf\n",
      "\n",
      "\n",
      "# prepare the plot\n",
      "plot_buf = gen_plot()\n",
      "\n",
      "# convert png buffer to tf image\n",
      "image = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
      "\n",
      "# add the batch dimension\n",
      "image = tf.expand_dims(image, 0)\n",
      "\n",
      "# add image summary\n",
      "summary_op = tf.summary.image(\"plot\", image)\n",
      "\n",
      "# session\n",
      "with tf.session() as sess:\n",
      "    # run\n",
      "    summary = sess.run(summary_op)\n",
      "    # write summary\n",
      "    writer = tf.train.summarywriter('./logs')\n",
      "    writer.add_summary(summary)\n",
      "    writer.close()\n",
      "\n",
      "\n",
      "this gives the following tensorboard visualization:\n",
      "\n",
      "\n",
      "\n",
      "!!!! Answer:  As an expert PyTorch engineer, I can suggest a similar approach using Matplotlib and Torchvision packages as below:\n",
      "\n",
      "import torch\n",
      "from torchvision.utils import make_grid\n",
      "from matplotlib import pyplot as plt\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "\n",
      "# Create your custom plot with Matplotlib\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.imshow(my_custom_image)\n",
      "\n",
      "# Convert Matplotlib plot to a PyTorch Tensor\n",
      "canvas = fig.canvas\n",
      "width, height = canvas.get_width_height()\n",
      "plot_tensor = torch.ByteTensor(\n",
      "    torch.ByteStorage.from_buffer(canvas.tostring())).view(height, width, 4).permute(2, 0, 1)\n",
      "\n",
      "# Create grid and add to Tensorboard\n",
      "grid = make_grid(plot_tensor)\n",
      "writer = SummaryWriter()\n",
      "writer.add_image('my_custom_image', grid, global_step=0)\n",
      "writer.close()\n",
      "\n",
      "In this code snippet, we create a custom Matplotlib plot and convert it to a PyTorch tensor using the ByteTensor function. Then we use the make_grid function from Torchvision to create a grid from this tensor. Finally, we add this grid to the Tensorboard writer using the add_image function. Hope this helps!\n",
      "****************************************************************************************************\n",
      "!!!! Question:  python wheels: cp27mu not supported\n",
      "I'm trying to install pytorch (http://pytorch.org/) on Linux, and according to my machine configuration, the command I should run is:\n",
      "\n",
      "pip install https://s3.amazonaws.com/pytorch/whl/torch-0.1.6.post17-cp27-cp27mu-linux_x86_64.whl\n",
      "\n",
      "On one machine (Linux distribution Slackware 14.1) the installation fails with error: torch-0.1.6.post17-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform., while on another one (Ubuntu 15.10) it succeeds. From what I understood, the problem seems to be the cp27mu in the wheel name.\n",
      "\n",
      "Using the command import pip; print(pip.pep425tags.get_supported()) from the Python shell, I get this from the Slackware machine:\n",
      "\n",
      "[('cp27', 'cp27m', 'manylinux1_x86_64'), ('cp27', 'cp27m', 'linux_x86_64'), ('cp27', 'none', 'manylinux1_x86_64'), ('cp27', 'none', 'linux_x86_64'), ('py2', 'none', 'manylinux1_x86_64'), ('py2', 'none', 'linux_x86_64'), ('cp27', 'none', 'any'), ('cp2', 'none', 'any'), ('py27', 'none', 'any'), ('py2', 'none', 'any'), ('py26', 'none', 'any'), ('py25', 'none', 'any'), ('py24', 'none', 'any'), ('py23', 'none', 'any'), ('py22', 'none', 'any'), ('py21', 'none', 'any'), ('py20', 'none', 'any')]\n",
      "\n",
      "and this from the Ubuntu machine:\n",
      "\n",
      "[('cp27', 'cp27mu', 'manylinux1_x86_64'), ('cp27', 'cp27mu', 'linux_x86_64'), ('cp27', 'none', 'manylinux1_x86_64'), ('cp27', 'none', 'linux_x86_64'), ('py2', 'none', 'manylinux1_x86_64'), ('py2', 'none', 'linux_x86_64'), ('cp27', 'none', 'any'), ('cp2', 'none', 'any'), ('py27', 'none', 'any'), ('py2', 'none', 'any'), ('py26', 'none', 'any'), ('py25', 'none', 'any'), ('py24', 'none', 'any'), ('py23', 'none', 'any'), ('py22', 'none', 'any'), ('py21', 'none', 'any'), ('py20', 'none', 'any')]\n",
      "\n",
      "From https://www.python.org/dev/peps/pep-0513/, it seems to me that supporting cp27m or cp27mu depends on an option passed at compile time, --enable-unicode.\n",
      "\n",
      "Now, maybe at this point I shouldn't even be asking the question, but just to be sure, does this mean that I have to compile Python with the --enable-unicode=ucs4 on the Slackware machine in order to install that wheel?\n",
      "\n",
      "!!!! Context:  this is exactly that. \n",
      "recompile python under slack with --enable-unicode=ucs4 and you can then install the whl.\n",
      "\n",
      "!!!! Answer:  Code to recompile Python with --enable-unicode=ucs4:\n",
      "\n",
      "1. Download the Python source code from https://www.python.org/downloads/source/.\n",
      "2. Extract the source code.\n",
      "3. Navigate to the \"Modules\" folder inside the extracted source code.\n",
      "4. Open the \"Setup.dist\" file and uncomment the line \"unicode=ucs4\".\n",
      "5. Navigate back to the root folder of the extracted source code.\n",
      "6. Run the following commands in the terminal:\n",
      "   - ./configure --enable-unicode=ucs4\n",
      "   - make\n",
      "   - make install\n",
      "7. After the recompilation is complete, try installing the PyTorch wheel again using the same command:\n",
      "   - pip install https://s3.amazonaws.com/pytorch/whl/torch-0.1.6.post17-cp27-cp27mu-linux_x86_64.whl\n",
      "****************************************************************************************************\n",
      "!!!! Question:  loading torch7 trained models (.t7) in pytorch\n",
      "I am using Torch7 library for implementing neural networks.  Mostly, I rely on pre-trained models. In Lua I use torch.load function to load a model saved as torch .t7 file. I am curious about switching to PyTorch( http://pytorch.org) and I read the documents. I couldn't find any information regarding the mechanisms to load a pre-trained model. The only relevant information I was able to find is this page:http://pytorch.org/docs/torch.html\n",
      "\n",
      "But the function torch.load described in the page seems to load a file saved with pickle. If someone has additional information on loading .t7 models in PyTorch, please share it here. \n",
      "\n",
      "!!!! Context:  as of pytorch 1.0 torch.utils.serialization is completely removed. hence no one can import models from lua torch into pytorch anymore. instead, i would suggest installing pytorch 0.4.1 through pip in a conda environment (so that you can remove it after this) and use this repo to convert your lua torch model to pytorch model, not just the torch.nn.legacy model that you cannot use for training. then use pytorch 1.xx to do whatever with it. you can also train your converted lua torch models in pytorch this way :)\n",
      "\n",
      "!!!! Answer:  An expert PyTorch engineer would first acknowledge that loading Torch7 trained models (.t7) in PyTorch is not possible anymore due to the complete removal of torch.utils.serialization in PyTorch 1.0. They would then provide a solution to the problem by recommending installing PyTorch 0.4.1 through pip in a conda environment, using a specific repo to convert the lua torch model to PyTorch model, and finally using PyTorch 1.xx to do whatever with it. Additionally, an expert engineer would mention that the converted lua torch models can also be trained in PyTorch as well. Below is an example code snippet demonstrating how to convert a lua torch model to PyTorch model using the recommended approach:\n",
      "\n",
      "```\n",
      "# install pytorch 0.4.1 in conda environment\n",
      "conda create -n lua-torch-env python=3.6\n",
      "source activate lua-torch-env\n",
      "pip install torch==0.4.1\n",
      "\n",
      "# clone repo for conversion\n",
      "git clone https://github.com/clcarwin/convert_torch_to_pytorch.git\n",
      "cd convert_torch_to_pytorch\n",
      "\n",
      "# convert lua torch model to pytorch model\n",
      "python convert_torch_to_pytorch.py /path/to/lua/torch/model.t7 /path/to/pytorch/model.pth\n",
      "```\n",
      "****************************************************************************************************\n",
      "!!!! Question:  pytorch: how to use dataloaders for custom datasets\n",
      "How to make use of the torch.utils.data.Dataset and torch.utils.data.DataLoader on your own data (not just the torchvision.datasets)?\n",
      "\n",
      "Is there a way to use the inbuilt DataLoaders which they use on TorchVisionDatasets to be used on any dataset?\n",
      "\n",
      "!!!! Context:  yes, that is possible. just create the objects by yourself, e.g.\n",
      "\n",
      "import torch.utils.data as data_utils\n",
      "\n",
      "train = data_utils.tensordataset(features, targets)\n",
      "train_loader = data_utils.dataloader(train, batch_size=50, shuffle=true)\n",
      "\n",
      "\n",
      "where features and targets are tensors. features has to be 2-d, i.e. a matrix where each line represents one training sample, and targets may be 1-d or 2-d, depending on whether you are trying to predict a scalar or a vector.\n",
      "\n",
      "hope that helps!\n",
      "\n",
      "\n",
      "\n",
      "edit: response to @sarthak's question\n",
      "\n",
      "basically yes. if you create an object of type tensordata, then the constructor investigates whether the first dimensions of the feature tensor (which is actually called data_tensor) and the target tensor (called target_tensor) have the same length:\n",
      "\n",
      "assert data_tensor.size(0) == target_tensor.size(0)\n",
      "\n",
      "\n",
      "however, if you want to feed these data into a neural network subsequently, then you need to be careful. while convolution layers work on data like yours, (i think) all of the other types of layers expect the data to be given in matrix form. so, if you run into an issue like this, then an easy solution would be to convert your 4d-dataset (given as some kind of tensor, e.g. floattensor) into a matrix by using the method view. for your 5000xnxnx3 dataset, this would look like this:\n",
      "\n",
      "2d_dataset = 4d_dataset.view(5000, -1)\n",
      "\n",
      "\n",
      "(the value -1 tells pytorch to figure out the length of the second dimension automatically.)\n",
      "\n",
      "!!!! Answer:  An expert PyTorch engineer would answer by providing example code on how to create custom datasets and dataloaders using torch.utils.data.Dataset and torch.utils.data.DataLoader. They would explain that it is indeed possible to use the inbuilt DataLoaders on any dataset by creating the objects yourself. They would also provide guidance on how to structure the feature and target tensors and caution about the potential issues when using non-convolutional layers with your custom dataset. The full answer would look like this:\n",
      "\n",
      "Yes, it is possible to use torch.utils.data.Dataset and torch.utils.data.DataLoader on custom datasets. Here is an example code on how to create custom datasets and dataloaders:\n",
      "\n",
      "import torch\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "# Define a custom dataset by inheriting from torch.utils.data.Dataset\n",
      "class CustomDataset(Dataset):\n",
      "    def __init__(self, features, targets):\n",
      "        super(CustomDataset, self).__init__()\n",
      "        self.features = features\n",
      "        self.targets = targets\n",
      "        \n",
      "    def __getitem__(self, idx):\n",
      "        return self.features[idx], self.targets[idx]\n",
      "    \n",
      "    def __len__(self):\n",
      "        return len(self.features)\n",
      "\n",
      "# Load your custom features and targets as tensors\n",
      "features = torch.randn(5000, 3, 28, 28)\n",
      "targets = torch.randint(0, 10, (5000,))\n",
      "\n",
      "# Create an object of CustomDataset\n",
      "custom_dataset = CustomDataset(features, targets)\n",
      "\n",
      "# Create an object of DataLoader using your CustomDataset\n",
      "batch_size = 50\n",
      "shuffle = True\n",
      "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=shuffle)\n",
      "\n",
      "You can also use the inbuilt DataLoaders on your custom datasets by creating the objects yourself as shown above.\n",
      "\n",
      "When using custom datasets it's important to structure your feature tensor such that it is 2D, i.e. a matrix where each row represents one training sample. Your target tensor may be 1D or 2D depending on whether you're predicting a scalar or a vector. Additionally, if you're using non-convolutional layers in your neural network, you need to be careful. While convolutional layers work on data like your custom dataset, other layers expect the data to be given in matrix form. If you run into an issue like this, an easy solution is to reshape your 4D dataset into a 2D matrix using the view() method. For example:\n",
      "\n",
      "# Reshape your 4D dataset into a 2D matrix using view()\n",
      "matrix_dataset = features.view(5000, -1)\n",
      "\n",
      "The value -1 tells PyTorch to automatically figure out the length of the second dimension.\n",
      "\n",
      "I hope that helps!\n"
     ]
    }
   ],
   "source": [
    "## checking sample data\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"*\" * 100)\n",
    "    print(\"!!!! Question: \", df.loc[i, \"question\"])\n",
    "    print(\"!!!! Context: \", df.loc[i, \"answer\"])\n",
    "    print(\"!!!! Answer: \", df.loc[i, \"short_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0d68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba808bd8",
   "metadata": {},
   "source": [
    "## preparing alpaca lora dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ff8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    d = { \"instruction\": row['question'],\n",
    "          \"input\": row['answer'],\n",
    "          \"output\": row['short_answer']\n",
    "        }\n",
    "    \n",
    "    data.append(d)\n",
    "    \n",
    "\n",
    "with open('so_5k_with_short_answer.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec787e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
