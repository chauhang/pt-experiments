{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7a9c13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f6fc54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.157)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (1.5.6)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.64.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.47)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from httpx) (0.17.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (3.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.3->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain  nest_asyncio httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe332c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import ConversationalAgent\n",
    "# dir(ConversationalAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c613ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22141977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, LlamaForCausalLM, LlamaTokenizer, TextStreamer\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from llama_index import download_loader, SummaryPrompt, LLMPredictor, GithubRepositoryReader, GPTVectorStoreIndex, GPTTreeIndex, GPTListIndex, GPTSimpleKeywordTableIndex, PromptHelper, SimpleDirectoryReader, ServiceContext, LangchainEmbedding\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser, NodeParser\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from llama_index.indices.composability import ComposableGraph\n",
    "\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
    "from langchain.agents import ConversationalAgent\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.agents import Tool\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b3129d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4f3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88844d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 512\n",
    "# set number of output tokens\n",
    "num_output = 128\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c63946",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "llm_predictor = LLMPredictor(llm=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ee1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e5412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39d05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=256, chunk_overlap=20)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "143417c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f39e03e7949a1b4e44670f1de77af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28865f0e5cea4dc6a17852f39717eb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4128b382265747af9976572899817eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00238efba2a7473e805bb72e2957ec48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765c16bc7bb149ada72d5cc0d4a1f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6d057a7a6c40a4bbd96d661fe80556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76900b13f031451bbe9f60d52cd5c8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236ec110e7a64acc97cb93c4754ac539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249a00fcd17411fa35ed6d4ee8e1785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0bd1b90af446e98c4a7824bd77c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28d9d7d24bf4adbace30382910edb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51f4a0c4e8e45fc9025815328e82ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fad95f7c9946f69abaad27d7e2a191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf4757361ef4485b642dd70900548b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5c4be8ddba4d4a86ac1231e0d1a9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf7e5b9bdb41098317978e036c551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92eb891d54849cca39f2cc73354a61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabad30a4eac4f1292e7c9b2cf6f16cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a755d595e36d4a658cd86a86eb207576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e39fd4d025f459b8cce7cc5b247a1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae47440b54846c795c0a014208e71ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d015b762764802862451f11f7626f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 49781 tokens\n"
     ]
    }
   ],
   "source": [
    "# RSS Feed\n",
    "RssReader = download_loader(\"RssReader\")\n",
    "\n",
    "reader = RssReader()\n",
    "rss_feed_documents = reader.load_data([\n",
    "    \"https://pytorch.org/feed.xml\",\n",
    "])\n",
    "\n",
    "rss_feed_nodes = parser.get_nodes_from_documents(rss_feed_documents)\n",
    "rss_feed_index = GPTVectorStoreIndex(rss_feed_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "537b3a6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GitHub Repo\n",
    "# download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "# from llama_index.readers.llamahub_modules.github_repo import GithubRepositoryReader, GithubClient\n",
    "\n",
    "# github_client = GithubClient(os.getenv(\"GITHUB_TOKEN\"))\n",
    "# loader = GithubRepositoryReader(\n",
    "#     github_client,\n",
    "#     owner =                  \"jagadeeshi2i\",\n",
    "#     repo =                   \"pytorch.github.io-1\",\n",
    "#     filter_directories =     ([\"docs\", \"_posts\", \"_getting_started\", \"_news\", \"_mobile\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     filter_file_extensions = ([\".html\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     verbose =                False,\n",
    "#     concurrent_requests =    10,\n",
    "# )\n",
    "\n",
    "# loader = GithubRepositoryReader(\n",
    "#     github_token=os.getenv(\"GITHUB_TOKEN\"),\n",
    "#     owner=\"jagadeeshi2i\",\n",
    "#     repo=\"pytorch.github.io-1\",\n",
    "#     use_parser=False,\n",
    "#     verbose=False,\n",
    "#     concurrent_requests = 10\n",
    "# )\n",
    "# github_docs = loader.load_data(branch=\"master\")\n",
    "\n",
    "# github_site_nodes = parser.get_nodes_from_documents(github_docs)\n",
    "# github_site_index = GPTVectorStoreIndex(github_site_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "49e35f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "# from llama_index.vector_stores import FaissVectorStore\n",
    "# from llama_index import FaissReader\n",
    "\n",
    "# # create faiss index\n",
    "# d = 1536\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# # construct vector store\n",
    "# vector_store = FaissVectorStore(faiss_index, persist_dir='./faiss')\n",
    "# # # construct vector store\n",
    "# # faiss_index = FAISS.load_local('./faiss', HuggingFaceEmbeddings())\n",
    "# # FaissReader.load_data(faiss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "786daa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rss_index_id = rss_feed_index.index_id\n",
    "# rss_feed_index.storage_context.persist()\n",
    "# github_site_index_id = github_site_index.index_id\n",
    "# github_site_index.storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da87a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# export DATASTORE=\"llama\"\n",
    "# export BEARER_TOKEN=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\"\n",
    "# export OPENAI_API_KEY=\"sk-Muvjr21fNSEc4rZQNThcT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d15faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPTRetrievalPluginReader = download_loader(\"ChatGPTRetrievalPluginReader\")\n",
    "\n",
    "# bearer_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\"\n",
    "# reader = ChatGPTRetrievalPluginReader(\n",
    "#     endpoint_url=\"http://localhost:8000\",\n",
    "#     bearer_token=bearer_token\n",
    "# )\n",
    "\n",
    "# plugin_documents = reader.load_data(\"text query\")\n",
    "# plugin_nodes = parser.get_nodes_from_documents(plugin_documents)\n",
    "# plugin_index = GPTVectorStoreIndex(plugin_nodes, service_context=service_context)\n",
    "# plugin_index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2890eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "indices.append(rss_feed_index)\n",
    "indices.append(faiss_index)\n",
    "# indices.append(github_site_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c95c2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736c704333e9454ebda065de2a002a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 757 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/IPython/core/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">execution.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1325</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">time</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 │   │   │   </span>st = clock2()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1325 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exec(code, glob, local_ns)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1326 │   │   │   │   </span>out=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1327 │   │   │   │   # multi-line %%time case</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1328 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> expr_val <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'as_query_engine'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/\u001b[0m\u001b[1;33mexecution.py\u001b[0m:\u001b[94m1325\u001b[0m in \u001b[92mtime\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   │   \u001b[0mst = clock2()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1325 \u001b[2m│   │   │   │   \u001b[0mexec(code, glob, local_ns)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1326 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mout=\u001b[94mNone\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1327 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# multi-line %%time case\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1328 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m expr_val \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'FAISS'\u001b[0m object has no attribute \u001b[32m'as_query_engine'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "indices_summary = []\n",
    "for index in indices:\n",
    "    index_engine = index.as_query_engine(response_mode=\"compact\")\n",
    "    summary = index_engine.query(\n",
    "        \"What is a summary of this document?\")\n",
    "    indices_summary.append(str(summary))\n",
    "print(indices_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b12e757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    GPTListIndex,\n",
    "    indices,\n",
    "    index_summaries=indices_summary,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1507940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# graph_engine = graph.as_query_engine()\n",
    "# response = graph_engine.query(\n",
    "#     \"What is pytorch?\", \n",
    "# )\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571ff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decompose transform\n",
    "# from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "# from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "# decompose_transform = DecomposeQueryTransform(\n",
    "#     llm_predictor, verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe028287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom query engines\n",
    "# custom_query_engines = {}\n",
    "# for index in indices:\n",
    "#     query_engine = index.as_query_engine()\n",
    "#     query_engine = TransformQueryEngine(\n",
    "#         query_engine,\n",
    "#         query_transform=decompose_transform,\n",
    "#         transform_extra_info={'index_summary': index.index_struct.summary},\n",
    "#     )\n",
    "#     custom_query_engines[index.index_id] = query_engine\n",
    "# custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
    "#     response_mode='tree_summarize',\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d01be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct query engine\n",
    "# graph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8695924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index configs\n",
    "index_configs = []\n",
    "for index in indices:\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=3,\n",
    "    )\n",
    "    tool_config = IndexToolConfig(\n",
    "        query_engine=query_engine, \n",
    "        name=f\"Vector Index\",\n",
    "        description=f\"useful for when you want to answer queries\",\n",
    "        tool_kwargs={\"return_direct\": True, \"return_sources\": True},\n",
    "    )\n",
    "    index_configs.append(tool_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4caf2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph config\n",
    "# graph_config = IndexToolConfig(\n",
    "#     query_engine=graph_query_engine,\n",
    "#     name=f\"Graph Index\",\n",
    "#     description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.\",\n",
    "#     tool_kwargs={\"return_direct\": True, \"return_sources\": True},\n",
    "#     return_sources=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1888f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a chatbot built to answer questions related to PyTorch.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40be0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "toolkit = LlamaToolkit(\n",
    "    index_configs=index_configs,\n",
    "#     graph_configs=[graph_config]\n",
    ")\n",
    "\n",
    "memory = GPTIndexChatMemory(\n",
    "    index=GPTListIndex([]), \n",
    "    memory_key=\"chat_history\", \n",
    "    query_kwargs={\"response_mode\": \"compact\"},\n",
    "    # return_source returns source nodes instead of querying index\n",
    "    return_source=True,\n",
    "    # return_messages returns context in message format\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "llm=hf_pipeline\n",
    "agent_chain = create_llama_chat_agent(\n",
    "    toolkit,\n",
    "    llm=HuggingFacePipeline(pipeline=generate_text),\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b803e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pytorch is a deep learning framework that is especially suited to machine learning and artificial neural networks. It is highly efficient and provides a high level of performance, especially when training deep learning models.\n",
      "\n",
      "Pytorch is widely used in machine learning and deep learning research and development because of its easy-to-use API and ability to quickly develop and train complex models. It is highly regarded for its stability and speed.\n",
      "\n",
      "This has resulted in Pytorch becoming one of the most popular deep learning frameworks. It is particularly well known among deep learning developers because of its broad spectrum of capabilities, fast development, and ease of use.\n",
      "\n",
      "Why use Pytorch?\n",
      "Pytorch enables you to achieve results faster with fewer resources. Using Pytorch, you can train neural networks for recognition, classification, image processing, and regression with ease.\n",
      "\n",
      "As a flexible and scalable platform, Pytorch is especially suitable for developers. It includes various optimization techniques, which helps to reduce the amount of code and compile time.\n",
      "\n",
      "If you are a developer and want to create your own AI model, Pytorch has the tools you need to create efficient and powerful AI models.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = agent_chain.run(input=\"what is pytorch\")\n",
    "except Exception as e:\n",
    "    response = str(e)\n",
    "    if not response.startswith(\"Could not parse LLM output: `\"):\n",
    "        raise e\n",
    "    response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d908230",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to load a model in pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First, install Pytorch with Conda:\n",
      "\n",
      "`conda install pytorch`\n",
      "\n",
      "Then, unzip the t8d-wmt18-large model to a new folder:\n",
      "\n",
      "`cd [folder]`\n",
      "\n",
      "`gunzip t8d-wmt18-large.gz`\n",
      "\n",
      "`mv t8d-wmt18-large t8d-wmt18-large-backup`\n",
      "\n",
      "Now you can load the model in pytorch:\n",
      "\n",
      "`cd t8d-wmt18-large-backup`\n",
      "\n",
      "`pip install -e.`\n",
      "\n",
      "`python -m pytorch_model_handler t8d-wmt18-large --output_folder /path/to/save`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 2 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>text_input = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   │   </span>response = agent_chain.run(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>=text_input)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1191</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raw_input</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1188 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._allow_stdin:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1189 │   │   │   </span>msg = <span style=\"color: #808000; text-decoration-color: #808000\">\"raw_input was called, but this frontend does not support input reques</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1190 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> StdinNotImplementedError(msg)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1191 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._input_request(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(prompt),                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_ident[<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>],                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1194 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_parent(<span style=\"color: #808000; text-decoration-color: #808000\">\"shell\"</span>),                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1234</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_input_request</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1231 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1232 │   │   │   │   # re-raise KeyboardInterrupt, to truncate traceback</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1233 │   │   │   │   </span>msg = <span style=\"color: #808000; text-decoration-color: #808000\">\"Interrupted by user\"</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1234 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>(msg) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1235 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1236 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.log.warning(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid Message:\"</span>, exc_info=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1237 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt: </span>Interrupted by user\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 2 \u001b[2m│   \u001b[0mtext_input = \u001b[96minput\u001b[0m()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   │   \u001b[0mresponse = agent_chain.run(\u001b[96minput\u001b[0m=text_input)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m1191\u001b[0m in \u001b[92mraw_input\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1188 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._allow_stdin:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1189 \u001b[0m\u001b[2m│   │   │   \u001b[0mmsg = \u001b[33m\"\u001b[0m\u001b[33mraw_input was called, but this frontend does not support input reques\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1190 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m StdinNotImplementedError(msg)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1191 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._input_request(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mstr\u001b[0m(prompt),                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_ident[\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m],                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1194 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.get_parent(\u001b[33m\"\u001b[0m\u001b[33mshell\u001b[0m\u001b[33m\"\u001b[0m),                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m1234\u001b[0m in \u001b[92m_input_request\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1231 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1232 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1233 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmsg = \u001b[33m\"\u001b[0m\u001b[33mInterrupted by user\u001b[0m\u001b[33m\"\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1234 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m(msg) \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1235 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1236 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.log.warning(\u001b[33m\"\u001b[0m\u001b[33mInvalid Message:\u001b[0m\u001b[33m\"\u001b[0m, exc_info=\u001b[94mTrue\u001b[0m)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1237 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt: \u001b[0mInterrupted by user\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while True:\n",
    "    text_input = input()\n",
    "    try:\n",
    "        response = agent_chain.run(input=text_input)\n",
    "    except Exception as e:\n",
    "        response = str(e)\n",
    "        if not response.startswith(\"Could not parse LLM output: `\"):\n",
    "            raise e\n",
    "        response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# generate_text = pipeline(model=\"databricks/dolly-v2-12b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n",
    "# res = generate_text(\"I wan to know about nuclear fission\")\n",
    "# print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12283f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
